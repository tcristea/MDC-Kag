{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":82370,"databundleVersionId":12656064,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":12304615,"sourceType":"datasetVersion","datasetId":7755821},{"sourceId":12304826,"sourceType":"datasetVersion","datasetId":7755957},{"sourceId":12305690,"sourceType":"datasetVersion","datasetId":7756507},{"sourceId":12334120,"sourceType":"datasetVersion","datasetId":7775152},{"sourceId":12338599,"sourceType":"datasetVersion","datasetId":7778248},{"sourceId":12339780,"sourceType":"datasetVersion","datasetId":7779053},{"sourceId":248033444,"sourceType":"kernelVersion"},{"sourceId":248091965,"sourceType":"kernelVersion"}],"dockerImageVersionId":31040,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 📈 Inference Pipeline (No GROBID)\n\nThis notebook:\n- Loads your trained classifier (`saved_model`)\n- Reads PDFs directly via PyPDF2\n- Predicts `type`\n- Extracts `dataset_id` using regex\n- Produces `submission.csv`\n\n**Note:** No GROBID needed.","metadata":{}},{"cell_type":"code","source":"# 📦 Install PyPDF2 if needed\n# !pip install /kaggle/input/pypdf2-mdc/pypdf2-3.0.1-py3-none-any.whl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T21:12:39.085501Z","iopub.execute_input":"2025-07-01T21:12:39.085775Z","iopub.status.idle":"2025-07-01T21:12:39.090382Z","shell.execute_reply.started":"2025-07-01T21:12:39.085750Z","shell.execute_reply":"2025-07-01T21:12:39.089833Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import os","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T21:12:39.091863Z","iopub.execute_input":"2025-07-01T21:12:39.092417Z","iopub.status.idle":"2025-07-01T21:12:39.110644Z","shell.execute_reply.started":"2025-07-01T21:12:39.092398Z","shell.execute_reply":"2025-07-01T21:12:39.109886Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"! mkdir cache data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T21:12:39.111480Z","iopub.execute_input":"2025-07-01T21:12:39.111756Z","iopub.status.idle":"2025-07-01T21:12:39.246149Z","shell.execute_reply.started":"2025-07-01T21:12:39.111726Z","shell.execute_reply":"2025-07-01T21:12:39.245149Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"os.environ[\"XDG_CACHE_HOME\"] = os.path.join(os.getcwd(), \"cache\")\nos.environ[\"XDG_DATA_HOME\"] = os.path.join(os.getcwd(), \"data\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T21:12:39.248231Z","iopub.execute_input":"2025-07-01T21:12:39.248569Z","iopub.status.idle":"2025-07-01T21:12:39.254440Z","shell.execute_reply.started":"2025-07-01T21:12:39.248531Z","shell.execute_reply":"2025-07-01T21:12:39.253686Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"! cp -r /kaggle/input/datalab-marker/kaggle/working/datalab ./cache","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T21:12:39.255174Z","iopub.execute_input":"2025-07-01T21:12:39.255386Z","iopub.status.idle":"2025-07-01T21:13:11.240409Z","shell.execute_reply.started":"2025-07-01T21:12:39.255371Z","shell.execute_reply":"2025-07-01T21:13:11.239591Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"! cp -r /kaggle/input/pip-datalab/kaggle/working/cache/pip ./cache","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T21:13:11.241429Z","iopub.execute_input":"2025-07-01T21:13:11.241674Z","iopub.status.idle":"2025-07-01T21:13:11.408952Z","shell.execute_reply.started":"2025-07-01T21:13:11.241648Z","shell.execute_reply":"2025-07-01T21:13:11.408173Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"! mkdir -p /usr/local/lib/python3.11/dist-packages/static/fonts","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T21:13:11.411875Z","iopub.execute_input":"2025-07-01T21:13:11.412449Z","iopub.status.idle":"2025-07-01T21:13:12.729574Z","shell.execute_reply.started":"2025-07-01T21:13:11.412420Z","shell.execute_reply":"2025-07-01T21:13:12.728478Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"! cp /kaggle/input/marker-font/GoNotoCurrent-Regular.ttf /usr/local/lib/python3.11/dist-packages/static/fonts/GoNotoCurrent-Regular.ttf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T21:13:12.730692Z","iopub.execute_input":"2025-07-01T21:13:12.730934Z","iopub.status.idle":"2025-07-01T21:13:14.390798Z","shell.execute_reply.started":"2025-07-01T21:13:12.730905Z","shell.execute_reply":"2025-07-01T21:13:14.389797Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"! pip install /kaggle/working/cache/pip/wheels/8d/ac/a9/4e6dd2d86235ea3da1c286279118c49e931f77cfb33e9b1af5/EbookLib-0.18-py3-none-any.whl","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"! sh /kaggle/input/mdc-marker-pdf-reqs/install_requirements.sh","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 📚 Imports\nimport re\n# import PyPDF2\nfrom marker.converters.pdf import PdfConverter\nfrom marker.models import create_model_dict\nfrom marker.output import text_from_rendered\nimport pandas as pd\nfrom glob import glob\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 🧠 Load model and tokenizer\nmodel_path = \"/kaggle/input/makedatacount-mixed-train/saved_model_dual_text\"\ntoken_path = \"/kaggle/input/makedatacount-mixed-train/saved_model_dual_text\"\n\ntokenizer = AutoTokenizer.from_pretrained(token_path)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_path)\nmodel.eval()\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 🧩 Utility functions\nREPO_KEYWORDS = ['zenodo', 'dryad', 'figshare', 'pangaea', 'tcia']\n\ndef is_repository_doi(doi_url):\n    return any(k in doi_url for k in REPO_KEYWORDS)\n\ndef clean_doi(doi):\n    return doi.rstrip(')]>.,;')\n\ndef clean_pdf_text(text):\n    return (\n        text.replace('](', '] (')\n            .replace(')](', ') (')\n            .replace(')] (', ') (')\n            .replace('] (', '] (')\n    )\n\ndef standardize_doi(doi):\n    doi = str(doi).strip()\n    if doi.startswith(\"http\"):\n        return doi.lower()\n    if doi.startswith(\"doi:\"):\n        return \"https://doi.org/\" + doi[4:].lower()\n    if doi.startswith(\"10.\"):\n        return \"https://doi.org/\" + doi.lower()\n    return doi.lower()\nimport re\n\ndef extract_candidate_dois(text):\n    pattern_url = r'https?://[^\\s\\)<>\\]]+'\n    return list(set([\n        u.rstrip('.,;)]>') for u in re.findall(pattern_url, text)\n    ]))\n\ndef extract_all_doi_candidates(text):\n    text = text.replace('](', '] (').replace(')](', ') (')\n\n    patterns = [\n        r'https?://doi\\.org/[^\\s\\)<>\\]]+',\n        r'http://dx\\.doi\\.org/[^\\s\\)<>\\]]+',\n        r'doi:\\s*10\\.\\d{4,9}/[^\\s\\)<>\\]]+',\n        r'\\b10\\.\\d{4,9}/[^\\s\\)<>\\]]+'\n    ]\n\n    matches = []\n    for pat in patterns:\n        matches.extend(re.findall(pat, text, flags=re.IGNORECASE))\n\n    # Standardize all DOIs\n    return [standardize_doi(m) for m in matches]\n\ndef clean_and_deduplicate_dois(dois, min_suffix_len=8):\n    cleaned = set()\n    for doi in dois:\n        doi = doi.strip().rstrip('.,;)]>\\\\')\n        if doi.startswith(\"doi:\"):\n            doi = doi[4:].strip()\n        if doi.startswith(\"http://dx.doi.org/\"):\n            doi = doi.replace(\"http://dx.doi.org/\", \"https://doi.org/\")\n        if not doi.startswith(\"https://doi.org/10.\"):\n            doi = \"https://doi.org/\" + doi if doi.startswith(\"10.\") else None\n        if not doi:\n            continue\n        try:\n            prefix, suffix = doi.replace(\"https://doi.org/\", \"\").split(\"/\", 1)\n            if not suffix or len(suffix) < min_suffix_len:\n                continue\n            cleaned.add(\"https://doi.org/\" + prefix + \"/\" + suffix)\n        except ValueError:\n            continue\n    return sorted(cleaned)\n    \ndef extract_dataset_dois(text):\n    text = text.replace('](', '] (').replace(')](', ') (')\n\n    url_pattern = r'https?://[^\\s\\)<>\\]]+'\n    urls = re.findall(url_pattern, text)\n\n    doi_pattern = r'\\b10\\.\\d{4,9}/[^\\s\\)<>\\]]+'\n    bare_dois = re.findall(doi_pattern, text)\n\n    candidates = []\n\n    for u in urls:\n        clean_u = u.rstrip('.,;)]>')\n        if 'doi.org/10.' in clean_u:\n            candidates.append(clean_u)\n\n    for d in bare_dois:\n        candidates.append(standardize_doi(d))\n\n    # Remove invalid or incomplete\n    def is_valid_doi(doi_url):\n        if not doi_url.startswith(\"https://doi.org/10.\"):\n            return False\n        parts = doi_url.replace(\"https://doi.org/\", \"\").split(\"/\", 1)\n        if len(parts) != 2:\n            return False\n        suffix = parts[1]\n        if suffix == \"\" or suffix == \".\":\n            return False\n        return True\n\n    candidates = [c for c in candidates if is_valid_doi(c)]\n    candidates = [c for c in candidates if any(k in c for k in REPO_KEYWORDS)]\n\n    return sorted(set(candidates))\n\ndef find_dataset_dois(text):\n    # Pre-sanitize\n    text = text.replace('](', '] (').replace(')](', ') (')\n    \n    # Extract all candidate DOIs with strict filtering\n    raw_dois = extract_dataset_dois(text)\n    clean_dois = clean_and_deduplicate_dois(raw_dois)\n\n    # Lowercase for searching\n    text_lc = text.lower()\n    \n    # Define dataset-related keywords\n    keywords = [\n        \"data availability\", \"data are available\", \"data is available\",\n        \"data can be found\", \"archived in\", \"deposited in\", \"repository\",\n        \"zenodo\", \"figshare\", \"dryad\", \"pangaea\", \"dataset\", \"available at\", \"tcia\"\n    ]\n    \n    # Keep only DOIs near relevant context\n    dataset_dois = []\n    for doi in clean_dois:\n        idx = text_lc.find(doi.lower())\n        if idx == -1:\n            continue\n        window = text_lc[max(0, idx - 200): idx + 200]\n        if any(kw in window for kw in keywords):\n            dataset_dois.append(doi)\n    \n    return dataset_dois\n\ndef clean_and_deduplicate_dois(dois, min_suffix_len=8):\n    cleaned = set()\n    for doi in dois:\n        doi = doi.strip().rstrip('.,;)]>\\\\')\n        if doi.startswith(\"doi:\"):\n            doi = doi[4:].strip()\n        if doi.startswith(\"http://dx.doi.org/\"):\n            doi = doi.replace(\"http://dx.doi.org/\", \"https://doi.org/\")\n        if not doi.startswith(\"https://doi.org/10.\"):\n            doi = \"https://doi.org/\" + doi if doi.startswith(\"10.\") else None\n        if not doi:\n            continue\n        try:\n            prefix, suffix = doi.replace(\"https://doi.org/\", \"\").split(\"/\", 1)\n            if not suffix or len(suffix) < min_suffix_len:\n                continue\n            cleaned.add(\"https://doi.org/\" + prefix + \"/\" + suffix)\n        except ValueError:\n            continue\n    return sorted(cleaned)\n\n\ndef find_accession_ids_in_text(text):\n    patterns = [\n        r'\\b(GSE\\d+)\\b',\n        r'\\b(PRJ[ENAD]\\d+)\\b',\n        r'\\b(SRP\\d+)\\b',\n        r'\\b(E-[A-Z]+-\\d+)\\b',\n        r'\\b(pdb\\s[\\d\\w]+)\\b',\n        r'\\b(CHEMBL\\d+)\\b',\n    ]\n    matches = []\n    for pat in patterns:\n        matches.extend(re.findall(pat, text, re.IGNORECASE))\n    return [m.lower().replace(\" \", \"\") for m in matches]\n\ndef extract_article_id(filename):\n    return filename[:-4].replace(\"_\", \"/\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T23:12:45.710157Z","iopub.execute_input":"2025-07-01T23:12:45.710745Z","iopub.status.idle":"2025-07-01T23:12:45.730276Z","shell.execute_reply.started":"2025-07-01T23:12:45.710722Z","shell.execute_reply":"2025-07-01T23:12:45.729394Z"}},"outputs":[],"execution_count":59},{"cell_type":"code","source":"import os\n\n\nos.environ[\"DETECTOR_MODEL_CHECKPOINT\"] = \"/kaggle/working/cache/datalab/modelstext_detection/2025_05_07\"\nos.environ[\"RECOGNITION_MODEL_CHECKPOINT\"] = \"/kaggle/working/cache/datalab/modelstext_recognition/2025_05_16\"\nos.environ[\"LAYOUT_MODEL_CHECKPOINT\"] = \"/kaggle/working/cache/datalab/modelslayout/2025_02_18\"\nos.environ[\"OCR_ERROR_MODEL_CHECKPOINT\"] = \"/kaggle/working/cache/datalab/modelsocr_error_detection/2025_02_18\"\nos.environ[\"TABLE_REC_MODEL_CHECKPOINT\"] = \"/kaggle/working/cache/datalab/modelstable_recognition/2025_02_18\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T21:16:49.995053Z","iopub.execute_input":"2025-07-01T21:16:49.995340Z","iopub.status.idle":"2025-07-01T21:16:50.017521Z","shell.execute_reply.started":"2025-07-01T21:16:49.995323Z","shell.execute_reply":"2025-07-01T21:16:50.016501Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# 📂 Process PDFs\npdf_dir = \"/kaggle/input/make-data-count-finding-data-references/test/PDF\"\npdf_files = sorted(glob(os.path.join(pdf_dir, \"*.pdf\")))\nconverter = PdfConverter(\n    artifact_dict=create_model_dict(device='cuda:0'),\n)\nrows = []\ndef extract_text_from_pdf(file_path):\n    rendered = converter(\n        file_path\n    )\n\n    text, _, _ = text_from_rendered(rendered)\n\n    return text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T21:16:50.055827Z","iopub.execute_input":"2025-07-01T21:16:50.056657Z","iopub.status.idle":"2025-07-01T21:17:05.174808Z","shell.execute_reply.started":"2025-07-01T21:16:50.056630Z","shell.execute_reply":"2025-07-01T21:17:05.174149Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"for pdf_path in tqdm(pdf_files):\n    filename = os.path.basename(pdf_path)\n    article_id = extract_article_id(filename)\n    text = extract_text_from_pdf(pdf_path)\n    # Extract text with PyPDF2\n    # reader = PyPDF2.PdfReader(pdf_path)\n    # text = \"\\n\".join(page.extract_text() or \"\" for page in reader.pages)\n    \n    rows.append({\"article_id\": article_id, \"text\": text})","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 🔮 Predict type\nlabel_map = {0: \"Primary\", 1: \"Secondary\", 2: \"Missing\"}\nbatch_size = 8\npreds = []\n\nfor i in tqdm(range(0, len(rows), batch_size)):\n    batch_texts = [r[\"text\"] for r in rows[i:i+batch_size]]\n    enc = tokenizer(batch_texts, truncation=True, padding=True, max_length=512, return_tensors=\"pt\").to(device)\n    with torch.no_grad():\n        logits = model(**enc).logits\n        p = torch.argmax(logits, dim=1).cpu().tolist()\n        preds.extend(p)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T21:22:31.928992Z","iopub.execute_input":"2025-07-01T21:22:31.929261Z","iopub.status.idle":"2025-07-01T21:22:34.194066Z","shell.execute_reply.started":"2025-07-01T21:22:31.929237Z","shell.execute_reply":"2025-07-01T21:22:34.193158Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 4/4 [00:02<00:00,  1.77it/s]\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# 🏷️ Extract dataset IDs and build results\ndef infer_dataset_ids(text):\n    raw = extract_all_doi_candidates(text)\n    return clean_and_deduplicate_dois(raw)\n\nresults = []\nfor i, r in enumerate(rows):\n    t = r[\"text\"]\n    dois = find_dataset_dois(t)\n    # print(\"Possible DOIs found in text:\", r[\"article_id\"])\n    # for d in dois:\n        # print(\"-\", d)\n    accs = find_accession_ids_in_text(t)\n    dataset_id = dois[0] if dois else (accs[0] if accs else \"\")\n    results.append({\n        \"article_id\": r[\"article_id\"],\n        \"dataset_id\": dataset_id,\n        \"type\": label_map[preds[i]]\n    })","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T23:17:19.299282Z","iopub.execute_input":"2025-07-01T23:17:19.299897Z","iopub.status.idle":"2025-07-01T23:17:19.712796Z","shell.execute_reply.started":"2025-07-01T23:17:19.299875Z","shell.execute_reply":"2025-07-01T23:17:19.712137Z"}},"outputs":[],"execution_count":68},{"cell_type":"code","source":"# 📝 Build submission DataFrame\ndf = pd.DataFrame(results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T23:13:24.110708Z","iopub.execute_input":"2025-07-01T23:13:24.111418Z","iopub.status.idle":"2025-07-01T23:13:24.115995Z","shell.execute_reply.started":"2025-07-01T23:13:24.111391Z","shell.execute_reply":"2025-07-01T23:13:24.115171Z"}},"outputs":[],"execution_count":62},{"cell_type":"code","source":"df['article_id'] = df['article_id'].str.replace('/','_')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T23:13:31.623006Z","iopub.execute_input":"2025-07-01T23:13:31.623710Z","iopub.status.idle":"2025-07-01T23:13:31.628618Z","shell.execute_reply.started":"2025-07-01T23:13:31.623687Z","shell.execute_reply":"2025-07-01T23:13:31.627937Z"}},"outputs":[],"execution_count":63},{"cell_type":"code","source":"df = df[df[\"type\"] != \"Missing\"].reset_index(drop=True)\ndf = df.drop_duplicates(subset=['article_id', 'dataset_id'])\ndf.insert(0, \"row_id\", range(len(df)))\ndf.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T23:13:48.530519Z","iopub.execute_input":"2025-07-01T23:13:48.530774Z","iopub.status.idle":"2025-07-01T23:13:48.539758Z","shell.execute_reply.started":"2025-07-01T23:13:48.530757Z","shell.execute_reply":"2025-07-01T23:13:48.539158Z"}},"outputs":[],"execution_count":65},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}