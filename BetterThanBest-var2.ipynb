{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5fcc6dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-03T13:08:03.686589Z",
     "iopub.status.busy": "2025-08-03T13:08:03.686364Z",
     "iopub.status.idle": "2025-08-03T13:08:03.692234Z",
     "shell.execute_reply": "2025-08-03T13:08:03.691586Z"
    },
    "papermill": {
     "duration": 0.015388,
     "end_time": "2025-08-03T13:08:03.693387",
     "exception": false,
     "start_time": "2025-08-03T13:08:03.677999",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"]=\"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c257bcdf",
   "metadata": {
    "_cell_guid": "eae4b221-a822-451f-8f4b-134c3f9bfe2c",
    "_uuid": "b1883565-f717-4130-a662-5bb541f45ea1",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-08-03T13:08:03.707004Z",
     "iopub.status.busy": "2025-08-03T13:08:03.706812Z",
     "iopub.status.idle": "2025-08-03T13:08:05.058720Z",
     "shell.execute_reply": "2025-08-03T13:08:05.057996Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 1.36014,
     "end_time": "2025-08-03T13:08:05.060230",
     "exception": false,
     "start_time": "2025-08-03T13:08:03.700090",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\r\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m1 package\u001b[0m \u001b[2min 68ms\u001b[0m\u001b[0m\r\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 503ms\u001b[0m\u001b[0m\r\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 15ms\u001b[0m\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mpymupdf\u001b[0m\u001b[2m==1.26.1\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "# ! uv pip uninstall --system 'tensorflow'\n",
    "! uv pip install --system --no-index --find-links='/kaggle/input/latest-mdc-whls/whls' 'pymupdf' # 'vllm' 'triton' 'logits-processor-zoo' 'numpy<2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d12c4d02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-03T13:08:05.076226Z",
     "iopub.status.busy": "2025-08-03T13:08:05.075960Z",
     "iopub.status.idle": "2025-08-03T13:08:55.694256Z",
     "shell.execute_reply": "2025-08-03T13:08:55.693494Z"
    },
    "papermill": {
     "duration": 50.626915,
     "end_time": "2025-08-03T13:08:55.695357",
     "exception": false,
     "start_time": "2025-08-03T13:08:05.068442",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\r\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m145 packages\u001b[0m \u001b[2min 461ms\u001b[0m\u001b[0m\r\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m54 packages\u001b[0m \u001b[2min 41.40s\u001b[0m\u001b[0m\r\n",
      "\u001b[2mUninstalled \u001b[1m23 packages\u001b[0m \u001b[2min 2.35s\u001b[0m\u001b[0m\r\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m54 packages\u001b[0m \u001b[2min 405ms\u001b[0m\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mairportsdata\u001b[0m\u001b[2m==20250706\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mastor\u001b[0m\u001b[2m==0.8.1\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mblake3\u001b[0m\u001b[2m==1.0.5\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mcompressed-tensors\u001b[0m\u001b[2m==0.10.2\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mdepyf\u001b[0m\u001b[2m==0.18.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mdiskcache\u001b[0m\u001b[2m==5.6.3\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mfastapi-cli\u001b[0m\u001b[2m==0.0.8\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mfastapi-cloud-cli\u001b[0m\u001b[2m==0.1.1\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mgguf\u001b[0m\u001b[2m==0.17.1\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mhttptools\u001b[0m\u001b[2m==0.6.4\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mhttpx\u001b[0m\u001b[2m==0.28.1\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mhttpx\u001b[0m\u001b[2m==0.27.2\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1minteregular\u001b[0m\u001b[2m==0.3.3\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mlark\u001b[0m\u001b[2m==1.2.2\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mllguidance\u001b[0m\u001b[2m==0.7.30\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mllvmlite\u001b[0m\u001b[2m==0.43.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mllvmlite\u001b[0m\u001b[2m==0.44.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mlm-format-enforcer\u001b[0m\u001b[2m==0.10.11\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mmistral-common\u001b[0m\u001b[2m==1.6.3\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mmsgspec\u001b[0m\u001b[2m==0.19.0\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mnumba\u001b[0m\u001b[2m==0.60.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mnumba\u001b[0m\u001b[2m==0.61.2\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-cublas-cu12\u001b[0m\u001b[2m==12.5.3.2\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cublas-cu12\u001b[0m\u001b[2m==12.6.4.1\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-cuda-cupti-cu12\u001b[0m\u001b[2m==12.5.82\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-cupti-cu12\u001b[0m\u001b[2m==12.6.80\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-cuda-nvrtc-cu12\u001b[0m\u001b[2m==12.5.82\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-nvrtc-cu12\u001b[0m\u001b[2m==12.6.77\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-cuda-runtime-cu12\u001b[0m\u001b[2m==12.5.82\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-runtime-cu12\u001b[0m\u001b[2m==12.6.77\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-cudnn-cu12\u001b[0m\u001b[2m==9.3.0.75\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cudnn-cu12\u001b[0m\u001b[2m==9.5.1.17\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-cufft-cu12\u001b[0m\u001b[2m==11.2.3.61\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cufft-cu12\u001b[0m\u001b[2m==11.3.0.4\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cufile-cu12\u001b[0m\u001b[2m==1.11.1.6\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-curand-cu12\u001b[0m\u001b[2m==10.3.6.82\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-curand-cu12\u001b[0m\u001b[2m==10.3.7.77\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-cusolver-cu12\u001b[0m\u001b[2m==11.6.3.83\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusolver-cu12\u001b[0m\u001b[2m==11.7.1.2\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-cusparse-cu12\u001b[0m\u001b[2m==12.5.1.3\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusparse-cu12\u001b[0m\u001b[2m==12.5.4.2\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-cusparselt-cu12\u001b[0m\u001b[2m==0.6.2\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusparselt-cu12\u001b[0m\u001b[2m==0.6.3\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-nccl-cu12\u001b[0m\u001b[2m==2.21.5\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-nccl-cu12\u001b[0m\u001b[2m==2.26.2\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-nvjitlink-cu12\u001b[0m\u001b[2m==12.5.82\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-nvjitlink-cu12\u001b[0m\u001b[2m==12.6.85\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-nvtx-cu12\u001b[0m\u001b[2m==12.4.127\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-nvtx-cu12\u001b[0m\u001b[2m==12.6.77\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mopenai\u001b[0m\u001b[2m==1.91.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mopenai\u001b[0m\u001b[2m==1.90.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1moutlines\u001b[0m\u001b[2m==0.1.11\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1moutlines-core\u001b[0m\u001b[2m==0.1.26\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mpartial-json-parser\u001b[0m\u001b[2m==0.2.1.1.post6\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mprometheus-fastapi-instrumentator\u001b[0m\u001b[2m==7.1.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mpybase64\u001b[0m\u001b[2m==1.4.1\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mpycountry\u001b[0m\u001b[2m==24.6.1\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mpython-dotenv\u001b[0m\u001b[2m==1.1.1\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mpyzmq\u001b[0m\u001b[2m==24.0.1\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mpyzmq\u001b[0m\u001b[2m==27.0.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mrich-toolkit\u001b[0m\u001b[2m==0.14.8\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mrignore\u001b[0m\u001b[2m==0.5.1\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1msympy\u001b[0m\u001b[2m==1.13.1\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1msympy\u001b[0m\u001b[2m==1.14.0\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mtorch\u001b[0m\u001b[2m==2.6.0+cu124 (from https://download.pytorch.org/whl/cu124/torch-2.6.0%2Bcu124-cp311-cp311-linux_x86_64.whl)\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mtorch\u001b[0m\u001b[2m==2.7.0\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mtorchaudio\u001b[0m\u001b[2m==2.6.0+cu124 (from https://download.pytorch.org/whl/cu124/torchaudio-2.6.0%2Bcu124-cp311-cp311-linux_x86_64.whl)\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mtorchaudio\u001b[0m\u001b[2m==2.7.0\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mtorchvision\u001b[0m\u001b[2m==0.21.0+cu124 (from https://download.pytorch.org/whl/cu124/torchvision-0.21.0%2Bcu124-cp311-cp311-linux_x86_64.whl)\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mtorchvision\u001b[0m\u001b[2m==0.22.0\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mtriton\u001b[0m\u001b[2m==3.2.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mtriton\u001b[0m\u001b[2m==3.3.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1muvloop\u001b[0m\u001b[2m==0.21.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mvllm\u001b[0m\u001b[2m==0.9.2\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mwatchfiles\u001b[0m\u001b[2m==1.1.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mxformers\u001b[0m\u001b[2m==0.0.30\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mxgrammar\u001b[0m\u001b[2m==0.1.19\u001b[0m\r\n",
      "\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\r\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m54 packages\u001b[0m \u001b[2min 26ms\u001b[0m\u001b[0m\r\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 9ms\u001b[0m\u001b[0m\r\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mlogits-processor-zoo\u001b[0m\u001b[2m==0.1.10\u001b[0m\r\n",
      "\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\r\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m1 package\u001b[0m \u001b[2min 25ms\u001b[0m\u001b[0m\r\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 4.86s\u001b[0m\u001b[0m\r\n",
      "\u001b[2mUninstalled \u001b[1m1 package\u001b[0m \u001b[2min 7ms\u001b[0m\u001b[0m\r\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 12ms\u001b[0m\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mtriton\u001b[0m\u001b[2m==3.3.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mtriton\u001b[0m\u001b[2m==3.2.0\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "# ! uv pip install /kaggle/input/mdcfitz/pymupdf-1.26.3-cp39-abi3-manylinux_2_28_x86_64.whl\n",
    "! uv pip install vllm --no-index --find-links file:///kaggle/input/mdcllm\n",
    "! uv pip install logits-processor-zoo==0.1.10 --no-index --find-links file:///kaggle/input/mdcllm\n",
    "! uv pip install triton==3.2.0 --no-index --find-links file:///kaggle/input/mdcllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa46a52c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-03T13:08:55.731055Z",
     "iopub.status.busy": "2025-08-03T13:08:55.730806Z",
     "iopub.status.idle": "2025-08-03T13:08:55.850367Z",
     "shell.execute_reply": "2025-08-03T13:08:55.849603Z"
    },
    "papermill": {
     "duration": 0.138269,
     "end_time": "2025-08-03T13:08:55.851774",
     "exception": false,
     "start_time": "2025-08-03T13:08:55.713505",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "! mkdir -p /tmp/src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b69c5125",
   "metadata": {
    "_cell_guid": "34135540-31fa-4d24-8934-acb1e0711a4f",
    "_uuid": "92f33014-02d3-41cb-b906-ffeb89a3f353",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-08-03T13:08:55.886405Z",
     "iopub.status.busy": "2025-08-03T13:08:55.886187Z",
     "iopub.status.idle": "2025-08-03T13:08:55.894339Z",
     "shell.execute_reply": "2025-08-03T13:08:55.893780Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.026917,
     "end_time": "2025-08-03T13:08:55.895399",
     "exception": false,
     "start_time": "2025-08-03T13:08:55.868482",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /tmp/src/helpers.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /tmp/src/helpers.py\n",
    "import logging, os, kagglehub, inspect\n",
    "from pathlib import Path\n",
    "import polars as pl\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"]=\"False\"\n",
    "IS_KAGGLE_ENV = sum(['KAGGLE' in k for k in os.environ]) > 0\n",
    "os.environ[\"KAGGLE_IS_COMPETITION_RERUN\"] = \"1\"\n",
    "\n",
    "IS_KAGGLE_SUBMISSION = bool(os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"))\n",
    "\n",
    "COMP_DIR = Path(('/kaggle/input/make-data-count-finding-data-references' if IS_KAGGLE_SUBMISSION else kagglehub.competition_download('make-data-count-finding-data-references')))\n",
    "PDF_DIR = COMP_DIR / ('test' if IS_KAGGLE_SUBMISSION else 'train') / 'PDF'\n",
    "WORKING_DIR = Path(('/kaggle/working/' if IS_KAGGLE_ENV else '.working/'))\n",
    "\n",
    "DOI_LINK = 'https://doi.org/'\n",
    "\n",
    "DEFAULT_LOG_LEVEL = os.getenv(\"LOG_LEVEL\", \"DEBUG\").upper() if not IS_KAGGLE_SUBMISSION else \"WARNING\"\n",
    "LOG_FILE_PATH = os.getenv(\"LOG_FILE\", \"logs/project.log\")\n",
    "LOG_DIR = Path(LOG_FILE_PATH).parent\n",
    "\n",
    "LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "LOG_FORMAT = \"%(levelname)s %(asctime)s  [%(filename)s:%(lineno)d - %(funcName)s()] %(message)s\"\n",
    "LOG_DATEFMT = \"%Y-%m-%d %H:%M:%S\"\n",
    "\n",
    "def get_logger(name=None):\n",
    "    if name is None:\n",
    "        frame = inspect.currentframe()\n",
    "        if frame is None or frame.f_back is None:\n",
    "            name = \"__main__\"\n",
    "        else:\n",
    "            name = frame.f_back.f_globals.get(\"__name__\", \"__main__\")\n",
    "\n",
    "    logger = logging.getLogger(name)\n",
    "\n",
    "    if not logger.handlers:\n",
    "        logger.setLevel(DEFAULT_LOG_LEVEL)\n",
    "        formatter = logging.Formatter(fmt=LOG_FORMAT, datefmt=LOG_DATEFMT)\n",
    "        ch = logging.StreamHandler()\n",
    "        ch.setLevel(DEFAULT_LOG_LEVEL)\n",
    "        ch.setFormatter(formatter)\n",
    "        fh = logging.FileHandler(LOG_FILE_PATH)\n",
    "        fh.setLevel(DEFAULT_LOG_LEVEL)\n",
    "        fh.setFormatter(formatter)\n",
    "        logger.addHandler(ch)\n",
    "        logger.addHandler(fh)\n",
    "        logger.propagate = False\n",
    "    return logger\n",
    "\n",
    "def is_doi_link(name: str) -> pl.Expr:\n",
    "    return pl.col(name).str.starts_with(DOI_LINK)\n",
    "\n",
    "def string_normalization(name: str) -> pl.Expr:\n",
    "    return pl.col(name).str.normalize(\"NFKC\").str.replace_all(r\"[^\\p{Ascii}]\", '').str.replace_all(r\"https?://zenodo\\.org/record/(\\d+)\", r\" 10.5281/zenodo.$1 \")\n",
    "\n",
    "def get_df(parse_dir: str):\n",
    "    records = []\n",
    "    txt_files = list(Path(parse_dir).glob('*.txt'))\n",
    "    for txt_file in txt_files:\n",
    "        id_ = txt_file.stem\n",
    "        with open(txt_file, 'r') as f:\n",
    "            text = f.read()\n",
    "        records.append({'article_id': id_, 'text': text})\n",
    "    return pl.DataFrame(records).with_columns(string_normalization('text').alias('text'))\n",
    "## remove\n",
    "def assume_type_bak(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    return (\n",
    "        df.with_columns(pl.when(is_doi_link('dataset_id').or_(pl.col('dataset_id').str.starts_with('SAMN'))).then(pl.lit('Primary')).otherwise(pl.lit('Secondary')).alias('type'))\n",
    "    )\n",
    "\n",
    "def assume_type(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"Better rule-based classification using context keywords\"\"\"\n",
    "    \n",
    "    def classify_type(window, dataset_id):\n",
    "        if pd.isna(window):\n",
    "            window = \"\"\n",
    "        window_lower = window.lower()\n",
    "        \n",
    "        # Strong Primary indicators\n",
    "        if any(phrase in window_lower for phrase in [\n",
    "            \"we deposited\", \"we uploaded\", \"we submitted\", \"deposited by us\",\n",
    "            \"our dataset\", \"we generated\", \"we collected\", \"we sequenced\"\n",
    "        ]):\n",
    "            return \"Primary\"\n",
    "        \n",
    "        # Strong Secondary indicators  \n",
    "        if any(phrase in window_lower for phrase in [\n",
    "            \"et al\", \"previously published\", \"downloaded from\", \n",
    "            \"obtained from\", \"retrieved from\", \"described by\"\n",
    "        ]):\n",
    "            return \"Secondary\"\n",
    "        \n",
    "        # Fallback to identifier-based logic\n",
    "        if is_doi_link_simple(dataset_id) or dataset_id.startswith('SAMN'):\n",
    "            return \"Primary\"\n",
    "        else:\n",
    "            return \"Secondary\"\n",
    "    \n",
    "    types = [classify_type(w, d) for w, d in df.select(['window', 'dataset_id']).rows()]\n",
    "    return df.with_columns(pl.Series('type', types))\n",
    "\n",
    "def is_doi_link_simple(dataset_id):\n",
    "    return dataset_id.startswith('https://doi.org/')\n",
    "# wait\n",
    "def assume_type_improved(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"Use context windows for Primary/Secondary classification\"\"\"\n",
    "    \n",
    "    # Extract features from context windows\n",
    "    primary_patterns = [\n",
    "        r\"(?i)\\b(we\\s+(collected|generated|created|obtained|sequenced|measured))\",\n",
    "        r\"(?i)\\b(our\\s+dataset|newly\\s+generated|custom\\s+dataset)\",\n",
    "        r\"(?i)\\b(experimental\\s+data|original\\s+data|novel\\s+dataset)\",\n",
    "        r\"(?i)\\b(deposited|uploaded|submitted|archived)\\b.*\\b(by\\s+us|in\\s+this\\s+study)\"\n",
    "    ]\n",
    "    \n",
    "    secondary_patterns = [\n",
    "        r\"(?i)\\b(previously\\s+published|existing\\s+dataset|publicly\\s+available)\",\n",
    "        r\"(?i)\\b(obtained\\s+from|downloaded\\s+from|retrieved\\s+from)\",\n",
    "        r\"(?i)\\b(reanalyz|secondary\\s+analysis|meta-analysis)\",\n",
    "        r\"(?i)\\b(Smith\\s+et\\s+al|Jones\\s+et\\s+al|\\w+\\s+et\\s+al\\.?\\s+\\(20\\d{2}\\))\",  # Citations\n",
    "        r\"(?i)\\b(reported\\s+by|described\\s+by|published\\s+by)\"\n",
    "    ]\n",
    "    \n",
    "    # Score based on context\n",
    "    primary_scores = []\n",
    "    secondary_scores = []\n",
    "    \n",
    "    for window in df['window']:\n",
    "        if pd.isna(window):\n",
    "            window = \"\"\n",
    "        \n",
    "        primary_score = sum(1 for pattern in primary_patterns \n",
    "                           if re.search(pattern, window))\n",
    "        secondary_score = sum(1 for pattern in secondary_patterns \n",
    "                             if re.search(pattern, window))\n",
    "        \n",
    "        primary_scores.append(primary_score)\n",
    "        secondary_scores.append(secondary_score)\n",
    "    \n",
    "    return df.with_columns([\n",
    "        pl.Series('primary_score', primary_scores),\n",
    "        pl.Series('secondary_score', secondary_scores)\n",
    "    ]).with_columns(\n",
    "        pl.when(pl.col('primary_score') > pl.col('secondary_score'))\n",
    "          .then(pl.lit('Primary'))\n",
    "          .when(pl.col('secondary_score') > pl.col('primary_score'))\n",
    "          .then(pl.lit('Secondary'))\n",
    "          .otherwise(\n",
    "              # Fallback to your current logic when context is unclear\n",
    "              pl.when(is_doi_link('dataset_id') | pl.col('dataset_id').str.starts_with('SAMN'))\n",
    "                .then(pl.lit('Primary'))\n",
    "                .otherwise(pl.lit('Secondary'))\n",
    "          )\n",
    "          .alias('type')\n",
    "    )\n",
    "\n",
    "def score(df, gt, on, tag='all'):\n",
    "    hits = gt.join(df, on=on)\n",
    "    tp = hits.height\n",
    "    fp = df.height - tp\n",
    "    fn = gt.height - tp\n",
    "    f1 = 2 * tp / (2 * tp + fp + fn) if (2 * tp + fp + fn) != 0 else 0.0\n",
    "    return f\"{tag} - f1: {f1:.4f} [{tp}/{fp}/{fn}]\"\n",
    "\n",
    "def evaluate(df, on=['article_id', 'dataset_id']):\n",
    "    gt = pl.read_csv(COMP_DIR/'train_labels.csv').filter(pl.col('type')!='Missing')\n",
    "    return (\n",
    "        score(df, gt, on),\n",
    "        score(df.filter(is_doi_link('dataset_id')), gt.filter(is_doi_link('dataset_id')), on, 'doi'),\n",
    "        score(df.filter(~is_doi_link('dataset_id')), gt.filter(~is_doi_link('dataset_id')), on, 'acc'),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3344d96b",
   "metadata": {
    "_cell_guid": "98be0899-3cad-423b-a3db-313209068df0",
    "_uuid": "84859532-6acd-4011-b783-d0d24257a19b",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-08-03T13:08:55.929331Z",
     "iopub.status.busy": "2025-08-03T13:08:55.929116Z",
     "iopub.status.idle": "2025-08-03T13:08:55.933585Z",
     "shell.execute_reply": "2025-08-03T13:08:55.932915Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.022715,
     "end_time": "2025-08-03T13:08:55.934686",
     "exception": false,
     "start_time": "2025-08-03T13:08:55.911971",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /tmp/src/parse.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /tmp/src/parse.py\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import pymupdf\n",
    "from helpers import get_logger, PDF_DIR\n",
    "\n",
    "l = get_logger()\n",
    "\n",
    "def pdf_to_txt(output_dir: Path):\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    pdf_files = list(PDF_DIR.glob(\"*.pdf\")) + list(PDF_DIR.glob(\"*.PDF\"))\n",
    "    existing_txt_files = {f.stem for f in output_dir.glob(\"*.txt\")}\n",
    "    for pdf_file in pdf_files:\n",
    "        txt_file = output_dir / f\"{pdf_file.stem}.txt\"\n",
    "        if pdf_file.stem in existing_txt_files:\n",
    "            continue\n",
    "        try:\n",
    "            text = \"\"\n",
    "            with pymupdf.open(pdf_file) as doc:\n",
    "                for page in doc:\n",
    "                    text += page.get_text()\n",
    "            txt_file.write_text(text, encoding='utf-8')\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('output_dir', type=Path, help='Directory to save text files')\n",
    "    args = parser.parse_args()\n",
    "    pdf_to_txt(args.output_dir)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3505a2af",
   "metadata": {
    "_cell_guid": "01632e8b-2a68-4dec-9606-f91214a8c020",
    "_uuid": "5210e49f-e5ab-45c6-b673-f0bd08dc1877",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-08-03T13:08:55.968906Z",
     "iopub.status.busy": "2025-08-03T13:08:55.968707Z",
     "iopub.status.idle": "2025-08-03T13:08:55.973131Z",
     "shell.execute_reply": "2025-08-03T13:08:55.972456Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.022688,
     "end_time": "2025-08-03T13:08:55.974137",
     "exception": false,
     "start_time": "2025-08-03T13:08:55.951449",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /tmp/src/check_parse.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /tmp/src/check_parse.py\n",
    "import polars as pl\n",
    "from pathlib import Path\n",
    "from helpers import *\n",
    "\n",
    "l=get_logger()\n",
    "\n",
    "def gt_dataset_id_normalization(name:str) -> pl.Expr:\n",
    "    return (\n",
    "        pl.when(is_doi_link(name))\n",
    "        .then(pl.col(name).str.split(DOI_LINK).list.last())\n",
    "        .otherwise(name)\n",
    "        .str.to_lowercase()\n",
    "    )\n",
    "\n",
    "def main():\n",
    "    if IS_KAGGLE_SUBMISSION:\n",
    "        l.debug('skipping check_parse for submission')\n",
    "        return\n",
    "    df = (\n",
    "        get_df('/tmp/train_parse')\n",
    "        .with_columns(pl.col('text').str.replace_all('\\s+', '').str.to_lowercase().alias('text'))\n",
    "    )\n",
    "\n",
    "    gt = (\n",
    "        pl.read_csv(COMP_DIR/'train_labels.csv')\n",
    "        .filter(pl.col('article_id').is_in(df['article_id']))\n",
    "        .filter(pl.col('type')!='Missing')\n",
    "        .with_columns(gt_dataset_id_normalization('dataset_id').alias('norm_id'))\n",
    "    )\n",
    "\n",
    "    l.info(f\"pymupdf misses: {gt.join(df, on='article_id').with_columns(hit=pl.col('text').str.contains(pl.col('norm_id'), literal=True)).filter(~pl.col('hit')).height} dataset_ids\")\n",
    "\n",
    "if __name__=='__main__': main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "067cf6fd",
   "metadata": {
    "_cell_guid": "83084a3e-ab3e-4c24-9045-7bc81df72e34",
    "_uuid": "5a79e391-1a5c-4264-9aa5-747cb657a266",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-08-03T13:08:56.008402Z",
     "iopub.status.busy": "2025-08-03T13:08:56.008192Z",
     "iopub.status.idle": "2025-08-03T13:08:56.015138Z",
     "shell.execute_reply": "2025-08-03T13:08:56.014599Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.025305,
     "end_time": "2025-08-03T13:08:56.016153",
     "exception": false,
     "start_time": "2025-08-03T13:08:55.990848",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /tmp/src/getid.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /tmp/src/getid.py\n",
    "import re\n",
    "import polars as pl\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "from helpers import *\n",
    "\n",
    "COMPILED_PATTERNS = {\n",
    "    'ref_header_patterns': [re.compile(r'\\b(R\\s*E\\s*F\\s*E\\s*R\\s*E\\s*N\\s*C\\s*E\\s*S|BIBLIOGRAPHY|LITERATURE CITED|WORKS CITED|CITED WORKS|ACKNOWLEDGEMENTS)\\b[:\\s]*', re.IGNORECASE)],\n",
    "    'citation_pattern': re.compile(r'^\\s*(\\[\\d+\\]|\\(\\d+\\)|\\d+\\.|\\d+\\)|\\d+(?=\\s|$))\\s*'),\n",
    "    'first_citation_patterns': [\n",
    "        re.compile(r'^\\s*\\[1\\]\\s*'),\n",
    "        re.compile(r'^\\s*\\(1\\)\\s*'),\n",
    "        re.compile(r'^\\s*1\\.\\s*'),\n",
    "        re.compile(r'^\\s*1\\)\\s*'),\n",
    "        re.compile(r'^\\s*1(?=\\s|$)'),\n",
    "    ],\n",
    "}\n",
    "\n",
    "l = get_logger()\n",
    "\n",
    "def find_last_reference_header(text: str, header_patterns: list[re.Pattern]) -> Optional[int]:\n",
    "    last_match_idx = None\n",
    "    for pattern in header_patterns:\n",
    "        matches = list(pattern.finditer(text))\n",
    "        if matches:\n",
    "            last_match_idx = matches[-1].start()\n",
    "    return last_match_idx\n",
    "\n",
    "def find_last_first_citation(text: str) -> Optional[int]:\n",
    "    lines = text.splitlines()\n",
    "    last_match_line = None\n",
    "    for line_num, line in enumerate(lines):\n",
    "        line = line.strip()\n",
    "        for pattern in COMPILED_PATTERNS['first_citation_patterns']:\n",
    "            if pattern.match(line):\n",
    "                next_lines = lines[line_num:line_num+3]\n",
    "                if any(COMPILED_PATTERNS['citation_pattern'].match(l.strip()) for l in next_lines[1:]):\n",
    "                    last_match_line = line_num\n",
    "                break\n",
    "    return last_match_line\n",
    "\n",
    "def find_reference_start(text: str) -> Optional[int]:\n",
    "    lines = text.splitlines()\n",
    "    last_first_citation = find_last_first_citation(text)\n",
    "    if last_first_citation is not None:\n",
    "        return last_first_citation\n",
    "    start_search_idx = int(len(lines) * 0.5)\n",
    "    for i in range(start_search_idx, len(lines)):\n",
    "        line = lines[i].strip()\n",
    "        if COMPILED_PATTERNS['citation_pattern'].match(line):\n",
    "            next_lines = lines[i:i+3]\n",
    "            if sum(1 for l in next_lines if COMPILED_PATTERNS['citation_pattern'].match(l.strip())) >= 2:\n",
    "                for j in range(i, max(-1, i-10), -1):\n",
    "                    if not COMPILED_PATTERNS['citation_pattern'].match(lines[j].strip()):\n",
    "                        return j + 1\n",
    "                return max(0, i-10)\n",
    "    return None\n",
    "\n",
    "def split_text_and_references(text: str) -> Tuple[str, str]:\n",
    "    header_idx = find_last_reference_header(text, COMPILED_PATTERNS['ref_header_patterns'])\n",
    "    if header_idx is not None:\n",
    "        header_idx2 = find_last_reference_header(text[:header_idx].strip(), COMPILED_PATTERNS['ref_header_patterns'])\n",
    "        if header_idx2 is not None:\n",
    "            header_idx3 = find_last_reference_header(text[:header_idx2].strip(), COMPILED_PATTERNS['ref_header_patterns'])\n",
    "            if header_idx3 is not None:\n",
    "                return text[:header_idx3].strip(), text[header_idx3:].strip()\n",
    "            return text[:header_idx2].strip(), text[header_idx2:].strip()\n",
    "        return text[:header_idx].strip(), text[header_idx:].strip()\n",
    "    ref_start_line = find_reference_start(text)\n",
    "    if ref_start_line is not None:\n",
    "        lines = text.splitlines()\n",
    "        body = '\\n'.join(lines[:ref_start_line])\n",
    "        refs = '\\n'.join(lines[ref_start_line:])\n",
    "        return body.strip(), refs.strip()\n",
    "    return text.strip(), ''\n",
    "\n",
    "def get_splits(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    bodies, refs = [], []\n",
    "    for raw_text in df['text']:\n",
    "        main, ref = split_text_and_references(raw_text)\n",
    "        bodies.append(main)\n",
    "        refs.append(ref)\n",
    "    return df.with_columns(pl.Series('body', bodies), pl.Series('ref', refs))\n",
    "\n",
    "def tidy_extraction(df) -> pl.DataFrame:\n",
    "    bad_ids = [f'{DOI_LINK}{e}' for e in ['10.5061/dryad', '10.5281/zenodo', '10.6073/pasta']]\n",
    "\n",
    "    doi_df = (\n",
    "        df.with_columns(pl.col('body').str.extract_all(r'10\\s*\\.\\s*\\d{4,9}\\s*/\\s*\\S+').alias('match'))\n",
    "          .explode('match')\n",
    "          .drop_nulls('match')\n",
    "          .with_columns(\n",
    "              pl.col('match').str.replace_all(r'\\s+', '')\n",
    "                             .str.replace(r'[^A-Za-z0-9]+$', '')\n",
    "                             .str.to_lowercase()\n",
    "                             .alias('dataset_id')\n",
    "          )\n",
    "          .group_by('article_id', 'dataset_id')\n",
    "          .agg('match')\n",
    "          .with_columns((DOI_LINK + pl.col('dataset_id')).alias('dataset_id'))\n",
    "    )\n",
    "\n",
    "    REGEX_IDS = (\n",
    "        r\"(?i)\\b(?:\"\n",
    "        r\"CHEMBL\\d+|\"\n",
    "        r\"E-GEOD-\\d+|E-PROT-\\d+|EMPIAR-\\d+|\"\n",
    "        r\"ENSBTAG\\d+|ENSOARG\\d+|\"\n",
    "        r\"EPI_ISL_\\d{5,}|EPI\\d{6,7}|\"\n",
    "        r\"HPA\\d+|CP\\d{6}|IPR\\d{6}|PF\\d{5}|KX\\d{6}|K0\\d{4}|\"\n",
    "        r\"PRJNA\\d+|PRJEB\\d+|PXD\\d+|SAMN\\d+|\"\n",
    "        r\"GSE\\d+|GSM\\d+|GPL\\d+|\"\n",
    "        r\"E-MTAB-\\d+|E-MEXP-\\d+|\"\n",
    "        r\"PDB\\s?\\w{4}|HMDB\\d+|\"\n",
    "        r\"dryad\\.[^\\s\\\"<>]+|pasta\\/[^\\s\\\"<>]+|\"\n",
    "        r\"(?:SRR|SRX|SRP|ERR|DRR|DRX|DRP|ERP|ERX)\\d+\"\n",
    "        r\")\"\n",
    "    )\n",
    "\n",
    "\n",
    "    acc_df = (\n",
    "        df.with_columns(\n",
    "            pl.col('text').str.extract_all(REGEX_IDS).alias('match')\n",
    "        )\n",
    "        .explode('match')\n",
    "        .drop_nulls('match')\n",
    "        .with_columns(\n",
    "            pl.col('match').str.replace_all(r'\\s+', '')\n",
    "                           .str.replace(r'[^A-Za-z0-9]+$', '')\n",
    "                           .alias('dataset_id')\n",
    "        )\n",
    "        .group_by('article_id', 'dataset_id')\n",
    "        .agg('match')\n",
    "        .with_columns(\n",
    "            pl.when(pl.col('dataset_id').str.starts_with('dryad.'))\n",
    "              .then(f'{DOI_LINK}10.5061/' + pl.col('dataset_id'))\n",
    "              .otherwise('dataset_id')\n",
    "              .alias('dataset_id')\n",
    "        )\n",
    "        .with_columns(\n",
    "            pl.when(pl.col('dataset_id').str.starts_with('pasta/'))\n",
    "              .then(f'{DOI_LINK}10.6073/' + pl.col('dataset_id'))\n",
    "              .otherwise('dataset_id')\n",
    "              .alias('dataset_id')\n",
    "        )\n",
    "    )\n",
    "\n",
    "    df = pl.concat([doi_df, acc_df])\n",
    "\n",
    "    df = (\n",
    "        df.unique(['article_id', 'dataset_id'])  # CHANGED\n",
    "          .filter(~pl.col('article_id').str.replace('_','/').str.contains(pl.col('dataset_id').str.split(DOI_LINK).list.last().str.escape_regex()))\n",
    "          .filter(~pl.col('dataset_id').str.contains(pl.col('article_id').str.replace('_','/').str.escape_regex()))\n",
    "          .filter(~pl.col('dataset_id').str.contains('figshare', literal=True))\n",
    "          .filter(~pl.col('dataset_id').is_in(bad_ids))\n",
    "          .filter(\n",
    "              pl.when(is_doi_link('dataset_id') &\n",
    "                      (pl.col('dataset_id').str.split('/').list.last().str.len_chars() < 5))\n",
    "               .then(False)\n",
    "               .otherwise(True)\n",
    "          )\n",
    "          .with_columns(pl.col('match').list.unique())\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def get_context_window(text: str, substring: str, window: int = 100) -> str:\n",
    "    idx = text.find(substring)\n",
    "    if idx == -1:\n",
    "        raise ValueError\n",
    "    start = max(idx - window, 0)\n",
    "    end = min(idx + len(substring) + window, len(text))\n",
    "    return text[start:end]\n",
    "\n",
    "def get_window_df(text_df, ids_df):\n",
    "    df = ids_df.join(text_df, on='article_id')\n",
    "    windows = []\n",
    "    for text, match_ids in df.select('text', 'match').rows():\n",
    "        windows.append(get_context_window(text, match_ids[0]))\n",
    "    return df.with_columns(pl.Series('window', windows)).select('article_id', 'dataset_id', 'window')\n",
    "\n",
    "def main():\n",
    "    text_df = get_df('/tmp/train_parse')\n",
    "    df = get_splits(text_df)\n",
    "    df = get_splits(text_df)\n",
    "    df.write_parquet(\"/tmp/train_parse_split.parquet\")  # <-- Add this line\n",
    "    df = tidy_extraction(df)\n",
    "    df = get_window_df(text_df, df)\n",
    "    df.write_parquet('/tmp/extracted.parquet')\n",
    "    df = assume_type(df)\n",
    "    df.select(['article_id', 'dataset_id', 'type']).with_row_index(name='row_id').write_csv('/kaggle/working/submission.csv')\n",
    "    if not IS_KAGGLE_SUBMISSION:\n",
    "        results = evaluate(df)\n",
    "        for r in results: l.info(r)\n",
    "        results = evaluate(df, on=['article_id', 'dataset_id', 'type'])\n",
    "        for r in results: l.info(r)\n",
    "\n",
    "if __name__=='__main__': main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df34f33b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-03T13:08:56.050159Z",
     "iopub.status.busy": "2025-08-03T13:08:56.049968Z",
     "iopub.status.idle": "2025-08-03T13:08:56.054477Z",
     "shell.execute_reply": "2025-08-03T13:08:56.053856Z"
    },
    "papermill": {
     "duration": 0.022436,
     "end_time": "2025-08-03T13:08:56.055466",
     "exception": false,
     "start_time": "2025-08-03T13:08:56.033030",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /tmp/src/addon_fix_broken_dois.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /tmp/src/addon_fix_broken_dois.py\n",
    "import re\n",
    "import polars as pl\n",
    "from helpers import *\n",
    "\n",
    "l = get_logger()\n",
    "\n",
    "BROKEN_DOI_REGEX = re.compile(r'10\\s*\\.\\s*\\d{4,9}\\s*/\\s*[A-Za-z0-9._;()/:\\-]+', re.IGNORECASE)\n",
    "\n",
    "def normalize_doi(s: str) -> str:\n",
    "    return re.sub(r'[^A-Za-z0-9./-]+', '', s).lower().rstrip('.;:,')\n",
    "\n",
    "def extract_broken_dois(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    l.info(\"Extracting broken DOIs from ref section\")\n",
    "    return (\n",
    "        df.select(['article_id', 'ref'])\n",
    "          .with_columns(\n",
    "              pl.col(\"ref\").map_elements(lambda x: BROKEN_DOI_REGEX.findall(x or ''), return_dtype=pl.List(pl.String)).alias(\"match\")\n",
    "          )\n",
    "          .explode(\"match\")\n",
    "          .drop_nulls(\"match\")\n",
    "          .with_columns(\n",
    "              pl.col(\"match\").map_elements(normalize_doi, return_dtype=pl.String).alias(\"dataset_id\")\n",
    "          )\n",
    "          .filter(pl.col(\"dataset_id\").str.starts_with(\"10.\"))\n",
    "          .group_by(\"article_id\", \"dataset_id\")\n",
    "          .agg(pl.col(\"match\"))\n",
    "          .with_columns((DOI_LINK + pl.col(\"dataset_id\")).alias(\"dataset_id\"))\n",
    "    )\n",
    "\n",
    "def main():\n",
    "    df = pl.read_parquet('/tmp/train_parse_split.parquet')\n",
    "    broken_dois = extract_broken_dois(df)\n",
    "    broken_dois.write_parquet('/tmp/addon_broken_dois.parquet')\n",
    "    l.info(f\"Extracted {broken_dois.shape[0]} broken DOIs\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98f33bc2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-03T13:08:56.090332Z",
     "iopub.status.busy": "2025-08-03T13:08:56.089831Z",
     "iopub.status.idle": "2025-08-03T13:08:56.094436Z",
     "shell.execute_reply": "2025-08-03T13:08:56.093800Z"
    },
    "papermill": {
     "duration": 0.023024,
     "end_time": "2025-08-03T13:08:56.095450",
     "exception": false,
     "start_time": "2025-08-03T13:08:56.072426",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /tmp/src/addon_reference_priority.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /tmp/src/addon_reference_priority.py\n",
    "import polars as pl\n",
    "from helpers import *\n",
    "\n",
    "l = get_logger()\n",
    "\n",
    "def extract_reference_dois(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    l.info(\"Tagging DOIs from reference section only\")\n",
    "    return (\n",
    "        df.select(['article_id', 'ref'])\n",
    "          .with_columns(\n",
    "              pl.col(\"ref\")\n",
    "              .str.extract_all(r'10\\.\\d{4,9}/\\S+')\n",
    "              .alias(\"match\")\n",
    "          )\n",
    "          .explode(\"match\")\n",
    "          .drop_nulls(\"match\")\n",
    "          .with_columns(\n",
    "              pl.col(\"match\")\n",
    "              .str.replace_all(r'[\\s]+', '')\n",
    "              .str.replace(r'[^A-Za-z0-9./-]+$', '')\n",
    "              .str.to_lowercase()\n",
    "              .alias(\"dataset_id\")\n",
    "          )\n",
    "          .filter(pl.col(\"dataset_id\").str.starts_with(\"10.\"))\n",
    "          .group_by(\"article_id\", \"dataset_id\")\n",
    "          .agg(pl.col(\"match\"))\n",
    "          .with_columns((DOI_LINK + pl.col(\"dataset_id\")).alias(\"dataset_id\"))\n",
    "    )\n",
    "\n",
    "def main():\n",
    "    df = pl.read_parquet('/tmp/train_parse_split.parquet')\n",
    "    ref_dois = extract_reference_dois(df)\n",
    "    ref_dois.write_parquet('/tmp/addon_ref_priority.parquet')\n",
    "    l.info(f\"Extracted {ref_dois.shape[0]} reference-priority DOIs\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72906ebc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-03T13:08:56.130377Z",
     "iopub.status.busy": "2025-08-03T13:08:56.130188Z",
     "iopub.status.idle": "2025-08-03T13:08:56.134210Z",
     "shell.execute_reply": "2025-08-03T13:08:56.133728Z"
    },
    "papermill": {
     "duration": 0.022517,
     "end_time": "2025-08-03T13:08:56.135267",
     "exception": false,
     "start_time": "2025-08-03T13:08:56.112750",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /tmp/src/merge_augmented_ids.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /tmp/src/merge_augmented_ids.py\n",
    "import polars as pl\n",
    "\n",
    "def drop_if_exists(df: pl.DataFrame, col: str) -> pl.DataFrame:\n",
    "    return df.drop(col) if col in df.columns else df\n",
    "\n",
    "def align_columns(df: pl.DataFrame, target_columns: list[str]) -> pl.DataFrame:\n",
    "    for col in target_columns:\n",
    "        if col not in df.columns:\n",
    "            df = df.with_columns(pl.lit(None).alias(col))\n",
    "    return df.select(target_columns)\n",
    "\n",
    "# Standard column order\n",
    "target_columns = [\"article_id\", \"dataset_id\", \"window\"]\n",
    "\n",
    "# Load base\n",
    "a = pl.read_parquet(\"/tmp/scibert_predictions.parquet\")\n",
    "a = align_columns(a, target_columns)\n",
    "\n",
    "# Load and align broken DOIs\n",
    "b = pl.read_parquet(\"/tmp/addon_broken_dois.parquet\")\n",
    "b = drop_if_exists(b, \"match\")\n",
    "b = align_columns(b, target_columns)\n",
    "\n",
    "# Merge a + b\n",
    "merged_ab = pl.concat([a, b]).unique(subset=[\"article_id\", \"dataset_id\"])\n",
    "merged_ab.write_parquet(\"/tmp/scibert_predictions.parquet\")\n",
    "\n",
    "# Load and align ref-section DOIs\n",
    "c = pl.read_parquet(\"/tmp/addon_ref_priority.parquet\")\n",
    "c = drop_if_exists(c, \"match\")\n",
    "c = align_columns(c, target_columns)\n",
    "\n",
    "# Final merge\n",
    "final = pl.concat([merged_ab, c]).unique(subset=[\"article_id\", \"dataset_id\"])\n",
    "final = final.filter(pl.col(\"window\").is_not_null())\n",
    "final.write_parquet(\"/tmp/scibert_predictions.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ee14aa1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-03T13:08:56.170347Z",
     "iopub.status.busy": "2025-08-03T13:08:56.170155Z",
     "iopub.status.idle": "2025-08-03T13:08:56.176928Z",
     "shell.execute_reply": "2025-08-03T13:08:56.176373Z"
    },
    "papermill": {
     "duration": 0.025541,
     "end_time": "2025-08-03T13:08:56.177989",
     "exception": false,
     "start_time": "2025-08-03T13:08:56.152448",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /tmp/src/llm_validate.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /tmp/src/llm_validate.py\n",
    "import polars as pl\n",
    "import os\n",
    "\n",
    "from helpers import *\n",
    "\n",
    "l = get_logger()\n",
    "\n",
    "SYS_PROMPT_PRIMARY_SECONDARY = \"\"\"\n",
    "You are classifying dataset usage in scientific papers. Given a text snippet containing a dataset identifier, determine:\n",
    "\n",
    "A) Primary - The dataset was created/generated within this study\n",
    "B) Secondary - An existing dataset was reused/reanalyzed in this study\n",
    "\n",
    "Key indicators:\n",
    "\n",
    "PRIMARY (A):\n",
    "- \"we collected\", \"we generated\", \"we sequenced\"\n",
    "- \"deposited\", \"uploaded\", \"submitted\" (by authors)\n",
    "- \"our dataset\", \"newly generated\", \"experimental data\"\n",
    "- Accession numbers for NEW submissions\n",
    "\n",
    "SECONDARY (B):\n",
    "- \"previously published\", \"publicly available\", \"obtained from\"\n",
    "- \"downloaded from\", \"retrieved from\", \"reanalyzed\"  \n",
    "- Author citations (Smith et al.)\n",
    "- \"described by\", \"reported in\"\n",
    "\n",
    "Examples:\n",
    "1) \"RNA-seq data was deposited in GEO under accession GSE123456.\" → A\n",
    "2) \"We reanalyzed data from GSE789012 (Smith et al., 2020).\" → B\n",
    "3) \"Raw sequencing data is available at Zenodo (DOI: 10.5281/zenodo.123).\" → A\n",
    "4) \"Expression data downloaded from ArrayExpress E-MTAB-456.\" → B\n",
    "\n",
    "Output exactly one letter: A or B.\n",
    "\"\"\"\n",
    "\n",
    "SYS_PROMPT_CLASSIFY_ALL = \"\"\"\n",
    "You are a highly accurate dataset identifier classifier. Given a snippet of academic text containing a DOI or accession number, choose:\n",
    "\n",
    "  A) Data — the identifier points directly to research data in a repository  \n",
    "  B) Literature — the identifier points to a journal article, book chapter, protocol paper, or other non-data resource  \n",
    "\n",
    "=== Repository Prefixes (Always DATA) ===\n",
    "DOIs starting with:\n",
    "  • 10.5061 (Dryad)  \n",
    "  • 10.5281 (Zenodo)  \n",
    "  • 10.6084 (Figshare)  \n",
    "  • 10.24433/ (Mendeley Data)  \n",
    "  • 10.17632 (Mendeley Data)  \n",
    "\n",
    "Accession numbers:\n",
    "  • SRA/E- (e.g. SRP, SRA, ERR, DRR)  \n",
    "  • PRJNA, PRJEB, PRJDB (NCBI BioProject)  \n",
    "  • SAMN, SAMEA, SAMD (BioSample)\n",
    "  • GSE, GDS, GPL, GSM (GEO)\n",
    "  • PXD (PRIDE Proteomics)  \n",
    "  • E-MTAB, E-GEOD, E-PROT (ArrayExpress)\n",
    "  • CHEMBL (ChEMBL)\n",
    "  • EMPIAR (Electron Microscopy)\n",
    "  • EPI_ISL, EPI (GISAID)\n",
    "  • HPA (Human Protein Atlas)\n",
    "  • CP (Cell Painting)\n",
    "  • IPR, PF (InterPro/Pfam)\n",
    "  • KX, K0 (KEGG)\n",
    "\n",
    "=== Publisher DOIs (Usually LITERATURE) ===\n",
    "  • 10.1038, 10.1126, 10.1016, 10.1007, 10.1101, 10.1371, 10.1073, etc.\n",
    "\n",
    "=== Few-Shot Examples ===\n",
    "1) \"Raw images are stored on Figshare (DOI 10.6084/m9.figshare.1234567).\" → A  \n",
    "2) \"Sequence reads available under BioProject accession PRJNA765432.\" → A  \n",
    "3) \"As described in Nature Methods (DOI 10.1038/s41592-020-0793-2).\" → B  \n",
    "4) \"See Supplementary Data at Zenodo (10.5281/zenodo.987654).\" → A  \n",
    "5) \"Method details published in J. Proteome Res. DOI: 10.1021/acs.jproteome.0c00845.\" → B  \n",
    "6) \"Data has been uploaded to Dryad (10.5061/dryad.x1y2z3).\" → A  \n",
    "7) \"Referenced paper: DOI 10.1101/2020.01.01.123456 (bioRxiv preprint).\" → B  \n",
    "8) \"Metabolomics data in MetaboLights MTBLS1234.\" → A  \n",
    "9) \"RNA-seq data deposited in GEO under accession GSE123456.\" → A\n",
    "10) \"Protein sequences from CHEMBL4567890 were analyzed.\" → A\n",
    "11) \"Sample metadata available as SAMN12345678.\" → A\n",
    "\n",
    "=== Instructions ===\n",
    "- Use both the identifier and its surrounding context.  \n",
    "- If the identifier matches known data repository patterns above, choose A.  \n",
    "- If it's a publisher DOI (journal article), choose B.  \n",
    "- Look for context clues: \"deposited\", \"uploaded\", \"archived\", \"available at\" → likely A\n",
    "- Look for context clues: \"described in\", \"published in\", \"see methods\" → likely B\n",
    "- Output exactly one letter: A or B, and nothing else.\n",
    "\"\"\".strip()\n",
    "\n",
    "def build_df():\n",
    "    \"\"\"Load extracted data and separate DOIs from accession numbers, but process both\"\"\"\n",
    "    df = pl.read_parquet('/tmp/extracted.parquet')\n",
    "    \n",
    "    # Separate but don't filter - we'll process both types\n",
    "    accession_df = df.filter(~is_doi_link('dataset_id'))\n",
    "    doi_df = df.filter(is_doi_link('dataset_id'))\n",
    "    \n",
    "    # Still write accession subset for backwards compatibility if needed\n",
    "    accession_df.select('article_id', 'dataset_id').write_csv('/tmp/accid_sub.csv')\n",
    "    \n",
    "    # Return the full dataframe for LLM processing\n",
    "    return df\n",
    "\n",
    "def build_prompt(tokenizer, df):\n",
    "    \"\"\"Build prompts for both DOIs and accession numbers\"\"\"\n",
    "    prompts = []\n",
    "    for dataset_id, text in df.select('dataset_id', 'window').rows():\n",
    "        # Enhanced context for the prompt\n",
    "        enhanced_text = f\"Text snippet: {text}\\n\\nIdentifier to classify: {dataset_id}\"\n",
    "        messages = [\n",
    "            {'role': 'system', 'content': SYS_PROMPT_CLASSIFY_ALL}, \n",
    "            {'role': 'user', 'content': enhanced_text}\n",
    "        ]\n",
    "        prompts.append(tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False, enable_thinking=False))\n",
    "    return df.with_columns(pl.Series('prompt', prompts))\n",
    "\n",
    "if __name__=='__main__':\n",
    "    os.environ[\"VLLM_USE_V1\"] = \"0\"\n",
    "    import vllm\n",
    "    from logits_processor_zoo.vllm import MultipleChoiceLogitsProcessor\n",
    "    \n",
    "    model_path = \"/kaggle/input/qwen-3/transformers/32b-awq/1\"\n",
    "    llm = vllm.LLM(model_path, quantization='awq', tensor_parallel_size=2, gpu_memory_utilization=0.9, trust_remote_code=True, dtype=\"half\", enforce_eager=True, max_model_len=2048, disable_log_stats=True, disable_custom_all_reduce=True, enable_prefix_caching=True, task='generate')\n",
    "    tokenizer = llm.get_tokenizer()\n",
    "    \n",
    "    # Process ALL identifiers (both DOIs and accession numbers)\n",
    "    df = build_df()\n",
    "    df = build_prompt(tokenizer, df)\n",
    "    prompts = df['prompt'].to_list()\n",
    "    \n",
    "    mclp = MultipleChoiceLogitsProcessor(tokenizer, choices=[\"A\", \"B\"])\n",
    "    outputs = llm.generate(prompts, vllm.SamplingParams(seed=777, temperature=0.1, skip_special_tokens=True, max_tokens=1, logits_processors=[mclp], logprobs=len(mclp.choices)), use_tqdm=True)\n",
    "    \n",
    "    logprobs = [{lp.decoded_token: lp.logprob for lp in list(lps)} for lps in [output.outputs[0].logprobs[0].values() for output in outputs]]\n",
    "    pred_letters = [max(lpdict, key=lpdict.get) for lpdict in logprobs]  # \"A\" or \"B\"\n",
    "    type_llm = [True if p == \"A\" else False for p in pred_letters]\n",
    "    \n",
    "    # Add all columns to your dataframe\n",
    "    df = df.with_columns([\n",
    "        pl.Series('type_llm', type_llm),\n",
    "        pl.Series('logprob_A', [d.get(\"A\", float('-inf')) for d in logprobs]),\n",
    "        pl.Series('logprob_B', [d.get(\"B\", float('-inf')) for d in logprobs]),\n",
    "        pl.Series('pred_letter', pred_letters)\n",
    "    ])\n",
    "    \n",
    "    # Save full result for hybrid work (now includes both DOIs and accessions)\n",
    "    df.select(['article_id', 'dataset_id', 'type_llm', 'logprob_A', 'logprob_B']).write_csv('/kaggle/working/submission_llm_logprobs.csv')\n",
    "\n",
    "    # Filter for data predictions (A)\n",
    "    choices = [max(d, key=d.get) for d in logprobs]\n",
    "    types = {'A': True, 'B': False}\n",
    "    choices = [types[c] for c in choices]\n",
    "    df = df.with_columns(pl.Series('type', choices))\n",
    "    \n",
    "    # Get all data predictions (both DOIs and accessions)\n",
    "    data_predictions = df.filter(pl.col('type'))\n",
    "    \n",
    "    # Apply type classification and write final submission\n",
    "    df_final = assume_type(data_predictions)\n",
    "    df_final.select(['article_id', 'dataset_id', 'type']).with_row_index(name='row_id').write_csv('/kaggle/working/submission.csv')\n",
    "    \n",
    "    # Evaluation (if not submission)\n",
    "    if not IS_KAGGLE_SUBMISSION:\n",
    "        results = evaluate(df_final)\n",
    "        for r in results: l.info(r) \n",
    "        results = evaluate(df_final, on=['article_id', 'dataset_id', 'type'])\n",
    "        for r in results: l.info(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62747930",
   "metadata": {
    "_cell_guid": "af30506a-e45e-4a3b-ba81-23dd154bde8d",
    "_uuid": "94fc4779-0b43-4058-ac64-f54a7ebd8a76",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-08-03T13:08:56.213478Z",
     "iopub.status.busy": "2025-08-03T13:08:56.213316Z",
     "iopub.status.idle": "2025-08-03T13:08:56.218815Z",
     "shell.execute_reply": "2025-08-03T13:08:56.218284Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.024517,
     "end_time": "2025-08-03T13:08:56.219962",
     "exception": false,
     "start_time": "2025-08-03T13:08:56.195445",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /tmp/src/llm_validate-bak.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /tmp/src/llm_validate-bak.py\n",
    "import polars as pl\n",
    "import os\n",
    "\n",
    "from helpers import *\n",
    "\n",
    "l = get_logger()\n",
    "\n",
    "SYS_PROMPT_CLASSIFY_DOI = \"\"\"\n",
    "You are a highly accurate DOI/type classifier. Given a snippet of academic text containing a DOI or accession, choose:\n",
    "\n",
    "  A) Data — the identifier points directly to research data in a repository  \n",
    "  B) Literature — the identifier points to a journal article, book chapter, protocol paper, or other non-data resource  \n",
    "\n",
    "=== Repository Prefixes ===\n",
    "Treat as DATA if the DOI starts with any of:\n",
    "  • 10.5061 (Dryad)  \n",
    "  • 10.5281 (Zenodo)  \n",
    "  • 10.6084 (Figshare)  \n",
    "  • 10.24433/ (Mendeley Data)  \n",
    "  • 10.17632 (Mendeley Data)  \n",
    "  • SRA/E- (e.g. SRP, SRA)  \n",
    "  • PRJNA, PRJEB, PRJDB (NCBI BioProject)  \n",
    "  • PRIDE:PXD (Proteomics)  \n",
    "  • EMBL:E-MTAB, E- (ArrayExpress)  \n",
    "\n",
    "Everything else is LITERATURE unless you see explicit data-repository context (e.g. “deposited in Dryad under DOI…”).\n",
    "\n",
    "=== Few-Shot Examples ===\n",
    "1) “Raw images are stored on Figshare (DOI 10.6084/m9.figshare.1234567).” → A  \n",
    "2) “Sequence reads available under BioProject accession PRJNA765432.” → A  \n",
    "3) “As described in Nature Methods (DOI 10.1038/s41592-020-0793-2).” → B  \n",
    "4) “See Supplementary Data at Zenodo (10.5281/zenodo.987654).” → A  \n",
    "5) “Method details published in J. Proteome Res. DOI: 10.1021/acs.jproteome.0c00845.” → B  \n",
    "6) “Data has been uploaded to Dryad (10.5061/dryad.x1y2z3).” → A  \n",
    "7) “Referenced paper: DOI 10.1101/2020.01.01.123456 (bioRxiv preprint).” → B  \n",
    "8) “Metabolomics data in MetaboLights MTBLS1234.” → A  \n",
    "\n",
    "=== Instructions ===\n",
    "- Use only the identifier itself and its context.  \n",
    "- If the DOI prefix is in the list above, always choose A.  \n",
    "- If it belongs to a known publisher prefix (e.g. 10.1007, 10.1038, 10.1126, 10.1016…), choose B.  \n",
    "- Otherwise, rely on context words (“deposited”, “uploaded”, “archived”) to decide.  \n",
    "- Output exactly one letter: A or B, and nothing else.\n",
    "\"\"\".strip()\n",
    "\n",
    "def build_df():\n",
    "    df = pl.read_parquet('/tmp/extracted.parquet')\n",
    "    df.filter(~is_doi_link('dataset_id')).select('article_id', 'dataset_id').write_csv('/tmp/accid_sub.csv')\n",
    "    return df.filter(is_doi_link('dataset_id'))\n",
    "\n",
    "def build_prompt(tokenizer, df):\n",
    "    prompts = []\n",
    "    for doi, text in df.select('dataset_id', 'window').rows():\n",
    "        messages = [{'role':'system','content': SYS_PROMPT_CLASSIFY_DOI}, {'role':'user', 'content': text}]\n",
    "        prompts.append(tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False, enable_thinking=False))\n",
    "    return df.with_columns(pl.Series('prompt', prompts))\n",
    "\n",
    "if __name__=='__main__':\n",
    "    os.environ[\"VLLM_USE_V1\"] = \"0\"\n",
    "    import vllm\n",
    "    from logits_processor_zoo.vllm import MultipleChoiceLogitsProcessor\n",
    "    model_path = \"/kaggle/input/qwen-3/transformers/32b-awq/1\"\n",
    "    llm = vllm.LLM(model_path, quantization='awq', tensor_parallel_size=2, gpu_memory_utilization=0.9, trust_remote_code=True, dtype=\"half\", enforce_eager=True, max_model_len=2048, disable_log_stats=True, disable_custom_all_reduce=True, enable_prefix_caching=True, task='generate')\n",
    "    tokenizer = llm.get_tokenizer()\n",
    "    df = build_df()\n",
    "    df = build_prompt(tokenizer, df)\n",
    "    prompts = df['prompt'].to_list()\n",
    "    mclp = MultipleChoiceLogitsProcessor(tokenizer, choices=[\"A\", \"B\"])\n",
    "    outputs = llm.generate(prompts, vllm.SamplingParams(seed=777, temperature=0.1, skip_special_tokens=True, max_tokens=1, logits_processors=[mclp], logprobs=len(mclp.choices)), use_tqdm=True)\n",
    "    logprobs = [{lp.decoded_token: lp.logprob for lp in list(lps)} for lps in [output.outputs[0].logprobs[0].values() for output in outputs]]\n",
    "    pred_letters = [max(lpdict, key=lpdict.get) for lpdict in logprobs]  # \"A\" or \"B\"\n",
    "    type_llm = [True if p == \"A\" else False for p in pred_letters]\n",
    "    \n",
    "    # Add all columns to your dataframe\n",
    "    df = df.with_columns([\n",
    "        pl.Series('type_llm', type_llm),\n",
    "        pl.Series('logprob_A', [d.get(\"A\", float('-inf')) for d in logprobs]),\n",
    "        pl.Series('logprob_B', [d.get(\"B\", float('-inf')) for d in logprobs]),\n",
    "        pl.Series('pred_letter', pred_letters)\n",
    "    ])\n",
    "    \n",
    "    # Save full result for hybrid work\n",
    "    df.select(['article_id', 'dataset_id', 'type_llm', 'logprob_A', 'logprob_B']).write_csv('/kaggle/working/submission_llm_logprobs.csv')\n",
    "\n",
    "    choices = [max(d, key=d.get) for d in logprobs]\n",
    "    types = {'A': True, 'B': False}\n",
    "    choices = [types[c] for c in choices]\n",
    "    df = df.with_columns(pl.Series('type', choices))\n",
    "    df.filter(pl.col('type')).select('article_id', 'dataset_id').write_csv('/tmp/doi_sub.csv')\n",
    "    df = pl.concat([pl.read_csv('/tmp/doi_sub.csv'), pl.read_csv('/tmp/accid_sub.csv')])\n",
    "    df = assume_type(df)\n",
    "    df.select(['article_id', 'dataset_id', 'type']).with_row_index(name='row_id').write_csv('/kaggle/working/submission.csv')\n",
    "    if not IS_KAGGLE_SUBMISSION:\n",
    "        results = evaluate(df)\n",
    "        for r in results: l.info(r) \n",
    "        results = evaluate(df, on=['article_id', 'dataset_id', 'type'])\n",
    "        for r in results: l.info(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e9b85d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-03T13:08:56.254437Z",
     "iopub.status.busy": "2025-08-03T13:08:56.254247Z",
     "iopub.status.idle": "2025-08-03T13:08:56.258569Z",
     "shell.execute_reply": "2025-08-03T13:08:56.258053Z"
    },
    "papermill": {
     "duration": 0.022619,
     "end_time": "2025-08-03T13:08:56.259559",
     "exception": false,
     "start_time": "2025-08-03T13:08:56.236940",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /tmp/src/post_filter.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /tmp/src/post_filter.py\n",
    "import polars as pl\n",
    "from helpers import *\n",
    "\n",
    "\"\"\"\n",
    "Fourth essence: Post-filter to cut FP DOIs that look like literature.\n",
    "- Read /kaggle/working/submission.csv (output of llm_validate.py)\n",
    "- Join with /tmp/extracted.parquet to get context window\n",
    "- Drop DOI rows that (1) start with typical publisher prefixes AND (2) have no data-ish words nearby\n",
    "- Keep accessions untouched\n",
    "\"\"\"\n",
    "\n",
    "l = get_logger()\n",
    "\n",
    "PAPER_PREFIXES = [\n",
    "    \"10.1007\", \"10.1002\", \"10.1016\", \"10.1021\", \"10.1038\", \"10.1056\",\n",
    "    \"10.1073\", \"10.1080\", \"10.1093\", \"10.1101\", \"10.1186\", \"10.1371\",\n",
    "    \"10.1111\", \"10.5194\", \"10.3390\", \"10.1126\"\n",
    "]\n",
    "\n",
    "CONTEXT_RE = r\"(?i)\\b(data(?:set)?|repository|archive|deposited|available|supplementary|raw(?:\\s+data)?|uploaded|hosted|stored|accession)\\b\"\n",
    "\n",
    "def is_paper_prefix(col: str = \"dataset_id\") -> pl.Expr:\n",
    "    expr = pl.lit(False)\n",
    "    for p in PAPER_PREFIXES:\n",
    "        expr = expr | pl.col(col).str.starts_with(f\"{DOI_LINK}{p}\")\n",
    "    return expr\n",
    "\n",
    "def main():\n",
    "    sub = pl.read_csv(\"/kaggle/working/submission.csv\")\n",
    "\n",
    "    # Normalize columns: drop row_id if present so concat widths match\n",
    "    if \"row_id\" in sub.columns:\n",
    "        sub = sub.drop(\"row_id\")\n",
    "\n",
    "    # Context windows\n",
    "    win = pl.read_parquet(\"/tmp/extracted.parquet\").select(\"article_id\", \"dataset_id\", \"window\")\n",
    "\n",
    "    # DOI & ACC split\n",
    "    doi_rows = sub.filter(is_doi_link(\"dataset_id\")).join(win, on=[\"article_id\", \"dataset_id\"], how=\"left\")\n",
    "    acc_rows = sub.filter(~is_doi_link(\"dataset_id\"))\n",
    "\n",
    "    keep_mask = (\n",
    "        (~is_paper_prefix(\"dataset_id\"))  # not a known paper prefix\n",
    "        | doi_rows[\"window\"].fill_null(\"\").str.contains(CONTEXT_RE)\n",
    "    )\n",
    "\n",
    "    kept_doi = doi_rows.filter(keep_mask).select(\"article_id\", \"dataset_id\", \"type\")\n",
    "    final = pl.concat([kept_doi, acc_rows.select(\"article_id\", \"dataset_id\", \"type\")])\n",
    "\n",
    "    # Re-eval & save\n",
    "    if not IS_KAGGLE_SUBMISSION:\n",
    "        for r in evaluate(final): l.info(r)\n",
    "        for r in evaluate(final, on=[\"article_id\", \"dataset_id\", \"type\"]): l.info(r)\n",
    "\n",
    "    final.with_row_index(\"row_id\").write_csv(\"/kaggle/working/submission.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03d32097",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-03T13:08:56.337481Z",
     "iopub.status.busy": "2025-08-03T13:08:56.337239Z",
     "iopub.status.idle": "2025-08-03T13:08:56.342177Z",
     "shell.execute_reply": "2025-08-03T13:08:56.341515Z"
    },
    "papermill": {
     "duration": 0.066268,
     "end_time": "2025-08-03T13:08:56.343242",
     "exception": false,
     "start_time": "2025-08-03T13:08:56.276974",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /tmp/src/post_filter_scibert.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /tmp/src/post_filter_scibert.py\n",
    "import polars as pl\n",
    "from helpers import *\n",
    "\n",
    "\"\"\"\n",
    "Fourth essence: Post-filter to cut FP DOIs that look like literature.\n",
    "- Read /kaggle/working/submission.csv (output of llm_validate.py)\n",
    "- Join with /tmp/extracted.parquet to get context window\n",
    "- Drop DOI rows that (1) start with typical publisher prefixes AND (2) have no data-ish words nearby\n",
    "- Keep accessions untouched\n",
    "\"\"\"\n",
    "\n",
    "l = get_logger()\n",
    "\n",
    "PAPER_PREFIXES = [\n",
    "    \"10.1007\", \"10.1002\", \"10.1016\", \"10.1021\", \"10.1038\", \"10.1056\",\n",
    "    \"10.1073\", \"10.1080\", \"10.1093\", \"10.1101\", \"10.1186\", \"10.1371\",\n",
    "    \"10.1111\", \"10.5194\", \"10.3390\", \"10.1126\"\n",
    "]\n",
    "\n",
    "CONTEXT_RE = r\"(?i)\\b(data(?:set)?|repository|archive|deposited|available|supplementary|raw(?:\\s+data)?|uploaded|hosted|stored|accession)\\b\"\n",
    "\n",
    "def is_paper_prefix(col: str = \"dataset_id\") -> pl.Expr:\n",
    "    expr = pl.lit(False)\n",
    "    for p in PAPER_PREFIXES:\n",
    "        expr = expr | pl.col(col).str.starts_with(f\"{DOI_LINK}{p}\")\n",
    "    return expr\n",
    "\n",
    "def main():\n",
    "    sub = pl.read_csv(\"/kaggle/working/submission.csv\")\n",
    "\n",
    "    # Normalize columns: drop row_id if present so concat widths match\n",
    "    if \"row_id\" in sub.columns:\n",
    "        sub = sub.drop(\"row_id\")\n",
    "\n",
    "    # Context windows\n",
    "    win = pl.read_parquet(\"/tmp/scibert_predictions.parquet\").select(\"article_id\", \"dataset_id\", \"window\",'r_type')\n",
    "\n",
    "    # DOI & ACC split\n",
    "    doi_rows = sub.filter(is_doi_link(\"dataset_id\")).join(win, on=[\"article_id\", \"dataset_id\"], how=\"left\")\n",
    "    acc_rows = sub.filter(~is_doi_link(\"dataset_id\"))\n",
    "\n",
    "    keep_mask = (\n",
    "        (~is_paper_prefix(\"dataset_id\"))  # not a known paper prefix\n",
    "        | doi_rows[\"window\"].fill_null(\"\").str.contains(CONTEXT_RE)\n",
    "    )\n",
    "\n",
    "    kept_doi = doi_rows.filter(keep_mask).select(\"article_id\", \"dataset_id\", \"r_type\", 'type')\n",
    "    acc_rows = acc_rows.join(win, on=[\"article_id\", \"dataset_id\"], how=\"left\")\n",
    "    final = pl.concat([kept_doi, acc_rows.select(\"article_id\", \"dataset_id\", \"r_type\", 'type')])\n",
    "    \n",
    "    # Replace 'type' with 'r_type' for submission\n",
    "    final = final.drop(\"type\").rename({\"r_type\": \"type\"})\n",
    "    \n",
    "    # Re-eval & save\n",
    "    if not IS_KAGGLE_SUBMISSION:\n",
    "        for r in evaluate(final): l.info(r)\n",
    "        for r in evaluate(final, on=[\"article_id\", \"dataset_id\", \"type\"]): l.info(r)\n",
    "    \n",
    "    final.with_row_index(\"row_id\").write_csv(\"/kaggle/working/submission_scibert.csv\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8acd178f",
   "metadata": {
    "_cell_guid": "c6a12705-737a-4b21-9bf2-125b3d1ab724",
    "_kg_hide-output": true,
    "_uuid": "bc8e0d68-3097-4ce9-99a1-536921913550",
    "execution": {
     "iopub.execute_input": "2025-08-03T13:08:56.379842Z",
     "iopub.status.busy": "2025-08-03T13:08:56.379275Z",
     "iopub.status.idle": "2025-08-03T13:09:03.878686Z",
     "shell.execute_reply": "2025-08-03T13:09:03.877658Z"
    },
    "papermill": {
     "duration": 7.51905,
     "end_time": "2025-08-03T13:09:03.880051",
     "exception": false,
     "start_time": "2025-08-03T13:08:56.361001",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp\n"
     ]
    }
   ],
   "source": [
    "%cd /tmp\n",
    "! LOG_LEVEL=INFO python src/parse.py /tmp/train_parse\n",
    "! python src/check_parse.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51e958a5",
   "metadata": {
    "_cell_guid": "c6a12705-737a-4b21-9bf2-125b3d1ab724",
    "_kg_hide-output": true,
    "_uuid": "bc8e0d68-3097-4ce9-99a1-536921913550",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-08-03T13:09:03.916121Z",
     "iopub.status.busy": "2025-08-03T13:09:03.915892Z",
     "iopub.status.idle": "2025-08-03T13:09:05.516829Z",
     "shell.execute_reply": "2025-08-03T13:09:05.516070Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 1.619859,
     "end_time": "2025-08-03T13:09:05.518129",
     "exception": false,
     "start_time": "2025-08-03T13:09:03.898270",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\r\n",
      "  File \"/tmp/src/getid.py\", line 196, in <module>\r\n",
      "    if __name__=='__main__': main()\r\n",
      "                             ^^^^^^\r\n",
      "  File \"/tmp/src/getid.py\", line 188, in main\r\n",
      "    df = assume_type(df)\r\n",
      "         ^^^^^^^^^^^^^^^\r\n",
      "  File \"/tmp/src/helpers.py\", line 99, in assume_type\r\n",
      "    types = [classify_type(w, d) for w, d in df.select(['window', 'dataset_id']).rows()]\r\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "  File \"/tmp/src/helpers.py\", line 99, in <listcomp>\r\n",
      "    types = [classify_type(w, d) for w, d in df.select(['window', 'dataset_id']).rows()]\r\n",
      "             ^^^^^^^^^^^^^^^^^^^\r\n",
      "  File \"/tmp/src/helpers.py\", line 75, in classify_type\r\n",
      "    if pd.isna(window):\r\n",
      "       ^^\r\n",
      "NameError: name 'pd' is not defined. Did you mean: 'pl'?\r\n"
     ]
    }
   ],
   "source": [
    "! python src/getid.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5fcabd96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-03T13:09:05.554372Z",
     "iopub.status.busy": "2025-08-03T13:09:05.554144Z",
     "iopub.status.idle": "2025-08-03T13:09:05.560883Z",
     "shell.execute_reply": "2025-08-03T13:09:05.560042Z"
    },
    "papermill": {
     "duration": 0.026824,
     "end_time": "2025-08-03T13:09:05.562710",
     "exception": false,
     "start_time": "2025-08-03T13:09:05.535886",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /tmp/src/SciBert_train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /tmp/src/SciBert_train.py\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    TrainerCallback,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "# Load data from parquet and CSV\n",
    "extracted_df = pd.read_parquet(\"/tmp/extracted.parquet\")\n",
    "labels_df = pd.read_csv(\"/kaggle/input/make-data-count-finding-data-references/train_labels.csv\")\n",
    "\n",
    "# Normalize keys for join\n",
    "labels_df[\"article_id\"] = labels_df[\"article_id\"].str.replace(\"/\", \"_\")\n",
    "labels_df[\"dataset_id\"] = labels_df[\"dataset_id\"].str.lower()\n",
    "\n",
    "extracted_df[\"dataset_id\"] = extracted_df[\"dataset_id\"].str.lower()\n",
    "\n",
    "# Merge and drop rows without labels\n",
    "df = extracted_df.merge(labels_df, on=[\"article_id\", \"dataset_id\"], how=\"inner\")\n",
    "df = df.dropna(subset=[\"window\", \"type\"])\n",
    "\n",
    "# Label encoding\n",
    "type_to_label = {\"Primary\": 0, \"Secondary\": 1} #, \"Missing\": 2}\n",
    "df[\"label\"] = df[\"type\"].map(type_to_label)\n",
    "\n",
    "# Train/val split\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df[\"window\"].tolist(),\n",
    "    df[\"label\"].tolist(),\n",
    "    test_size=0.2,\n",
    "    stratify=df[\"label\"],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Tokenization\n",
    "model_name = \"allenai/scibert_scivocab_uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=512)\n",
    "\n",
    "# Dataset wrapper\n",
    "class DataRelationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = DataRelationDataset(train_encodings, train_labels)\n",
    "val_dataset = DataRelationDataset(val_encodings, val_labels)\n",
    "\n",
    "# Model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2, \n",
    "                                                           hidden_dropout_prob=0.3,      # Default is usually 0.1\n",
    "                                                           attention_probs_dropout_prob=0.3,  # Default is usually 0.1\n",
    ")\n",
    "\n",
    "# Metrics\n",
    "def compute_metrics(p):    \n",
    "    preds = p.predictions.argmax(-1)\n",
    "    f1_micro = f1_score(p.label_ids, preds, average='micro')\n",
    "    f1_macro = f1_score(p.label_ids, preds, average='macro')\n",
    "    report = classification_report(p.label_ids, preds, output_dict=True, zero_division=0)\n",
    "\n",
    "    def safe_f1(class_label):\n",
    "        return report[str(class_label)][\"f1-score\"] if str(class_label) in report else 0.0\n",
    "\n",
    "    return {\n",
    "        \"f1_micro\": f1_micro,\n",
    "        \"f1_macro\": f1_macro,\n",
    "        \"f1_primary\": safe_f1(0),\n",
    "        \"f1_secondary\": safe_f1(1),\n",
    "        \"f1_missing\": safe_f1(2)\n",
    "    }\n",
    "\n",
    "# Training setup\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/kaggle/working/results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=101,\n",
    "    learning_rate=.5e-5,\n",
    "    weight_decay=0.15, # Increase from 0.01 to 0.1 or even 0.2\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_f1_macro\",\n",
    "    greater_is_better=True,\n",
    "    save_total_limit = 4,\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "class PrintDevicesCallback(TrainerCallback):\n",
    "    def on_train_begin(self, args, state, control, **kwargs):\n",
    "        print(f\"Training on {torch.cuda.device_count()} GPUs:\", [torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())])\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer.train()\n",
    "model.save_pretrained(\"/kaggle/working/best_model\")\n",
    "tokenizer.save_pretrained(\"/kaggle/working/best_model\")\n",
    "! zip -r /kaggle/working/output-tbt-2L.zip /kaggle/working/best_model\n",
    "! cd /kaggle/working\n",
    "from IPython.display import FileLink\n",
    "FileLink(r'output-tbt-2L.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "438cd387",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-03T13:09:05.602138Z",
     "iopub.status.busy": "2025-08-03T13:09:05.601939Z",
     "iopub.status.idle": "2025-08-03T13:09:47.870941Z",
     "shell.execute_reply": "2025-08-03T13:09:47.869996Z"
    },
    "papermill": {
     "duration": 42.288062,
     "end_time": "2025-08-03T13:09:47.872156",
     "exception": false,
     "start_time": "2025-08-03T13:09:05.584094",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-03 13:09:27.434898: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1754226567.772960      19 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1754226567.869828      19 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "100%|██████████| 3/3 [00:00<00:00,  7.62it/s]\n"
     ]
    }
   ],
   "source": [
    "# %%writefile /tmp/src/SciBert_infer.py\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "# 🧠 Load model and tokenizer\n",
    "model_path = \"/kaggle/input/btb-2l-scibert/kaggle/working/best_model\"\n",
    "token_path = model_path\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(token_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# 🔮 Predict type\n",
    "label_map = {0: \"Primary\", 1: \"Secondary\",} # 2: 'Missing'}\n",
    "batch_size = 8\n",
    "preds = []\n",
    "rows = pd.read_parquet(\"/tmp/extracted.parquet\")\n",
    "\n",
    "# Drop NA windows\n",
    "rows = rows.dropna(subset=[\"window\"]).reset_index(drop=True)\n",
    "\n",
    "logits_list = []\n",
    "\n",
    "for i in tqdm(range(0, len(rows), batch_size)):\n",
    "    batch_texts = rows[\"window\"].iloc[i:i+batch_size].tolist()\n",
    "    enc = tokenizer(batch_texts, truncation=True, padding=True, max_length=512, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**enc).logits\n",
    "        p = torch.argmax(logits, dim=1).cpu().tolist()\n",
    "        preds.extend(p)\n",
    "        logits_list.extend(logits.cpu().tolist())\n",
    "\n",
    "\n",
    "logits_df = pd.DataFrame(logits_list, columns=[\"logit_primary\", \"logit_secondary\"])\n",
    "rows = pd.concat([rows.reset_index(drop=True), logits_df], axis=1)\n",
    "label_map = {0: \"Primary\", 1: \"Secondary\"}\n",
    "\n",
    "# preds is a list of ints from argmax\n",
    "rows[\"pred_label\"] = [label_map[p] for p in preds]\n",
    "\n",
    "# Then export\n",
    "rows[[\"article_id\", \"dataset_id\", \"pred_label\", \"logit_primary\", \"logit_secondary\"]].to_csv(\n",
    "    \"/kaggle/working/submission_scibert_logits.csv\", index=False\n",
    ")\n",
    "rows[[\"article_id\", \"dataset_id\", \"pred_label\", \"logit_primary\", \"logit_secondary\"]].to_csv(\"/kaggle/working/submission_scibert_logits.csv\", index=False)\n",
    "rows[\"r_type\"] = [label_map.get(p, \"Missing\") for p in preds]\n",
    "rows[[\"article_id\", \"dataset_id\", 'window',\"r_type\"]].to_parquet(\"/tmp/scibert_predictions.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bbc5e7c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-03T13:09:47.909798Z",
     "iopub.status.busy": "2025-08-03T13:09:47.909137Z",
     "iopub.status.idle": "2025-08-03T13:09:48.717424Z",
     "shell.execute_reply": "2025-08-03T13:09:48.716696Z"
    },
    "papermill": {
     "duration": 0.828279,
     "end_time": "2025-08-03T13:09:48.718748",
     "exception": false,
     "start_time": "2025-08-03T13:09:47.890469",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\r\n",
      "  File \"/tmp/src/post_filter_scibert.py\", line 63, in <module>\r\n",
      "    main()\r\n",
      "  File \"/tmp/src/post_filter_scibert.py\", line 29, in main\r\n",
      "    sub = pl.read_csv(\"/kaggle/working/submission.csv\")\r\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/polars/_utils/deprecation.py\", line 92, in wrapper\r\n",
      "    return function(*args, **kwargs)\r\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/polars/_utils/deprecation.py\", line 92, in wrapper\r\n",
      "    return function(*args, **kwargs)\r\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/polars/_utils/deprecation.py\", line 92, in wrapper\r\n",
      "    return function(*args, **kwargs)\r\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/polars/io/csv/functions.py\", line 534, in read_csv\r\n",
      "    df = _read_csv_impl(\r\n",
      "         ^^^^^^^^^^^^^^^\r\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/polars/io/csv/functions.py\", line 682, in _read_csv_impl\r\n",
      "    pydf = PyDataFrame.read_csv(\r\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\r\n",
      "FileNotFoundError: No such file or directory (os error 2): /kaggle/working/submission.csv\r\n"
     ]
    }
   ],
   "source": [
    "! python src/post_filter_scibert.py\n",
    "#! rm /kaggle/working/submission.csv\n",
    "#! mv /kaggle/working/submission_scibert.csv /kaggle/working/submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e1f0dab1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-03T13:09:48.756136Z",
     "iopub.status.busy": "2025-08-03T13:09:48.755454Z",
     "iopub.status.idle": "2025-08-03T13:09:48.759158Z",
     "shell.execute_reply": "2025-08-03T13:09:48.758444Z"
    },
    "papermill": {
     "duration": 0.023037,
     "end_time": "2025-08-03T13:09:48.760319",
     "exception": false,
     "start_time": "2025-08-03T13:09:48.737282",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#! python src/addon_fix_broken_dois.py\n",
    "#! python src/addon_reference_priority.py\n",
    "#! python src/merge_augmented_ids.py  # or insert the merging logic inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8f28395a",
   "metadata": {
    "_cell_guid": "c6a12705-737a-4b21-9bf2-125b3d1ab724",
    "_kg_hide-output": true,
    "_uuid": "bc8e0d68-3097-4ce9-99a1-536921913550",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-08-03T13:09:48.795769Z",
     "iopub.status.busy": "2025-08-03T13:09:48.795544Z",
     "iopub.status.idle": "2025-08-03T13:14:49.170205Z",
     "shell.execute_reply": "2025-08-03T13:14:49.169387Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 300.393685,
     "end_time": "2025-08-03T13:14:49.171586",
     "exception": false,
     "start_time": "2025-08-03T13:09:48.777901",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-03 13:09:54.398109: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1754226594.420327     181 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1754226594.427004     181 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "INFO 08-03 13:09:58 [__init__.py:244] Automatically detected platform cuda.\r\n",
      "INFO 08-03 13:10:15 [config.py:1472] Using max model len 2048\r\n",
      "WARNING 08-03 13:10:15 [config.py:960] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\r\n",
      "WARNING 08-03 13:10:16 [cuda.py:102] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\r\n",
      "INFO 08-03 13:10:16 [llm_engine.py:230] Initializing a V0 LLM engine (v0.9.2) with config: model='/kaggle/input/qwen-3/transformers/32b-awq/1', speculative_config=None, tokenizer='/kaggle/input/qwen-3/transformers/32b-awq/1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=awq, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='xgrammar', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=None, served_model_name=/kaggle/input/qwen-3/transformers/32b-awq/1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=False, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":false,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":0,\"local_cache_dir\":null}, use_cached_outputs=False, \r\n",
      "WARNING 08-03 13:10:16 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 2 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=209)\u001b[0;0m INFO 08-03 13:10:16 [multiproc_worker_utils.py:226] Worker ready; awaiting tasks\r\n",
      "INFO 08-03 13:10:17 [cuda.py:311] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 08-03 13:10:17 [cuda.py:360] Using XFormers backend.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=209)\u001b[0;0m INFO 08-03 13:10:17 [cuda.py:311] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=209)\u001b[0;0m INFO 08-03 13:10:17 [cuda.py:360] Using XFormers backend.\r\n",
      "[W803 13:10:28.684078027 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W803 13:10:28.185065803 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W803 13:10:38.694632920 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W803 13:10:48.705066066 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=209)\u001b[0;0m INFO 08-03 13:10:48 [__init__.py:1152] Found nccl from library libnccl.so.2\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=209)\u001b[0;0m INFO 08-03 13:10:48 [pynccl.py:70] vLLM is using nccl==2.26.2\r\n",
      "INFO 08-03 13:10:48 [__init__.py:1152] Found nccl from library libnccl.so.2\r\n",
      "INFO 08-03 13:10:48 [pynccl.py:70] vLLM is using nccl==2.26.2\r\n",
      "INFO 08-03 13:10:48 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_a467073f'), local_subscribe_addr='ipc:///tmp/3fcff002-6d65-4be1-8186-cd03df1fba52', remote_subscribe_addr=None, remote_addr_ipv6=False)\r\n",
      "INFO 08-03 13:10:48 [parallel_state.py:1076] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=209)\u001b[0;0m INFO 08-03 13:10:48 [parallel_state.py:1076] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\r\n",
      "INFO 08-03 13:10:48 [model_runner.py:1171] Starting to load model /kaggle/input/qwen-3/transformers/32b-awq/1...\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=209)\u001b[0;0m INFO 08-03 13:10:49 [model_runner.py:1171] Starting to load model /kaggle/input/qwen-3/transformers/32b-awq/1...\r\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\r\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:53<02:40, 53.60s/it]\r\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [01:39<01:38, 49.36s/it]\r\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [02:20<00:45, 45.18s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [03:15<00:00, 49.09s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [03:15<00:00, 48.82s/it]\r\n",
      "\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=209)\u001b[0;0m INFO 08-03 13:14:04 [default_loader.py:272] Loading weights took 194.98 seconds\r\n",
      "INFO 08-03 13:14:05 [default_loader.py:272] Loading weights took 195.68 seconds\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=209)\u001b[0;0m INFO 08-03 13:14:05 [model_runner.py:1203] Model loading took 9.0570 GiB and 195.317381 seconds\r\n",
      "INFO 08-03 13:14:06 [model_runner.py:1203] Model loading took 9.0570 GiB and 196.026460 seconds\r\n",
      "\r\n",
      "\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=209)\u001b[0;0m INFO 08-03 13:14:18 [worker.py:294] model weights take 9.06GiB; non_torch_memory takes 0.11GiB; PyTorch activation peak memory takes 0.41GiB; the rest of the memory reserved for KV Cache is 3.70GiB.\r\n",
      "\r\n",
      "\r\n",
      "INFO 08-03 13:14:18 [worker.py:294] model weights take 9.06GiB; non_torch_memory takes 0.11GiB; PyTorch activation peak memory takes 1.40GiB; the rest of the memory reserved for KV Cache is 2.70GiB.\r\n",
      "INFO 08-03 13:14:18 [executor_base.py:113] # cuda blocks: 1381, # CPU blocks: 2048\r\n",
      "INFO 08-03 13:14:18 [executor_base.py:118] Maximum concurrency for 2048 tokens per request: 10.79x\r\n",
      "INFO 08-03 13:14:23 [llm_engine.py:428] init engine (profile, create kv cache, warmup model) took 17.06 seconds\r\n",
      "Adding requests: 100%|█████████████████████████| 21/21 [00:00<00:00, 450.65it/s]\r\n",
      "Processed prompts: 100%|█| 21/21 [00:14<00:00,  1.42it/s, est. speed input: 1439\r\n",
      "[rank0]: Traceback (most recent call last):\r\n",
      "[rank0]:   File \"/tmp/src/llm_validate.py\", line 160, in <module>\r\n",
      "[rank0]:     df_final = assume_type(data_predictions)\r\n",
      "[rank0]:                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "[rank0]:   File \"/tmp/src/helpers.py\", line 99, in assume_type\r\n",
      "[rank0]:     types = [classify_type(w, d) for w, d in df.select(['window', 'dataset_id']).rows()]\r\n",
      "[rank0]:             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "[rank0]:   File \"/tmp/src/helpers.py\", line 99, in <listcomp>\r\n",
      "[rank0]:     types = [classify_type(w, d) for w, d in df.select(['window', 'dataset_id']).rows()]\r\n",
      "[rank0]:              ^^^^^^^^^^^^^^^^^^^\r\n",
      "[rank0]:   File \"/tmp/src/helpers.py\", line 75, in classify_type\r\n",
      "[rank0]:     if pd.isna(window):\r\n",
      "[rank0]:        ^^\r\n",
      "[rank0]: NameError: name 'pd' is not defined. Did you mean: 'pl'?\r\n",
      "[rank0]:[W803 13:14:41.026598632 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/tmp/src/post_filter.py\", line 58, in <module>\r\n",
      "    main()\r\n",
      "  File \"/tmp/src/post_filter.py\", line 29, in main\r\n",
      "    sub = pl.read_csv(\"/kaggle/working/submission.csv\")\r\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/polars/_utils/deprecation.py\", line 92, in wrapper\r\n",
      "    return function(*args, **kwargs)\r\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/polars/_utils/deprecation.py\", line 92, in wrapper\r\n",
      "    return function(*args, **kwargs)\r\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/polars/_utils/deprecation.py\", line 92, in wrapper\r\n",
      "    return function(*args, **kwargs)\r\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/polars/io/csv/functions.py\", line 534, in read_csv\r\n",
      "    df = _read_csv_impl(\r\n",
      "         ^^^^^^^^^^^^^^^\r\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/polars/io/csv/functions.py\", line 682, in _read_csv_impl\r\n",
      "    pydf = PyDataFrame.read_csv(\r\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\r\n",
      "FileNotFoundError: No such file or directory (os error 2): /kaggle/working/submission.csv\r\n"
     ]
    }
   ],
   "source": [
    "! python src/llm_validate.py\n",
    "! python src/post_filter.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "38e2cf0a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-03T13:14:49.211882Z",
     "iopub.status.busy": "2025-08-03T13:14:49.211593Z",
     "iopub.status.idle": "2025-08-03T13:14:49.301307Z",
     "shell.execute_reply": "2025-08-03T13:14:49.300549Z"
    },
    "papermill": {
     "duration": 0.110651,
     "end_time": "2025-08-03T13:14:49.302425",
     "exception": false,
     "start_time": "2025-08-03T13:14:49.191774",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n",
      "  has_large_values = (abs_vals > 1e6).any()\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n",
      "  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n",
      "  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>dataset_id</th>\n",
       "      <th>type_truth</th>\n",
       "      <th>type_llm</th>\n",
       "      <th>logprob_A</th>\n",
       "      <th>logprob_B</th>\n",
       "      <th>type_scibert</th>\n",
       "      <th>logit_primary</th>\n",
       "      <th>logit_secondary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.1002_2017jc013030</td>\n",
       "      <td>https://doi.org/10.17882/49388</td>\n",
       "      <td>Primary</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.1002_ece3.4466</td>\n",
       "      <td>https://doi.org/10.5061/dryad.r6nq870</td>\n",
       "      <td>Primary</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-204.424896</td>\n",
       "      <td>Primary</td>\n",
       "      <td>2.372571</td>\n",
       "      <td>-1.902298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.1002_ece3.5260</td>\n",
       "      <td>https://doi.org/10.5061/dryad.2f62927</td>\n",
       "      <td>Primary</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-199.736267</td>\n",
       "      <td>Primary</td>\n",
       "      <td>2.089813</td>\n",
       "      <td>-1.519837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.1002_ece3.6144</td>\n",
       "      <td>https://doi.org/10.5061/dryad.zw3r22854</td>\n",
       "      <td>Primary</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-193.484772</td>\n",
       "      <td>Primary</td>\n",
       "      <td>2.192287</td>\n",
       "      <td>-1.654931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.1002_ece3.6303</td>\n",
       "      <td>https://doi.org/10.5061/dryad.37pvmcvgb</td>\n",
       "      <td>Primary</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-190.984161</td>\n",
       "      <td>Primary</td>\n",
       "      <td>2.379811</td>\n",
       "      <td>-1.916410</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             article_id                               dataset_id type_truth  \\\n",
       "0  10.1002_2017jc013030           https://doi.org/10.17882/49388    Primary   \n",
       "1     10.1002_ece3.4466    https://doi.org/10.5061/dryad.r6nq870    Primary   \n",
       "2     10.1002_ece3.5260    https://doi.org/10.5061/dryad.2f62927    Primary   \n",
       "3     10.1002_ece3.6144  https://doi.org/10.5061/dryad.zw3r22854    Primary   \n",
       "4     10.1002_ece3.6303  https://doi.org/10.5061/dryad.37pvmcvgb    Primary   \n",
       "\n",
       "  type_llm  logprob_A   logprob_B type_scibert  logit_primary  logit_secondary  \n",
       "0      NaN        NaN         NaN          NaN            NaN              NaN  \n",
       "1     True        0.0 -204.424896      Primary       2.372571        -1.902298  \n",
       "2     True        0.0 -199.736267      Primary       2.089813        -1.519837  \n",
       "3     True        0.0 -193.484772      Primary       2.192287        -1.654931  \n",
       "4     True        0.0 -190.984161      Primary       2.379811        -1.916410  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%writefile /tmp/src/HybridProbs.py\n",
    "import pandas as pd\n",
    "\n",
    "# Load everything\n",
    "truth = pd.read_csv(\"/kaggle/input/make-data-count-finding-data-references/train_labels.csv\")\n",
    "truth = truth[truth[\"type\"] != \"Missing\"].copy()\n",
    "llm = pd.read_csv(\"/kaggle/working/submission_llm_logprobs.csv\").rename(columns={\"type\": \"type_llm\"})\n",
    "scibert = pd.read_csv(\"/kaggle/working/submission_scibert_logits.csv\").rename(columns={\"pred_label\": \"type_scibert\"})\n",
    "# Ensure no trailing slashes, lowercase, and consistent formats\n",
    "truth[\"article_id\"] = truth[\"article_id\"].astype(str).str.strip()\n",
    "truth[\"dataset_id\"] = truth[\"dataset_id\"].astype(str).str.strip().str.lower()\n",
    "\n",
    "llm[\"article_id\"] = llm[\"article_id\"].astype(str).str.strip()\n",
    "llm[\"dataset_id\"] = llm[\"dataset_id\"].astype(str).str.strip().str.lower()\n",
    "\n",
    "scibert[\"article_id\"] = scibert[\"article_id\"].astype(str).str.strip()\n",
    "scibert[\"dataset_id\"] = scibert[\"dataset_id\"].astype(str).str.strip().str.lower()\n",
    "\n",
    "# Merge\n",
    "df = (\n",
    "    truth.rename(columns={\"type\": \"type_truth\"})\n",
    "    .merge(llm, on=[\"article_id\", \"dataset_id\"], how=\"left\")\n",
    "    .merge(scibert, on=[\"article_id\", \"dataset_id\"], how=\"left\")\n",
    ")\n",
    "\n",
    "# Save\n",
    "df.to_csv(\"/kaggle/working/merged_predictions_with_confidence.csv\", index=False)\n",
    "\n",
    "# Preview\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8faa249c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-03T13:14:49.343307Z",
     "iopub.status.busy": "2025-08-03T13:14:49.342862Z",
     "iopub.status.idle": "2025-08-03T13:14:49.345670Z",
     "shell.execute_reply": "2025-08-03T13:14:49.345144Z"
    },
    "papermill": {
     "duration": 0.024025,
     "end_time": "2025-08-03T13:14:49.346702",
     "exception": false,
     "start_time": "2025-08-03T13:14:49.322677",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#! python src/HybridProbs.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e94e8cd8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-03T13:14:49.387856Z",
     "iopub.status.busy": "2025-08-03T13:14:49.387439Z",
     "iopub.status.idle": "2025-08-03T13:14:49.422368Z",
     "shell.execute_reply": "2025-08-03T13:14:49.421708Z"
    },
    "papermill": {
     "duration": 0.057139,
     "end_time": "2025-08-03T13:14:49.423354",
     "exception": false,
     "start_time": "2025-08-03T13:14:49.366215",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "truth total: 719\n",
      "truth ∩ llm: 8\n",
      "truth ∩ scibert: 8\n",
      "Missing in LLM: [('10.1136_jitc-2021-003114', 'cvcl_1919'), ('10.5194_essd-2023-198', 'https://doi.org/10.17882/94032'), ('10.1016_j.jlp.2022.104761', 'https://doi.org/10.23642/usn.15134442.v1'), ('10.1371_journal.pone.0212669', 'e-geod-48780'), ('10.1038_s41396-020-00885-8', 'ipr004963')]\n",
      "Missing in SciBERT: [('10.1136_jitc-2021-003114', 'cvcl_1919'), ('10.5194_essd-2023-198', 'https://doi.org/10.17882/94032'), ('10.1016_j.jlp.2022.104761', 'https://doi.org/10.23642/usn.15134442.v1'), ('10.1371_journal.pone.0212669', 'e-geod-48780'), ('10.1038_s41396-020-00885-8', 'ipr004963')]\n"
     ]
    }
   ],
   "source": [
    "#%%writefile /tmp/src/HybridProbsMerge.py\n",
    "\n",
    "truth_keys = set(zip(truth[\"article_id\"], truth[\"dataset_id\"]))\n",
    "llm_keys = set(zip(llm[\"article_id\"], llm[\"dataset_id\"]))\n",
    "scibert_keys = set(zip(scibert[\"article_id\"], scibert[\"dataset_id\"]))\n",
    "\n",
    "print(\"truth total:\", len(truth_keys))\n",
    "print(\"truth ∩ llm:\", len(truth_keys & llm_keys))\n",
    "print(\"truth ∩ scibert:\", len(truth_keys & scibert_keys))\n",
    "missing_in_llm = truth_keys - llm_keys\n",
    "missing_in_scibert = truth_keys - scibert_keys\n",
    "\n",
    "print(\"Missing in LLM:\", list(missing_in_llm)[:5])\n",
    "print(\"Missing in SciBERT:\", list(missing_in_scibert)[:5])\n",
    "import numpy as np\n",
    "\n",
    "def softmax_probs_llm(logA, logB):\n",
    "    logits = np.array([logA, logB])\n",
    "    probs = np.exp(logits - np.max(logits))  # logit-stabilized\n",
    "    probs /= probs.sum()\n",
    "    return probs[0], probs[1]  # P(A), P(B)\n",
    "llm_df = llm\n",
    "llm_df[\"llm_prob_A\"], llm_df[\"llm_prob_B\"] = zip(*llm_df.apply(\n",
    "    lambda row: softmax_probs_llm(row[\"logprob_A\"], row[\"logprob_B\"]),\n",
    "    axis=1\n",
    "))\n",
    "llm_df[\"llm_margin\"] = abs(llm_df[\"llm_prob_A\"] - llm_df[\"llm_prob_B\"])\n",
    "scibert_df = scibert.copy()\n",
    "import numpy as np\n",
    "\n",
    "def softmax_probs_scibert_2class(logit_primary, logit_secondary):\n",
    "    logits = np.array([logit_primary, logit_secondary])\n",
    "    exps = np.exp(logits - np.max(logits))  # for numerical stability\n",
    "    return exps / exps.sum()\n",
    "\n",
    "# Apply row-wise\n",
    "scibert_df[[\"scibert_Primary\", \"scibert_Secondary\"]] = scibert_df.apply(\n",
    "    lambda row: pd.Series(softmax_probs_scibert_2class(row[\"logit_primary\"], row[\"logit_secondary\"])),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Compute margin\n",
    "scibert_df[\"scibert_margin\"] = abs(scibert_df[\"scibert_Primary\"] - scibert_df[\"scibert_Secondary\"])\n",
    "merged_df = df.copy()\n",
    "merged_df = merged_df.merge(llm_df[[\"article_id\", \"dataset_id\", \"llm_prob_A\", \"llm_prob_B\", \"llm_margin\"]], on=[\"article_id\", \"dataset_id\"], how=\"left\")\n",
    "merged_df = merged_df.merge(scibert_df[[\"article_id\", \"dataset_id\", \"scibert_Primary\", \"scibert_Secondary\", \"scibert_margin\"]], on=[\"article_id\", \"dataset_id\"], how=\"left\")\n",
    "def choose_type(row):\n",
    "    if row[\"llm_margin\"] >= 0.6:\n",
    "        return row[\"type_llm\"]\n",
    "    elif row[\"scibert_margin\"] >= 0.3:\n",
    "        return row[\"type_scibert\"]\n",
    "    else:\n",
    "        return row[\"type_llm\"]  # fallback if both are unsure\n",
    "merged_df[\"type_hybrid\"] = merged_df.apply(choose_type, axis=1)\n",
    "\n",
    "##### OR \n",
    "def combine_probs(row, alpha=0.6):\n",
    "    # LLM is binary: A=Primary, B=Secondary\n",
    "    p_primary = alpha * row[\"llm_prob_A\"] + (1 - alpha) * row[\"scibert_Primary\"]\n",
    "    p_secondary = alpha * row[\"llm_prob_B\"] + (1 - alpha) * row[\"scibert_Secondary\"]\n",
    "    return \"Primary\" if p_primary > p_secondary else \"Secondary\"\n",
    "\n",
    "# merged_df[\"type_hybrid\"] = merged_df.apply(combine_probs, axis=1)\n",
    "merged_df.to_csv(\"/kaggle/working/merged_predictions_with_confidence.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "13220cb7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-03T13:14:49.463226Z",
     "iopub.status.busy": "2025-08-03T13:14:49.462815Z",
     "iopub.status.idle": "2025-08-03T13:14:49.468312Z",
     "shell.execute_reply": "2025-08-03T13:14:49.467606Z"
    },
    "papermill": {
     "duration": 0.026542,
     "end_time": "2025-08-03T13:14:49.469320",
     "exception": false,
     "start_time": "2025-08-03T13:14:49.442778",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /tmp/src/nn_train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /tmp/src/nn_train.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load inputs\n",
    "llm_df = pd.read_csv(\"/kaggle/working/submission_llm_logprobs.csv\")\n",
    "scibert_df = pd.read_csv(\"/kaggle/working/submission_scibert_logits.csv\")\n",
    "extracted_df = pd.read_parquet(\"/tmp/extracted.parquet\")\n",
    "\n",
    "# Merge and keep only non-Missing\n",
    "df = (\n",
    "    extracted_df.merge(llm_df, on=[\"article_id\", \"dataset_id\"], how=\"left\")\n",
    "               .merge(scibert_df, on=[\"article_id\", \"dataset_id\"], how=\"left\")\n",
    ")\n",
    "df = df[df[\"pred_label\"] != \"Missing\"].reset_index(drop=True)\n",
    "\n",
    "# 🔠 Sentence embeddings\n",
    "embedder = SentenceTransformer(\"/kaggle/input/all-minilm-l6-v2-and-deps/all-MiniLM-L6-v2/\")\n",
    "embed_window = embedder.encode(df[\"window\"].tolist(), batch_size=64, show_progress_bar=True)\n",
    "embed_dsid = embedder.encode(df[\"dataset_id\"].tolist(), batch_size=64, show_progress_bar=True)\n",
    "\n",
    "# 🔢 One-hot encoding of types\n",
    "llm_onehot = pd.get_dummies(df[\"type_llm\"], prefix=\"llm\")\n",
    "scibert_onehot = pd.get_dummies(df[\"pred_label\"], prefix=\"scibert\")\n",
    "llm_onehot = llm_onehot.reindex(columns=[\"llm_Primary\", \"llm_Secondary\"], fill_value=0)\n",
    "scibert_onehot = scibert_onehot.reindex(columns=[\"scibert_Primary\", \"scibert_Secondary\"], fill_value=0)\n",
    "\n",
    "# 🧠 Construct training matrix\n",
    "X = np.hstack([\n",
    "    embed_window,\n",
    "    embed_dsid,\n",
    "    df[[\"logit_primary\", \"logit_secondary\"]].values,\n",
    "    df[[\"logprob_A\", \"logprob_B\"]].values,\n",
    "    llm_onehot.values,\n",
    "    scibert_onehot.values\n",
    "])\n",
    "\n",
    "# 🎯 Labels (0=Primary, 1=Secondary)\n",
    "label_map = {\"Primary\": 0, \"Secondary\": 1}\n",
    "y = df[\"pred_label\"].map(label_map).values\n",
    "\n",
    "# 🧽 Normalize\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# 🔀 Train/val split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# 🧱 Dataset and dataloader\n",
    "train_ds = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train))\n",
    "val_ds = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val))\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=64)\n",
    "\n",
    "# 🧠 Define model\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 2)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Classifier(input_dim=X.shape[1]).to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=.5e-5)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# 🔁 Training loop\n",
    "best_f1 = 0.0\n",
    "for epoch in range(1, 5000):\n",
    "    model.train()\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        opt.zero_grad()\n",
    "        loss = loss_fn(model(xb), yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "    model.eval()\n",
    "    preds, true = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb = xb.to(device)\n",
    "            logits = model(xb)\n",
    "            p = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            preds.extend(p)\n",
    "            true.extend(yb.numpy())\n",
    "\n",
    "    f1 = f1_score(true, preds, average=\"macro\")\n",
    "    print(f\"Epoch {epoch+1}: Val F1_macro={f1:.4f}, Loss={loss.item():.3f}\")\n",
    "    if f1 > best_f1:\n",
    "        print(\"✅ New best model saved!\")\n",
    "        torch.save(model, \"/kaggle/working/best_embed_mlp.pt\")\n",
    "        best_f1 = f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5f73b539",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-03T13:14:49.509337Z",
     "iopub.status.busy": "2025-08-03T13:14:49.509116Z",
     "iopub.status.idle": "2025-08-03T13:15:00.754545Z",
     "shell.execute_reply": "2025-08-03T13:15:00.753981Z"
    },
    "papermill": {
     "duration": 11.267446,
     "end_time": "2025-08-03T13:15:00.755916",
     "exception": false,
     "start_time": "2025-08-03T13:14:49.488470",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ff6c0b57e0e4ac1ae0c720e6c04fc97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f061f1047a5d4caa81563ab256b44370",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Primary       1.00      0.25      0.40         4\n",
      "   Secondary       0.25      1.00      0.40         1\n",
      "\n",
      "    accuracy                           0.40         5\n",
      "   macro avg       0.62      0.62      0.40         5\n",
      "weighted avg       0.85      0.40      0.40         5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def load_and_merge_data():\n",
    "    \"\"\"Load and merge all data sources\"\"\"\n",
    "    try:\n",
    "        llm_df = pd.read_csv(\"/kaggle/working/submission_llm_logprobs.csv\")\n",
    "        scibert_df = pd.read_csv(\"/kaggle/working/submission_scibert_logits.csv\")\n",
    "        extracted_df = pd.read_parquet(\"/tmp/extracted.parquet\")\n",
    "        \n",
    "        # Merge datasets\n",
    "        df = (\n",
    "            extracted_df.merge(llm_df, on=[\"article_id\", \"dataset_id\"], how=\"left\")\n",
    "                       .merge(scibert_df, on=[\"article_id\", \"dataset_id\"], how=\"left\")\n",
    "        )\n",
    "        \n",
    "        # Filter out Missing labels\n",
    "        df = df[df[\"pred_label\"] != \"Missing\"].reset_index(drop=True)\n",
    "        logger.info(f\"Loaded {len(df)} samples after filtering\")\n",
    "        \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading data: {e}\")\n",
    "        raise\n",
    "\n",
    "def create_embeddings(df, embedder_path=\"/kaggle/input/all-minilm-l6-v2-and-deps/all-MiniLM-L6-v2/\"):\n",
    "    \"\"\"Generate sentence embeddings for window and dataset_id\"\"\"\n",
    "    try:\n",
    "        embedder = SentenceTransformer(embedder_path)\n",
    "        \n",
    "        logger.info(\"Generating window embeddings...\")\n",
    "        embed_window = embedder.encode(\n",
    "            df[\"window\"].tolist(), \n",
    "            batch_size=64, \n",
    "            show_progress_bar=True,\n",
    "            convert_to_numpy=True\n",
    "        )\n",
    "        \n",
    "        logger.info(\"Generating dataset_id embeddings...\")\n",
    "        embed_dsid = embedder.encode(\n",
    "            df[\"dataset_id\"].tolist(), \n",
    "            batch_size=64, \n",
    "            show_progress_bar=True,\n",
    "            convert_to_numpy=True\n",
    "        )\n",
    "        \n",
    "        return embed_window, embed_dsid\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error creating embeddings: {e}\")\n",
    "        raise\n",
    "\n",
    "def prepare_features(df, embed_window, embed_dsid):\n",
    "    \"\"\"Prepare feature matrix and labels\"\"\"\n",
    "    # One-hot encodings with consistent columns\n",
    "    llm_onehot = pd.get_dummies(df[\"type_llm\"], prefix=\"llm\")\n",
    "    scibert_onehot = pd.get_dummies(df[\"pred_label\"], prefix=\"scibert\")\n",
    "    \n",
    "    # Ensure consistent columns\n",
    "    for col in [\"llm_Primary\", \"llm_Secondary\"]:\n",
    "        if col not in llm_onehot.columns:\n",
    "            llm_onehot[col] = 0\n",
    "    \n",
    "    for col in [\"scibert_Primary\", \"scibert_Secondary\"]:\n",
    "        if col not in scibert_onehot.columns:\n",
    "            scibert_onehot[col] = 0\n",
    "    \n",
    "    llm_onehot = llm_onehot[[\"llm_Primary\", \"llm_Secondary\"]]\n",
    "    scibert_onehot = scibert_onehot[[\"scibert_Primary\", \"scibert_Secondary\"]]\n",
    "    \n",
    "    # Construct feature matrix\n",
    "    X = np.hstack([\n",
    "        embed_window,\n",
    "        embed_dsid,\n",
    "        df[[\"logit_primary\", \"logit_secondary\"]].values,\n",
    "        df[[\"logprob_A\", \"logprob_B\"]].values,\n",
    "        llm_onehot.values,\n",
    "        scibert_onehot.values\n",
    "    ])\n",
    "    \n",
    "    # Labels\n",
    "    label_map = {\"Primary\": 0, \"Secondary\": 1}\n",
    "    y = df[\"pred_label\"].map(label_map).values\n",
    "    \n",
    "    logger.info(f\"Feature matrix shape: {X.shape}\")\n",
    "    logger.info(f\"Label distribution: {np.bincount(y)}\")\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    \"\"\"Improved classifier with configurable architecture\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim=128, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.BatchNorm1d(hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, 2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "def train_epoch(model, train_loader, optimizer, loss_fn, device):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(xb)\n",
    "        loss = loss_fn(logits, yb)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "    \n",
    "    return total_loss / num_batches\n",
    "\n",
    "def evaluate(model, val_loader, device):\n",
    "    \"\"\"Evaluate model on validation set\"\"\"\n",
    "    model.eval()\n",
    "    preds, true = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb = xb.to(device)\n",
    "            logits = model(xb)\n",
    "            p = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            preds.extend(p)\n",
    "            true.extend(yb.numpy())\n",
    "    \n",
    "    f1 = f1_score(true, preds, average=\"macro\")\n",
    "    return f1, preds, true\n",
    "\n",
    "def main():\n",
    "    # Set random seeds for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Load data\n",
    "    df = load_and_merge_data()\n",
    "    \n",
    "    # Create embeddings\n",
    "    embed_window, embed_dsid = create_embeddings(df)\n",
    "    \n",
    "    # Prepare features\n",
    "    X, y = prepare_features(df, embed_window, embed_dsid)\n",
    "    \n",
    "    # Normalize features\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    \n",
    "    # Train/validation split\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=0.2, stratify=y, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Create datasets and dataloaders\n",
    "    train_ds = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train))\n",
    "    val_ds = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val))\n",
    "    \n",
    "    train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=64)\n",
    "    \n",
    "    # Model setup\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    logger.info(f\"Using device: {device}\")\n",
    "    \n",
    "    model = Classifier(input_dim=X.shape[1]).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='max', factor=0.5, patience=10\n",
    "    )\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Training loop\n",
    "    best_f1 = 0.0\n",
    "    patience_counter = 0\n",
    "    max_patience = 50\n",
    "    \n",
    "    logger.info(\"Starting training...\")\n",
    "    \n",
    "    for epoch in range(1000):  # Reduced max epochs\n",
    "        # Train\n",
    "        avg_loss = train_epoch(model, train_loader, optimizer, loss_fn, device)\n",
    "        \n",
    "        # Evaluate\n",
    "        val_f1, preds, true = evaluate(model, val_loader, device)\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(val_f1)\n",
    "        \n",
    "        # Logging\n",
    "        if epoch % 10 == 0:\n",
    "            logger.info(f\"Epoch {epoch}: Val F1={val_f1:.4f}, Loss={avg_loss:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            patience_counter = 0\n",
    "            \n",
    "            # Save model\n",
    "            save_path = \"/kaggle/working/best_embed_mlp.pt\"\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'scaler': scaler,\n",
    "                'best_f1': best_f1,\n",
    "                'epoch': epoch\n",
    "            }, save_path)\n",
    "            \n",
    "            logger.info(f\"✅ New best model saved! F1: {best_f1:.4f}\")\n",
    "            \n",
    "            # Print detailed classification report\n",
    "            if epoch % 50 == 0:\n",
    "                print(\"\\nClassification Report:\")\n",
    "                print(classification_report(true, preds, target_names=[\"Primary\", \"Secondary\"]))\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        # Early stopping\n",
    "        if patience_counter >= max_patience:\n",
    "            logger.info(f\"Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "    \n",
    "    logger.info(f\"Training completed. Best F1: {best_f1:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "261fbd1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-03T13:15:00.797468Z",
     "iopub.status.busy": "2025-08-03T13:15:00.797239Z",
     "iopub.status.idle": "2025-08-03T13:15:01.055712Z",
     "shell.execute_reply": "2025-08-03T13:15:01.054828Z"
    },
    "papermill": {
     "duration": 0.280243,
     "end_time": "2025-08-03T13:15:01.056889",
     "exception": false,
     "start_time": "2025-08-03T13:15:00.776646",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2022b4e60ba141749de066262acd13e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ba35e454ca44b3c99188bdc7d28ab7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 5 rows of submission:\n",
      "   row_id              article_id                              dataset_id  \\\n",
      "0       0       10.1002_ecs2.4619         https://doi.org/10.25349/d9qw5x   \n",
      "1       1       10.1002_ecs2.4619  PASTA/D835832D7FD00D9E4466E44EEA87FAB3   \n",
      "2       2       10.1002_ece3.4466   https://doi.org/10.5061/dryad.r6nq870   \n",
      "3       3  10.1002_anie.201916483  https://doi.org/10.1002/ange.201916483   \n",
      "4       4       10.1002_ece3.5395  https://doi.org/10.5441/001/1.71r7pp6q   \n",
      "\n",
      "        type  \n",
      "0    Primary  \n",
      "1    Primary  \n",
      "2    Primary  \n",
      "3    Primary  \n",
      "4  Secondary  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Paths\n",
    "LLM_PATH = \"/kaggle/working/submission_llm_logprobs.csv\"\n",
    "SCIBERT_PATH = \"/kaggle/working/submission_scibert_logits.csv\"\n",
    "EXTRACTED_PATH = \"/tmp/extracted.parquet\"\n",
    "MODEL_PATH = \"/kaggle/working/best_embed_mlp.pt\"\n",
    "OUTPUT_PATH = \"/kaggle/working/submission_nn.csv\"\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    \"\"\"Improved classifier matching training code\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim=128, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.BatchNorm1d(hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, 2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "def load_and_merge_data():\n",
    "    \"\"\"Load and merge all data sources\"\"\"\n",
    "    try:\n",
    "        llm = pd.read_csv(LLM_PATH)\n",
    "        scibert = pd.read_csv(SCIBERT_PATH)\n",
    "        extracted = pd.read_parquet(EXTRACTED_PATH)\n",
    "        \n",
    "        # Merge datasets\n",
    "        df = (\n",
    "            extracted.merge(llm, on=[\"article_id\", \"dataset_id\"], how=\"inner\")\n",
    "                     .merge(scibert, on=[\"article_id\", \"dataset_id\"], how=\"inner\")\n",
    "        )\n",
    "        \n",
    "        # Filter out 'Missing'\n",
    "        df = df[df[\"pred_label\"] != \"Missing\"].reset_index(drop=True)\n",
    "        logger.info(f\"Loaded {len(df)} samples for inference\")\n",
    "        \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading data: {e}\")\n",
    "        raise\n",
    "\n",
    "def create_embeddings(df, embedder_path=\"/kaggle/input/all-minilm-l6-v2-and-deps/all-MiniLM-L6-v2/\"):\n",
    "    \"\"\"Generate sentence embeddings for window and dataset_id\"\"\"\n",
    "    try:\n",
    "        embedder = SentenceTransformer(embedder_path)\n",
    "        \n",
    "        logger.info(\"Generating window embeddings...\")\n",
    "        embed_window = embedder.encode(\n",
    "            df[\"window\"].tolist(), \n",
    "            batch_size=64, \n",
    "            show_progress_bar=True,\n",
    "            convert_to_numpy=True\n",
    "        )\n",
    "        \n",
    "        logger.info(\"Generating dataset_id embeddings...\")\n",
    "        embed_dsid = embedder.encode(\n",
    "            df[\"dataset_id\"].tolist(), \n",
    "            batch_size=64, \n",
    "            show_progress_bar=True,\n",
    "            convert_to_numpy=True\n",
    "        )\n",
    "        \n",
    "        return embed_window, embed_dsid\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error creating embeddings: {e}\")\n",
    "        raise\n",
    "\n",
    "def prepare_features(df, embed_window, embed_dsid):\n",
    "    \"\"\"Prepare feature matrix - must match training exactly\"\"\"\n",
    "    # One-hot encodings with consistent columns\n",
    "    llm_onehot = pd.get_dummies(df[\"type_llm\"], prefix=\"llm\")\n",
    "    scibert_onehot = pd.get_dummies(df[\"pred_label\"], prefix=\"scibert\")\n",
    "    \n",
    "    # Ensure consistent columns (same as training)\n",
    "    for col in [\"llm_Primary\", \"llm_Secondary\"]:\n",
    "        if col not in llm_onehot.columns:\n",
    "            llm_onehot[col] = 0\n",
    "    \n",
    "    for col in [\"scibert_Primary\", \"scibert_Secondary\"]:\n",
    "        if col not in scibert_onehot.columns:\n",
    "            scibert_onehot[col] = 0\n",
    "    \n",
    "    llm_onehot = llm_onehot[[\"llm_Primary\", \"llm_Secondary\"]]\n",
    "    scibert_onehot = scibert_onehot[[\"scibert_Primary\", \"scibert_Secondary\"]]\n",
    "    \n",
    "    # Construct feature matrix (same order as training)\n",
    "    X = np.hstack([\n",
    "        embed_window,\n",
    "        embed_dsid,\n",
    "        df[[\"logit_primary\", \"logit_secondary\"]].values,\n",
    "        df[[\"logprob_A\", \"logprob_B\"]].values,\n",
    "        llm_onehot.values,\n",
    "        scibert_onehot.values\n",
    "    ])\n",
    "    \n",
    "    logger.info(f\"Feature matrix shape: {X.shape}\")\n",
    "    return X\n",
    "\n",
    "def load_model_and_scaler():\n",
    "    \"\"\"Load trained model and scaler\"\"\"\n",
    "    try:\n",
    "        # Load saved checkpoint\n",
    "        checkpoint = torch.load(MODEL_PATH, map_location='cpu', weights_only=False)\n",
    "        \n",
    "        # Check if it's the new format (with scaler) or old format\n",
    "        if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:\n",
    "            # New format\n",
    "            scaler = checkpoint['scaler']\n",
    "            model_state = checkpoint['model_state_dict']\n",
    "            logger.info(f\"Loaded model with F1: {checkpoint.get('best_f1', 'unknown')}\")\n",
    "        else:\n",
    "            # Old format - model only\n",
    "            model_state = checkpoint.state_dict() if hasattr(checkpoint, 'state_dict') else None\n",
    "            scaler = None\n",
    "            logger.warning(\"Old model format detected - scaler not saved with model\")\n",
    "        \n",
    "        return model_state, scaler\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading model: {e}\")\n",
    "        raise\n",
    "\n",
    "def main():\n",
    "    # Load data\n",
    "    df = load_and_merge_data()\n",
    "    \n",
    "    # Create embeddings\n",
    "    embed_window, embed_dsid = create_embeddings(df)\n",
    "    \n",
    "    # Prepare features\n",
    "    X = prepare_features(df, embed_window, embed_dsid)\n",
    "    \n",
    "    # Load model and scaler\n",
    "    model_state, saved_scaler = load_model_and_scaler()\n",
    "    \n",
    "    # Normalize features\n",
    "    if saved_scaler is not None:\n",
    "        # Use saved scaler from training\n",
    "        logger.info(\"Using saved scaler from training\")\n",
    "        X = saved_scaler.transform(X)\n",
    "    else:\n",
    "        # Fallback: fit new scaler (not ideal for inference)\n",
    "        logger.warning(\"No saved scaler found - fitting new scaler (may cause issues)\")\n",
    "        scaler = StandardScaler()\n",
    "        X = scaler.fit_transform(X)\n",
    "    \n",
    "    # Setup device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    logger.info(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load model\n",
    "    model = Classifier(input_dim=X.shape[1])\n",
    "    \n",
    "    if model_state is not None:\n",
    "        model.load_state_dict(model_state)\n",
    "    else:\n",
    "        # Fallback for old format\n",
    "        model = torch.load(MODEL_PATH, map_location=device, weights_only=False)\n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    logger.info(\"Running inference...\")\n",
    "    \n",
    "    # Predict in batches to handle large datasets\n",
    "    batch_size = 1000\n",
    "    all_preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(X), batch_size):\n",
    "            batch_X = X[i:i+batch_size]\n",
    "            inputs = torch.tensor(batch_X, dtype=torch.float32).to(device)\n",
    "            logits = model(inputs)\n",
    "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "    \n",
    "    # Map predictions\n",
    "    id2label = {0: \"Primary\", 1: \"Secondary\"}\n",
    "    df[\"type\"] = [id2label[p] for p in all_preds]\n",
    "    \n",
    "    # Log prediction distribution\n",
    "    pred_counts = df[\"type\"].value_counts()\n",
    "    logger.info(f\"Prediction distribution: {pred_counts.to_dict()}\")\n",
    "    \n",
    "    # Create submission\n",
    "    submission = df[[\"article_id\", \"dataset_id\", \"type\"]].copy()\n",
    "    submission.insert(0, \"row_id\", submission.index)\n",
    "    \n",
    "    # Save submission\n",
    "    submission.to_csv(OUTPUT_PATH, index=False)\n",
    "    logger.info(f\"✅ Submission saved to {OUTPUT_PATH}\")\n",
    "    logger.info(f\"Submission shape: {submission.shape}\")\n",
    "    \n",
    "    # Display first few rows\n",
    "    print(\"\\nFirst 5 rows of submission:\")\n",
    "    print(submission.head())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e63cb93b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-03T13:15:01.099261Z",
     "iopub.status.busy": "2025-08-03T13:15:01.099066Z",
     "iopub.status.idle": "2025-08-03T13:15:01.104468Z",
     "shell.execute_reply": "2025-08-03T13:15:01.103743Z"
    },
    "papermill": {
     "duration": 0.027415,
     "end_time": "2025-08-03T13:15:01.105707",
     "exception": false,
     "start_time": "2025-08-03T13:15:01.078292",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /tmp/src/nn_infer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /tmp/src/nn_infer.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Paths\n",
    "LLM_PATH = \"/kaggle/working/submission_llm_logprobs.csv\"\n",
    "SCIBERT_PATH = \"/kaggle/working/submission_scibert_logits.csv\"\n",
    "EXTRACTED_PATH = \"/tmp/extracted.parquet\"\n",
    "MODEL_PATH = \"/kaggle/working/best_embed_mlp.pt\"\n",
    "OUTPUT_PATH = \"/kaggle/working/submission_nn.csv\"\n",
    "\n",
    "# Load data\n",
    "llm = pd.read_csv(LLM_PATH)\n",
    "scibert = pd.read_csv(SCIBERT_PATH)\n",
    "extracted = pd.read_parquet(EXTRACTED_PATH)\n",
    "\n",
    "# Merge\n",
    "df = (\n",
    "    extracted.merge(llm, on=[\"article_id\", \"dataset_id\"], how=\"inner\")\n",
    "             .merge(scibert, on=[\"article_id\", \"dataset_id\"], how=\"inner\")\n",
    ")\n",
    "\n",
    "# Filter out 'Missing'\n",
    "df = df[df[\"pred_label\"] != \"Missing\"].reset_index(drop=True)\n",
    "\n",
    "# Load embedder\n",
    "embedder = SentenceTransformer(\"/kaggle/input/all-minilm-l6-v2-and-deps/all-MiniLM-L6-v2/\")\n",
    "embed_window = embedder.encode(df[\"window\"].tolist(), batch_size=64, show_progress_bar=True)\n",
    "embed_dsid = embedder.encode(df[\"dataset_id\"].tolist(), batch_size=64, show_progress_bar=True)\n",
    "\n",
    "# One-hot types\n",
    "llm_onehot = pd.get_dummies(df[\"type_llm\"], prefix=\"llm\")\n",
    "scibert_onehot = pd.get_dummies(df[\"pred_label\"], prefix=\"scibert\")\n",
    "llm_onehot = llm_onehot.reindex(columns=[\"llm_Primary\", \"llm_Secondary\"], fill_value=0)\n",
    "scibert_onehot = scibert_onehot.reindex(columns=[\"scibert_Primary\", \"scibert_Secondary\"], fill_value=0)\n",
    "\n",
    "# Features\n",
    "X = np.hstack([\n",
    "    embed_window,\n",
    "    embed_dsid,\n",
    "    df[[\"logit_primary\", \"logit_secondary\"]].values,\n",
    "    df[[\"logprob_A\", \"logprob_B\"]].values,\n",
    "    llm_onehot.values,\n",
    "    scibert_onehot.values\n",
    "])\n",
    "\n",
    "# Normalize (same transform as training)\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# 🧠 Define model\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 2)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Classifier(input_dim=X.shape[1]).to(device)\n",
    "model = torch.load(MODEL_PATH, weights_only=False)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Predict\n",
    "with torch.no_grad():\n",
    "    inputs = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "    logits = model(inputs)\n",
    "    preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "\n",
    "# Map predictions\n",
    "id2label = {0: \"Primary\", 1: \"Secondary\"}\n",
    "df[\"type\"] = [id2label[p] for p in preds]\n",
    "\n",
    "# Create submission\n",
    "submission = df[[\"article_id\", \"dataset_id\", \"type\"]].copy()\n",
    "submission.insert(0, \"row_id\", submission.index)\n",
    "submission.to_csv(OUTPUT_PATH, index=False)\n",
    "\n",
    "print(f\"✅ Submission saved to {OUTPUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "997ed95a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-03T13:15:01.146385Z",
     "iopub.status.busy": "2025-08-03T13:15:01.146012Z",
     "iopub.status.idle": "2025-08-03T13:15:01.150489Z",
     "shell.execute_reply": "2025-08-03T13:15:01.149929Z"
    },
    "papermill": {
     "duration": 0.025727,
     "end_time": "2025-08-03T13:15:01.151531",
     "exception": false,
     "start_time": "2025-08-03T13:15:01.125804",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /tmp/src/Failures.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /tmp/src/Failures.py\n",
    "import polars as pl\n",
    "\n",
    "# Load predictions\n",
    "pred_df = pl.read_csv(\"/kaggle/working/submission_nn.csv\")\n",
    "\n",
    "# Load official labels\n",
    "truth_df = pl.read_csv(\"/kaggle/input/make-data-count-finding-data-references/train_labels.csv\")\n",
    "\n",
    "# Rename truth type for disambiguation\n",
    "truth_df = truth_df.rename({\"type\": \"type_truth\"})\n",
    "\n",
    "# Full outer join on article_id and dataset_id\n",
    "joined = pred_df.join(truth_df, on=[\"article_id\", \"dataset_id\"], how=\"right\")\n",
    "print(len(joined))\n",
    "# Add error type column\n",
    "error_expr = (\n",
    "    pl.when(pl.col(\"type\").is_null() & pl.col(\"type_truth\").is_not_null()).then(pl.lit(\"false_negative\"))\n",
    "    .when(pl.col(\"type\").is_not_null() & pl.col(\"type_truth\").is_null()).then(pl.lit(\"false_positive\"))\n",
    "    .when(\n",
    "        pl.col(\"type\").is_not_null()\n",
    "        & pl.col(\"type_truth\").is_not_null()\n",
    "        & (pl.col(\"type\") != pl.col(\"type_truth\"))\n",
    "    ).then(pl.lit(\"type_mismatch\"))\n",
    "    .otherwise(pl.lit(\"correct\"))\n",
    ")\n",
    "\n",
    "joined = joined.with_columns(error_expr.alias(\"error_type\"))\n",
    "\n",
    "# Extract error subsets\n",
    "false_negatives = joined.filter(pl.col(\"error_type\") == \"false_negative\")\n",
    "false_positives = joined.filter(pl.col(\"error_type\") == \"false_positive\")\n",
    "type_mismatches = joined.filter(pl.col(\"error_type\") == \"type_mismatch\")\n",
    "\n",
    "# Report\n",
    "print(f\"False negatives: {false_negatives.shape[0]}\")\n",
    "print(f\"False positives: {false_positives.shape[0]}\")\n",
    "print(f\"Type mismatches: {type_mismatches.shape[0]}\")\n",
    "\n",
    "# Save detailed breakdowns\n",
    "false_negatives.write_csv(\"/kaggle/working/false_negatives.csv\")\n",
    "false_positives.write_csv(\"/kaggle/working/false_positives.csv\")\n",
    "type_mismatches.write_csv(\"/kaggle/working/type_mismatches.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9745e77c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-03T13:15:01.192041Z",
     "iopub.status.busy": "2025-08-03T13:15:01.191613Z",
     "iopub.status.idle": "2025-08-03T13:15:01.353954Z",
     "shell.execute_reply": "2025-08-03T13:15:01.353068Z"
    },
    "papermill": {
     "duration": 0.183933,
     "end_time": "2025-08-03T13:15:01.355253",
     "exception": false,
     "start_time": "2025-08-03T13:15:01.171320",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "! cat /tmp/logs/project.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e9ce4d",
   "metadata": {
    "papermill": {
     "duration": 0.019686,
     "end_time": "2025-08-03T13:15:01.395382",
     "exception": false,
     "start_time": "2025-08-03T13:15:01.375696",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "INFO 2025-07-23 15:22:18  [post_filter.py:52 - main()] all - f1: 0.6891 [481/196/238]\n",
    "\n",
    "INFO 2025-07-23 15:22:18  [post_filter.py:52 - main()] doi - f1: 0.5889 [164/68/161]\n",
    "\n",
    "INFO 2025-07-23 15:22:18  [post_filter.py:52 - main()] acc - f1: 0.7557 [317/128/77]\n",
    "\n",
    "INFO 2025-07-23 15:22:18  [post_filter.py:53 - main()] all - f1: 0.5759 [402/275/317]\n",
    "\n",
    "INFO 2025-07-23 15:22:18  [post_filter.py:53 - main()] doi - f1: 0.4596 [128/104/197]\n",
    "\n",
    "INFO 2025-07-23 15:22:18  [post_filter.py:53 - main()] acc - f1: 0.6532 [274/171/120]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b37dc1",
   "metadata": {
    "papermill": {
     "duration": 0.019884,
     "end_time": "2025-08-03T13:15:01.435859",
     "exception": false,
     "start_time": "2025-08-03T13:15:01.415975",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "INFO 2025-07-30 10:24:30  [post_filter_scibert.py:56 - main()] all - f1: 0.5960 [486/426/233]\n",
    "\n",
    "INFO 2025-07-30 10:24:30  [post_filter_scibert.py:56 - main()] doi - f1: 0.5795 [164/77/161]\n",
    "\n",
    "INFO 2025-07-30 10:24:30  [post_filter_scibert.py:56 - main()] acc - f1: 0.6047 [322/349/72]\n",
    "\n",
    "INFO 2025-07-30 10:24:30  [post_filter_scibert.py:57 - main()] all - f1: 0.5861 [478/434/241]\n",
    "\n",
    "INFO 2025-07-30 10:24:30  [post_filter_scibert.py:57 - main()] doi - f1: 0.5583 [158/83/167]\n",
    "\n",
    "INFO 2025-07-30 10:24:30  [post_filter_scibert.py:57 - main()] acc - f1: 0.6009 [320/351/74]\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 13015230,
     "isSourceIdPinned": false,
     "sourceId": 82370,
     "sourceType": "competition"
    },
    {
     "datasetId": 7850099,
     "sourceId": 12444547,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7970947,
     "sourceId": 12616808,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7977103,
     "sourceId": 12625129,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7988469,
     "sourceId": 12641617,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 248118764,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 253744833,
     "sourceType": "kernelVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 164048,
     "modelInstanceId": 141565,
     "sourceId": 166368,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 322000,
     "modelInstanceId": 310551,
     "sourceId": 375840,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 427.114048,
   "end_time": "2025-08-03T13:15:05.217817",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-08-03T13:07:58.103769",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "018b809cc6f54ab5b81d3ba54dfd758f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_931209695052490db53e93f3c2793815",
       "placeholder": "​",
       "style": "IPY_MODEL_5f83351c45e74822a35baa3e61d3ab61",
       "tabbable": null,
       "tooltip": null,
       "value": " 1/1 [00:00&lt;00:00, 50.19it/s]"
      }
     },
     "0428623019034ab5b46bb621722a8e88": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "0ff6c0b57e0e4ac1ae0c720e6c04fc97": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_215ca5f03a904500a339e9879da0b36b",
        "IPY_MODEL_f6d53e07bda247e5b41a796ecdca4bd3",
        "IPY_MODEL_8d5155b2b28b44f1ab659c8ca6c75070"
       ],
       "layout": "IPY_MODEL_11e058c70e41450fa6b1640b1b25db8a",
       "tabbable": null,
       "tooltip": null
      }
     },
     "11e058c70e41450fa6b1640b1b25db8a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "16c5ce76b42649068065cf3c5e22a562": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2022b4e60ba141749de066262acd13e2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_7ed7843b000b49a9a3a379f241d9aec3",
        "IPY_MODEL_2ea49280e96e4da4b5ff590fcf4cf53e",
        "IPY_MODEL_9e035ea656b64a94988fdf194441ebe2"
       ],
       "layout": "IPY_MODEL_781661cbccac4945933b2a9eea12218b",
       "tabbable": null,
       "tooltip": null
      }
     },
     "215ca5f03a904500a339e9879da0b36b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_fefcbe49a8944dc99e62e0eaeea990f7",
       "placeholder": "​",
       "style": "IPY_MODEL_87c71e88047b44b28560fcafc2af9675",
       "tabbable": null,
       "tooltip": null,
       "value": "Batches: 100%"
      }
     },
     "2ea49280e96e4da4b5ff590fcf4cf53e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_f8f5f99c1362427f820d3bdec81a4d8c",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_a52f7acde25f45f09b67be58c7b5f30b",
       "tabbable": null,
       "tooltip": null,
       "value": 1.0
      }
     },
     "3ba35e454ca44b3c99188bdc7d28ab7b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_74097062302a41ddb25cfb9e3272bc26",
        "IPY_MODEL_50686ca421b64ae2a228867f1b6d0623",
        "IPY_MODEL_61bb4e1152c447f1bd2ebb05dfeb4072"
       ],
       "layout": "IPY_MODEL_4b3e4a32434b4098a4245e56e3fdff85",
       "tabbable": null,
       "tooltip": null
      }
     },
     "3cf96dcbd13946b78e8274b82208e2e4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_513b9fb0dbc64f5d80b25d8cc30a2be5",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_93cd6edf533e434b94c43ef3844549e2",
       "tabbable": null,
       "tooltip": null,
       "value": 1.0
      }
     },
     "43aa0a4097514553aa5f858faa173c87": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4b3e4a32434b4098a4245e56e3fdff85": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "50686ca421b64ae2a228867f1b6d0623": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_64e4ecf2399347dabb57e5c7a30327a9",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_c6f19ef68d784a73a75c3d4fa50b7c1a",
       "tabbable": null,
       "tooltip": null,
       "value": 1.0
      }
     },
     "513b9fb0dbc64f5d80b25d8cc30a2be5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "530b02640be84cd982f05c4ca5e707d9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "5f83351c45e74822a35baa3e61d3ab61": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "61bb4e1152c447f1bd2ebb05dfeb4072": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_d99ff36859384a778b64889b0c91439c",
       "placeholder": "​",
       "style": "IPY_MODEL_0428623019034ab5b46bb621722a8e88",
       "tabbable": null,
       "tooltip": null,
       "value": " 1/1 [00:00&lt;00:00, 49.26it/s]"
      }
     },
     "64e4ecf2399347dabb57e5c7a30327a9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6995ed7865864c1eb72291830d782dbe": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_16c5ce76b42649068065cf3c5e22a562",
       "placeholder": "​",
       "style": "IPY_MODEL_823e89ea544e4062a95fa8ea55569436",
       "tabbable": null,
       "tooltip": null,
       "value": "Batches: 100%"
      }
     },
     "74097062302a41ddb25cfb9e3272bc26": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_9044be9dfbfc4ea79ec9b96454d7f64a",
       "placeholder": "​",
       "style": "IPY_MODEL_d7f9a35d810b4a6f85876581ef0da209",
       "tabbable": null,
       "tooltip": null,
       "value": "Batches: 100%"
      }
     },
     "781661cbccac4945933b2a9eea12218b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7bc2418ffe39455696788797eece1326": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "7ed7843b000b49a9a3a379f241d9aec3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_43aa0a4097514553aa5f858faa173c87",
       "placeholder": "​",
       "style": "IPY_MODEL_7bc2418ffe39455696788797eece1326",
       "tabbable": null,
       "tooltip": null,
       "value": "Batches: 100%"
      }
     },
     "823e89ea544e4062a95fa8ea55569436": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "87c71e88047b44b28560fcafc2af9675": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "8ac43540400b4953ad01596fef102f92": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8d5155b2b28b44f1ab659c8ca6c75070": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_fe4cafe4c47a4cfa9d3bfd319b0175c0",
       "placeholder": "​",
       "style": "IPY_MODEL_c6f63ab9d6124a4aa39681b73e889830",
       "tabbable": null,
       "tooltip": null,
       "value": " 1/1 [00:00&lt;00:00,  4.31it/s]"
      }
     },
     "9044be9dfbfc4ea79ec9b96454d7f64a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "931209695052490db53e93f3c2793815": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "93cd6edf533e434b94c43ef3844549e2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "9e035ea656b64a94988fdf194441ebe2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_dc78eeea2391447aaeb0987ea5111a85",
       "placeholder": "​",
       "style": "IPY_MODEL_d9ff460f31994953a772281fb348dd44",
       "tabbable": null,
       "tooltip": null,
       "value": " 1/1 [00:00&lt;00:00, 20.50it/s]"
      }
     },
     "a52f7acde25f45f09b67be58c7b5f30b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "c6f19ef68d784a73a75c3d4fa50b7c1a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "c6f63ab9d6124a4aa39681b73e889830": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "d7f9a35d810b4a6f85876581ef0da209": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "d99ff36859384a778b64889b0c91439c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d9ff460f31994953a772281fb348dd44": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "dc78eeea2391447aaeb0987ea5111a85": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f061f1047a5d4caa81563ab256b44370": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_6995ed7865864c1eb72291830d782dbe",
        "IPY_MODEL_3cf96dcbd13946b78e8274b82208e2e4",
        "IPY_MODEL_018b809cc6f54ab5b81d3ba54dfd758f"
       ],
       "layout": "IPY_MODEL_fc9ab761ba4e462ea21697c7fbac90f0",
       "tabbable": null,
       "tooltip": null
      }
     },
     "f6d53e07bda247e5b41a796ecdca4bd3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_8ac43540400b4953ad01596fef102f92",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_530b02640be84cd982f05c4ca5e707d9",
       "tabbable": null,
       "tooltip": null,
       "value": 1.0
      }
     },
     "f8f5f99c1362427f820d3bdec81a4d8c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fc9ab761ba4e462ea21697c7fbac90f0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fe4cafe4c47a4cfa9d3bfd319b0175c0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fefcbe49a8944dc99e62e0eaeea990f7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
