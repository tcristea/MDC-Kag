{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a34848d",
   "metadata": {
    "_cell_guid": "d382f10b-f2da-4c7c-836d-cc7a6cb424b0",
    "_uuid": "d6a30285-a4d2-49d8-ba2d-8f365731bbb5",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-07-25T11:50:45.303370Z",
     "iopub.status.busy": "2025-07-25T11:50:45.302693Z",
     "iopub.status.idle": "2025-07-25T11:50:46.257463Z",
     "shell.execute_reply": "2025-07-25T11:50:46.256725Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.961028,
     "end_time": "2025-07-25T11:50:46.259165",
     "exception": false,
     "start_time": "2025-07-25T11:50:45.298137",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\r\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m1 package\u001b[0m \u001b[2min 41ms\u001b[0m\u001b[0m\r\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 389ms\u001b[0m\u001b[0m\r\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 11ms\u001b[0m\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mpymupdf\u001b[0m\u001b[2m==1.26.1\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "# ! uv pip uninstall --system 'tensorflow'\n",
    "! uv pip install --system --no-index --find-links='/kaggle/input/latest-mdc-whls/whls' 'pymupdf' # 'vllm' 'triton' 'logits-processor-zoo' 'numpy<2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0b865e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-25T11:50:46.267530Z",
     "iopub.status.busy": "2025-07-25T11:50:46.267279Z",
     "iopub.status.idle": "2025-07-25T11:51:22.232700Z",
     "shell.execute_reply": "2025-07-25T11:51:22.231709Z"
    },
    "papermill": {
     "duration": 35.970985,
     "end_time": "2025-07-25T11:51:22.234131",
     "exception": false,
     "start_time": "2025-07-25T11:50:46.263146",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\r\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m145 packages\u001b[0m \u001b[2min 300ms\u001b[0m\u001b[0m\r\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m54 packages\u001b[0m \u001b[2min 28.70s\u001b[0m\u001b[0m\r\n",
      "\u001b[2mUninstalled \u001b[1m23 packages\u001b[0m \u001b[2min 2.01s\u001b[0m\u001b[0m\r\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m54 packages\u001b[0m \u001b[2min 520ms\u001b[0m\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mairportsdata\u001b[0m\u001b[2m==20250706\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mastor\u001b[0m\u001b[2m==0.8.1\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mblake3\u001b[0m\u001b[2m==1.0.5\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mcompressed-tensors\u001b[0m\u001b[2m==0.10.2\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mdepyf\u001b[0m\u001b[2m==0.18.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mdiskcache\u001b[0m\u001b[2m==5.6.3\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mfastapi-cli\u001b[0m\u001b[2m==0.0.8\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mfastapi-cloud-cli\u001b[0m\u001b[2m==0.1.1\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mgguf\u001b[0m\u001b[2m==0.17.1\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mhttptools\u001b[0m\u001b[2m==0.6.4\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mhttpx\u001b[0m\u001b[2m==0.28.1\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mhttpx\u001b[0m\u001b[2m==0.27.2\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1minteregular\u001b[0m\u001b[2m==0.3.3\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mlark\u001b[0m\u001b[2m==1.2.2\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mllguidance\u001b[0m\u001b[2m==0.7.30\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mllvmlite\u001b[0m\u001b[2m==0.43.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mllvmlite\u001b[0m\u001b[2m==0.44.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mlm-format-enforcer\u001b[0m\u001b[2m==0.10.11\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mmistral-common\u001b[0m\u001b[2m==1.6.3\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mmsgspec\u001b[0m\u001b[2m==0.19.0\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mnumba\u001b[0m\u001b[2m==0.60.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mnumba\u001b[0m\u001b[2m==0.61.2\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-cublas-cu12\u001b[0m\u001b[2m==12.5.3.2\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cublas-cu12\u001b[0m\u001b[2m==12.6.4.1\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-cuda-cupti-cu12\u001b[0m\u001b[2m==12.5.82\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-cupti-cu12\u001b[0m\u001b[2m==12.6.80\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-cuda-nvrtc-cu12\u001b[0m\u001b[2m==12.5.82\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-nvrtc-cu12\u001b[0m\u001b[2m==12.6.77\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-cuda-runtime-cu12\u001b[0m\u001b[2m==12.5.82\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-runtime-cu12\u001b[0m\u001b[2m==12.6.77\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-cudnn-cu12\u001b[0m\u001b[2m==9.3.0.75\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cudnn-cu12\u001b[0m\u001b[2m==9.5.1.17\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-cufft-cu12\u001b[0m\u001b[2m==11.2.3.61\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cufft-cu12\u001b[0m\u001b[2m==11.3.0.4\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cufile-cu12\u001b[0m\u001b[2m==1.11.1.6\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-curand-cu12\u001b[0m\u001b[2m==10.3.6.82\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-curand-cu12\u001b[0m\u001b[2m==10.3.7.77\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-cusolver-cu12\u001b[0m\u001b[2m==11.6.3.83\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusolver-cu12\u001b[0m\u001b[2m==11.7.1.2\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-cusparse-cu12\u001b[0m\u001b[2m==12.5.1.3\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusparse-cu12\u001b[0m\u001b[2m==12.5.4.2\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-cusparselt-cu12\u001b[0m\u001b[2m==0.6.2\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusparselt-cu12\u001b[0m\u001b[2m==0.6.3\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-nccl-cu12\u001b[0m\u001b[2m==2.21.5\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-nccl-cu12\u001b[0m\u001b[2m==2.26.2\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-nvjitlink-cu12\u001b[0m\u001b[2m==12.5.82\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-nvjitlink-cu12\u001b[0m\u001b[2m==12.6.85\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-nvtx-cu12\u001b[0m\u001b[2m==12.4.127\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-nvtx-cu12\u001b[0m\u001b[2m==12.6.77\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mopenai\u001b[0m\u001b[2m==1.91.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mopenai\u001b[0m\u001b[2m==1.90.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1moutlines\u001b[0m\u001b[2m==0.1.11\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1moutlines-core\u001b[0m\u001b[2m==0.1.26\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mpartial-json-parser\u001b[0m\u001b[2m==0.2.1.1.post6\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mprometheus-fastapi-instrumentator\u001b[0m\u001b[2m==7.1.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mpybase64\u001b[0m\u001b[2m==1.4.1\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mpycountry\u001b[0m\u001b[2m==24.6.1\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mpython-dotenv\u001b[0m\u001b[2m==1.1.1\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mpyzmq\u001b[0m\u001b[2m==24.0.1\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mpyzmq\u001b[0m\u001b[2m==27.0.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mrich-toolkit\u001b[0m\u001b[2m==0.14.8\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mrignore\u001b[0m\u001b[2m==0.5.1\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1msympy\u001b[0m\u001b[2m==1.13.1\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1msympy\u001b[0m\u001b[2m==1.14.0\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mtorch\u001b[0m\u001b[2m==2.6.0+cu124 (from https://download.pytorch.org/whl/cu124/torch-2.6.0%2Bcu124-cp311-cp311-linux_x86_64.whl)\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mtorch\u001b[0m\u001b[2m==2.7.0\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mtorchaudio\u001b[0m\u001b[2m==2.6.0+cu124 (from https://download.pytorch.org/whl/cu124/torchaudio-2.6.0%2Bcu124-cp311-cp311-linux_x86_64.whl)\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mtorchaudio\u001b[0m\u001b[2m==2.7.0\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mtorchvision\u001b[0m\u001b[2m==0.21.0+cu124 (from https://download.pytorch.org/whl/cu124/torchvision-0.21.0%2Bcu124-cp311-cp311-linux_x86_64.whl)\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mtorchvision\u001b[0m\u001b[2m==0.22.0\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mtriton\u001b[0m\u001b[2m==3.2.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mtriton\u001b[0m\u001b[2m==3.3.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1muvloop\u001b[0m\u001b[2m==0.21.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mvllm\u001b[0m\u001b[2m==0.9.2\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mwatchfiles\u001b[0m\u001b[2m==1.1.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mxformers\u001b[0m\u001b[2m==0.0.30\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mxgrammar\u001b[0m\u001b[2m==0.1.19\u001b[0m\r\n",
      "\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\r\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m54 packages\u001b[0m \u001b[2min 20ms\u001b[0m\u001b[0m\r\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 8ms\u001b[0m\u001b[0m\r\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 2ms\u001b[0m\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mlogits-processor-zoo\u001b[0m\u001b[2m==0.1.10\u001b[0m\r\n",
      "\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\r\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m1 package\u001b[0m \u001b[2min 10ms\u001b[0m\u001b[0m\r\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 3.37s\u001b[0m\u001b[0m\r\n",
      "\u001b[2mUninstalled \u001b[1m1 package\u001b[0m \u001b[2min 8ms\u001b[0m\u001b[0m\r\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 13ms\u001b[0m\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mtriton\u001b[0m\u001b[2m==3.3.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mtriton\u001b[0m\u001b[2m==3.2.0\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "# ! uv pip install /kaggle/input/mdcfitz/pymupdf-1.26.3-cp39-abi3-manylinux_2_28_x86_64.whl\n",
    "! uv pip install vllm --no-index --find-links file:///kaggle/input/mdcllm\n",
    "! uv pip install logits-processor-zoo==0.1.10 --no-index --find-links file:///kaggle/input/mdcllm\n",
    "! uv pip install triton==3.2.0 --no-index --find-links file:///kaggle/input/mdcllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e329c817",
   "metadata": {
    "_cell_guid": "d382f10b-f2da-4c7c-836d-cc7a6cb424b0",
    "_uuid": "d6a30285-a4d2-49d8-ba2d-8f365731bbb5",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-07-25T11:51:22.256516Z",
     "iopub.status.busy": "2025-07-25T11:51:22.255778Z",
     "iopub.status.idle": "2025-07-25T11:51:22.375398Z",
     "shell.execute_reply": "2025-07-25T11:51:22.374226Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.132116,
     "end_time": "2025-07-25T11:51:22.376994",
     "exception": false,
     "start_time": "2025-07-25T11:51:22.244878",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "! mkdir -p /tmp/src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73c10fb1",
   "metadata": {
    "_cell_guid": "18e82e70-4cf3-4b9a-8390-dd4a9feefa56",
    "_uuid": "ba64a284-6ef8-4117-8ae7-c8d78eab9005",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-07-25T11:51:22.400560Z",
     "iopub.status.busy": "2025-07-25T11:51:22.400334Z",
     "iopub.status.idle": "2025-07-25T11:51:22.407151Z",
     "shell.execute_reply": "2025-07-25T11:51:22.406494Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.019793,
     "end_time": "2025-07-25T11:51:22.408285",
     "exception": false,
     "start_time": "2025-07-25T11:51:22.388492",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /tmp/src/helpers.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /tmp/src/helpers.py\n",
    "import logging, os, kagglehub, inspect\n",
    "from pathlib import Path\n",
    "import polars as pl\n",
    "\n",
    "IS_KAGGLE_ENV = sum(['KAGGLE' in k for k in os.environ]) > 0\n",
    "os.environ[\"KAGGLE_IS_COMPETITION_RERUN\"] = \"1\"\n",
    "IS_KAGGLE_SUBMISSION = bool(os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"))\n",
    "COMP_DIR = Path(('/kaggle/input/make-data-count-finding-data-references' if IS_KAGGLE_SUBMISSION else kagglehub.competition_download('make-data-count-finding-data-references')))\n",
    "PDF_DIR = COMP_DIR / ('test' if IS_KAGGLE_SUBMISSION else 'train') / 'PDF'\n",
    "WORKING_DIR = Path(('/kaggle/working/' if IS_KAGGLE_ENV else '.working/'))\n",
    "\n",
    "DOI_LINK = 'https://doi.org/'\n",
    "\n",
    "DEFAULT_LOG_LEVEL = os.getenv(\"LOG_LEVEL\", \"DEBUG\").upper() if not IS_KAGGLE_SUBMISSION else \"WARNING\"\n",
    "LOG_FILE_PATH = os.getenv(\"LOG_FILE\", \"logs/project.log\")\n",
    "LOG_DIR = Path(LOG_FILE_PATH).parent\n",
    "\n",
    "LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "LOG_FORMAT = \"%(levelname)s %(asctime)s  [%(filename)s:%(lineno)d - %(funcName)s()] %(message)s\"\n",
    "LOG_DATEFMT = \"%Y-%m-%d %H:%M:%S\"\n",
    "\n",
    "def get_logger(name=None):\n",
    "    if name is None:\n",
    "        frame = inspect.currentframe()\n",
    "        if frame is None or frame.f_back is None:\n",
    "            name = \"__main__\"\n",
    "        else:\n",
    "            name = frame.f_back.f_globals.get(\"__name__\", \"__main__\")\n",
    "\n",
    "    logger = logging.getLogger(name)\n",
    "\n",
    "    if not logger.handlers:\n",
    "        logger.setLevel(DEFAULT_LOG_LEVEL)\n",
    "        formatter = logging.Formatter(fmt=LOG_FORMAT, datefmt=LOG_DATEFMT)\n",
    "        ch = logging.StreamHandler()\n",
    "        ch.setLevel(DEFAULT_LOG_LEVEL)\n",
    "        ch.setFormatter(formatter)\n",
    "        fh = logging.FileHandler(LOG_FILE_PATH)\n",
    "        fh.setLevel(DEFAULT_LOG_LEVEL)\n",
    "        fh.setFormatter(formatter)\n",
    "        logger.addHandler(ch)\n",
    "        logger.addHandler(fh)\n",
    "        logger.propagate = False\n",
    "    return logger\n",
    "\n",
    "def is_doi_link(name: str) -> pl.Expr:\n",
    "    return pl.col(name).str.starts_with(DOI_LINK)\n",
    "\n",
    "def string_normalization(name: str) -> pl.Expr:\n",
    "    return pl.col(name).str.normalize(\"NFKC\").str.replace_all(r\"[^\\p{Ascii}]\", '').str.replace_all(r\"https?://zenodo\\.org/record/(\\d+)\", r\" 10.5281/zenodo.$1 \")\n",
    "\n",
    "def get_df(parse_dir: str):\n",
    "    records = []\n",
    "    txt_files = list(Path(parse_dir).glob('*.txt'))\n",
    "    for txt_file in txt_files:\n",
    "        id_ = txt_file.stem\n",
    "        with open(txt_file, 'r') as f:\n",
    "            text = f.read()\n",
    "        records.append({'article_id': id_, 'text': text})\n",
    "    return pl.DataFrame(records).with_columns(string_normalization('text').alias('text'))\n",
    "\n",
    "def assume_type(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    return (\n",
    "        df.with_columns(pl.when(is_doi_link('dataset_id').or_(pl.col('dataset_id').str.starts_with('SAMN'))).then(pl.lit('Primary')).otherwise(pl.lit('Secondary')).alias('type'))\n",
    "    )\n",
    "\n",
    "def score(df, gt, on, tag='all'):\n",
    "    hits = gt.join(df, on=on)\n",
    "    tp = hits.height\n",
    "    fp = df.height - tp\n",
    "    fn = gt.height - tp\n",
    "    f1 = 2 * tp / (2 * tp + fp + fn) if (2 * tp + fp + fn) != 0 else 0.0\n",
    "    return f\"{tag} - f1: {f1:.4f} [{tp}/{fp}/{fn}]\"\n",
    "\n",
    "def evaluate(df, on=['article_id', 'dataset_id']):\n",
    "    gt = pl.read_csv(COMP_DIR/'train_labels.csv').filter(pl.col('type')!='Missing')\n",
    "    return (\n",
    "        score(df, gt, on),\n",
    "        score(df.filter(is_doi_link('dataset_id')), gt.filter(is_doi_link('dataset_id')), on, 'doi'),\n",
    "        score(df.filter(~is_doi_link('dataset_id')), gt.filter(~is_doi_link('dataset_id')), on, 'acc'),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bb25d78",
   "metadata": {
    "_cell_guid": "28a81a8a-efe6-4168-bc35-32a706fda00f",
    "_uuid": "9e6ff967-bb1a-48c4-a513-09195d3b1806",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-07-25T11:51:22.430045Z",
     "iopub.status.busy": "2025-07-25T11:51:22.429849Z",
     "iopub.status.idle": "2025-07-25T11:51:22.434245Z",
     "shell.execute_reply": "2025-07-25T11:51:22.433620Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.016334,
     "end_time": "2025-07-25T11:51:22.435254",
     "exception": false,
     "start_time": "2025-07-25T11:51:22.418920",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /tmp/src/parse.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /tmp/src/parse.py\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import pymupdf\n",
    "from helpers import get_logger, PDF_DIR\n",
    "\n",
    "l = get_logger()\n",
    "\n",
    "def pdf_to_txt(output_dir: Path):\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    pdf_files = list(PDF_DIR.glob(\"*.pdf\")) + list(PDF_DIR.glob(\"*.PDF\"))\n",
    "    existing_txt_files = {f.stem for f in output_dir.glob(\"*.txt\")}\n",
    "    for pdf_file in pdf_files:\n",
    "        txt_file = output_dir / f\"{pdf_file.stem}.txt\"\n",
    "        if pdf_file.stem in existing_txt_files:\n",
    "            continue\n",
    "        try:\n",
    "            text = \"\"\n",
    "            with pymupdf.open(pdf_file) as doc:\n",
    "                for page in doc:\n",
    "                    text += page.get_text()\n",
    "            txt_file.write_text(text, encoding='utf-8')\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('output_dir', type=Path, help='Directory to save text files')\n",
    "    args = parser.parse_args()\n",
    "    pdf_to_txt(args.output_dir)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8795c455",
   "metadata": {
    "_cell_guid": "820ae983-8aea-4e2f-8e10-e98a61ae6bc9",
    "_uuid": "9356808e-1990-4aba-9374-018ecd44e57b",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-07-25T11:51:22.456740Z",
     "iopub.status.busy": "2025-07-25T11:51:22.456536Z",
     "iopub.status.idle": "2025-07-25T11:51:22.461139Z",
     "shell.execute_reply": "2025-07-25T11:51:22.460521Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.016135,
     "end_time": "2025-07-25T11:51:22.462298",
     "exception": false,
     "start_time": "2025-07-25T11:51:22.446163",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /tmp/src/check_parse.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /tmp/src/check_parse.py\n",
    "import polars as pl\n",
    "from pathlib import Path\n",
    "from helpers import *\n",
    "\n",
    "l=get_logger()\n",
    "\n",
    "def gt_dataset_id_normalization(name:str) -> pl.Expr:\n",
    "    return (\n",
    "        pl.when(is_doi_link(name))\n",
    "        .then(pl.col(name).str.split(DOI_LINK).list.last())\n",
    "        .otherwise(name)\n",
    "        .str.to_lowercase()\n",
    "    )\n",
    "\n",
    "def main():\n",
    "    if IS_KAGGLE_SUBMISSION:\n",
    "        l.debug('skipping check_parse for submission')\n",
    "        return\n",
    "    df = (\n",
    "        get_df('/tmp/train_parse')\n",
    "        .with_columns(pl.col('text').str.replace_all('\\s+', '').str.to_lowercase().alias('text'))\n",
    "    )\n",
    "\n",
    "    gt = (\n",
    "        pl.read_csv(COMP_DIR/'train_labels.csv')\n",
    "        .filter(pl.col('article_id').is_in(df['article_id']))\n",
    "        .filter(pl.col('type')!='Missing')\n",
    "        .with_columns(gt_dataset_id_normalization('dataset_id').alias('norm_id'))\n",
    "    )\n",
    "\n",
    "    l.info(f\"pymupdf misses: {gt.join(df, on='article_id').with_columns(hit=pl.col('text').str.contains(pl.col('norm_id'), literal=True)).filter(~pl.col('hit')).height} dataset_ids\")\n",
    "\n",
    "if __name__=='__main__': main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d09c7a82",
   "metadata": {
    "_cell_guid": "f9b4ec0e-e208-4eb0-9c5a-4273400ee7f6",
    "_uuid": "6081f6cf-0cba-4b83-acab-60bf0c6fdbd8",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-07-25T11:51:22.485420Z",
     "iopub.status.busy": "2025-07-25T11:51:22.485221Z",
     "iopub.status.idle": "2025-07-25T11:51:22.493297Z",
     "shell.execute_reply": "2025-07-25T11:51:22.492400Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.020754,
     "end_time": "2025-07-25T11:51:22.494498",
     "exception": false,
     "start_time": "2025-07-25T11:51:22.473744",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /tmp/src/getid.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /tmp/src/getid.py\n",
    "import re\n",
    "import polars as pl\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "from helpers import *\n",
    "\n",
    "COMPILED_PATTERNS = {\n",
    "'ref_header_patterns': [re.compile(r'\\b(R\\s*E\\s*F\\s*E\\s*R\\s*E\\s*N\\s*C\\s*E\\s*S|BIBLIOGRAPHY|LITERATURE CITED|WORKS CITED|CITED WORKS|ACKNOWLEDGEMENTS)\\b[:\\s]*', re.IGNORECASE)],\n",
    "'citation_pattern': re.compile(r'^\\s*(\\[\\d+\\]|\\(\\d+\\)|\\d+\\.|\\d+\\)|\\d+(?=\\s|$))\\s*'),\n",
    "'first_citation_patterns': [re.compile(r'^\\s*\\[1\\]\\s*'), re.compile(r'^\\s*\\(1\\)\\s*'), re.compile(r'^\\s*1\\.\\s*'), re.compile(r'^\\s*1\\)\\s*'), re.compile(r'^\\s*1(?=\\s|$)')],\n",
    "}\n",
    "\n",
    "l = get_logger()\n",
    "\n",
    "def find_last_reference_header(text: str, header_patterns: list[re.Pattern]) -> Optional[int]:\n",
    "    last_match_idx = None\n",
    "    for pattern in header_patterns:\n",
    "        matches = list(pattern.finditer(text))\n",
    "        if matches: last_match_idx = matches[-1].start()\n",
    "    return last_match_idx\n",
    "\n",
    "def find_last_first_citation(text: str) -> Optional[int]:\n",
    "    lines = text.splitlines()\n",
    "    last_match_line = None\n",
    "    for line_num, line in enumerate(lines):\n",
    "        line = line.strip()\n",
    "        for pattern in COMPILED_PATTERNS['first_citation_patterns']:\n",
    "            if pattern.match(line):\n",
    "                next_lines = lines[line_num:line_num+3]\n",
    "                if any(COMPILED_PATTERNS['citation_pattern'].match(l.strip()) for l in next_lines[1:]):\n",
    "                    last_match_line = line_num\n",
    "                break\n",
    "    return last_match_line\n",
    "\n",
    "def find_reference_start(text: str) -> Optional[int]:\n",
    "    lines = text.splitlines()\n",
    "    last_first_citation = find_last_first_citation(text)\n",
    "    if last_first_citation is not None: return last_first_citation\n",
    "    start_search_idx = int(len(lines) * 0.5)\n",
    "    for i in range(start_search_idx, len(lines)):\n",
    "        line = lines[i].strip()\n",
    "        if COMPILED_PATTERNS['citation_pattern'].match(line):\n",
    "            next_lines = lines[i:i+3]\n",
    "            if sum(1 for l in next_lines if COMPILED_PATTERNS['citation_pattern'].match(l.strip())) >= 2:\n",
    "                for j in range(i, max(-1, i-10), -1):\n",
    "                    if not COMPILED_PATTERNS['citation_pattern'].match(lines[j].strip()): return j + 1\n",
    "                return max(0, i-10)\n",
    "    return None\n",
    "\n",
    "def split_text_and_references(text: str) -> Tuple[str, str]:\n",
    "    header_idx = find_last_reference_header(text, COMPILED_PATTERNS['ref_header_patterns'])\n",
    "    if header_idx is not None:\n",
    "        header_idx2 = find_last_reference_header(text[:header_idx].strip(), COMPILED_PATTERNS['ref_header_patterns'])\n",
    "        if header_idx2 is not None:\n",
    "            header_idx3 = find_last_reference_header(text[:header_idx2].strip(), COMPILED_PATTERNS['ref_header_patterns'])\n",
    "            if header_idx3 is not None: return text[:header_idx3].strip(), text[header_idx3:].strip()\n",
    "            return text[:header_idx2].strip(), text[header_idx2:].strip()\n",
    "        return text[:header_idx].strip(), text[header_idx:].strip()\n",
    "    ref_start_line = find_reference_start(text)\n",
    "    if ref_start_line is not None:\n",
    "        lines = text.splitlines()\n",
    "        body = '\\n'.join(lines[:ref_start_line])\n",
    "        refs = '\\n'.join(lines[ref_start_line:])\n",
    "        return body.strip(), refs.strip()\n",
    "    return text.strip(), ''\n",
    "\n",
    "def get_splits(df):\n",
    "    main_texts, ref_texts = [], []\n",
    "    for raw_text in df['text']:\n",
    "        main, refs = split_text_and_references(raw_text)\n",
    "        main_texts.append(main)\n",
    "        ref_texts.append(refs)\n",
    "    df = df.with_columns(pl.Series('body', main_texts), pl.Series('ref', ref_texts))\n",
    "    return df\n",
    "\n",
    "def tidy_extraction(df) -> pl.DataFrame:\n",
    "    bad_ids = [f'{DOI_LINK}{e}' for e in ['10.5061/dryad', '10.5281/zenodo', '10.6073/pasta']]\n",
    "    doi_df = (df.with_columns(pl.col('body').str.extract_all(r'10\\s*\\.\\s*\\d{4,9}\\s*/\\s*\\S+').alias('match')).explode('match').drop_nulls('match').with_columns(pl.col('match').str.replace_all(r'\\s+', '').str.replace(r'[^A-Za-z0-9]+$', '').str.to_lowercase().alias('dataset_id')).group_by('article_id','dataset_id').agg('match').with_columns((DOI_LINK+pl.col('dataset_id')).alias('dataset_id')))\n",
    "    acc_df = (df.with_columns(pl.col('text').str.extract_all(r'(?i)\\b(?:CHEMBL\\d+|E-GEOD-\\d+|E-PROT-\\d+|EMPIAR-\\d+|ENSBTAG\\d+|ENSOARG\\d+|EPI_ISL_\\d{5,}|EPI\\d{6,7}|HPA\\d+|CP\\d{6}|IPR\\d{6}|PF\\d{5}|KX\\d{6}|K0\\d{4}|PRJNA\\d+|PXD\\d+|SAMN\\d+|dryad\\s*\\.\\s*[^\\s\"<>]+|pasta\\s*/\\s*[^\\s\"<>])').alias('match')).explode('match').drop_nulls('match').with_columns(pl.col('match').str.replace_all(r'\\s+', '').str.replace(r'[^A-Za-z0-9]+$', '').alias('dataset_id')).group_by('article_id','dataset_id').agg('match').with_columns(pl.when(pl.col('dataset_id').str.starts_with('dryad.')).then(f'{DOI_LINK}10.5061/' + pl.col('dataset_id')).otherwise('dataset_id').alias('dataset_id')).with_columns(pl.when(pl.col('dataset_id').str.starts_with('pasta/')).then(f'{DOI_LINK}10.6073/' + pl.col('dataset_id')).otherwise('dataset_id').alias('dataset_id')))\n",
    "    df = pl.concat([doi_df, acc_df])\n",
    "    df = (df.unique('dataset_id').filter(~pl.col('article_id').str.replace('_','/').str.contains(pl.col('dataset_id').str.split(DOI_LINK).list.last().str.escape_regex())).filter(~pl.col('dataset_id').str.contains(pl.col('article_id').str.replace('_','/').str.escape_regex())).filter(~pl.col('dataset_id').str.contains('figshare', literal=True)).filter(~pl.col('dataset_id').is_in(bad_ids)).filter(pl.when(is_doi_link('dataset_id').and_(pl.col('dataset_id').str.split('/').list.last().str.len_chars()<5)).then(False).otherwise(True)).with_columns(pl.col('match').list.unique()))\n",
    "    return df\n",
    "\n",
    "def get_context_window(text: str, substring: str, window: int = 100) -> str:\n",
    "    index = text.find(substring)\n",
    "    if index == -1: raise ValueError\n",
    "    start = max(index - window, 0)\n",
    "    end = min(index + len(substring) + window, len(text))\n",
    "    return text[start:end]\n",
    "\n",
    "def get_window_df(text_df, ids_df):\n",
    "    df = ids_df.join(text_df, on='article_id')\n",
    "    windows = []\n",
    "    for text, match_ids in df.select('text', 'match').rows():\n",
    "        windows.append(get_context_window(text, match_ids[0]))\n",
    "    return df.with_columns(pl.Series('window', windows)).select('article_id', 'dataset_id', 'window')\n",
    "\n",
    "\n",
    "def main():\n",
    "    text_df = get_df('/tmp/train_parse')\n",
    "    df = get_splits(text_df)\n",
    "    df = tidy_extraction(df)\n",
    "    df = get_window_df(text_df, df)\n",
    "    df.write_parquet('/tmp/extracted.parquet')\n",
    "    df = assume_type(df)\n",
    "    df.select(['article_id', 'dataset_id', 'type']).with_row_index(name='row_id').write_csv('/kaggle/working/submission.csv')\n",
    "    if not IS_KAGGLE_SUBMISSION:\n",
    "        results = evaluate(df)\n",
    "        for r in results: l.info(r) \n",
    "        results = evaluate(df, on=['article_id', 'dataset_id', 'type'])\n",
    "        for r in results: l.info(r) \n",
    "\n",
    "if __name__=='__main__': main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "460191de",
   "metadata": {
    "_cell_guid": "13d5bff2-0f7e-4ac0-9498-4cc99ef6ca37",
    "_uuid": "d8f93359-7f6d-4dd1-afec-2acc8f784dc4",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-07-25T11:51:22.517705Z",
     "iopub.status.busy": "2025-07-25T11:51:22.517460Z",
     "iopub.status.idle": "2025-07-25T11:51:22.523898Z",
     "shell.execute_reply": "2025-07-25T11:51:22.523248Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.019808,
     "end_time": "2025-07-25T11:51:22.525427",
     "exception": false,
     "start_time": "2025-07-25T11:51:22.505619",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /tmp/src/llm_validate.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /tmp/src/llm_validate.py\n",
    "import polars as pl\n",
    "import os\n",
    "\n",
    "from helpers import *\n",
    "\n",
    "l = get_logger()\n",
    "\n",
    "SYS_PROMPT_CLASSIFY_DOI = \"\"\"\n",
    "You are a highly accurate DOI/type classifier. Given a snippet of academic text containing a DOI or accession, choose:\n",
    "\n",
    "  A) Data — the identifier points directly to research data in a repository  \n",
    "  B) Literature — the identifier points to a journal article, book chapter, protocol paper, or other non-data resource  \n",
    "\n",
    "=== Repository Prefixes ===\n",
    "Treat as DATA if the DOI starts with any of:\n",
    "  • 10.5061 (Dryad)  \n",
    "  • 10.5281 (Zenodo)  \n",
    "  • 10.6084 (Figshare)  \n",
    "  • 10.24433/ (Mendeley Data)  \n",
    "  • 10.17632 (Mendeley Data)  \n",
    "  • SRA/E- (e.g. SRP, SRA)  \n",
    "  • PRJNA, PRJEB, PRJDB (NCBI BioProject)  \n",
    "  • PRIDE:PXD (Proteomics)  \n",
    "  • EMBL:E-MTAB, E- (ArrayExpress)  \n",
    "\n",
    "Everything else is LITERATURE unless you see explicit data-repository context (e.g. “deposited in Dryad under DOI…”).\n",
    "\n",
    "=== Few-Shot Examples ===\n",
    "1) “Raw images are stored on Figshare (DOI 10.6084/m9.figshare.1234567).” → A  \n",
    "2) “Sequence reads available under BioProject accession PRJNA765432.” → A  \n",
    "3) “As described in Nature Methods (DOI 10.1038/s41592-020-0793-2).” → B  \n",
    "4) “See Supplementary Data at Zenodo (10.5281/zenodo.987654).” → A  \n",
    "5) “Method details published in J. Proteome Res. DOI: 10.1021/acs.jproteome.0c00845.” → B  \n",
    "6) “Data has been uploaded to Dryad (10.5061/dryad.x1y2z3).” → A  \n",
    "7) “Referenced paper: DOI 10.1101/2020.01.01.123456 (bioRxiv preprint).” → B  \n",
    "8) “Metabolomics data in MetaboLights MTBLS1234.” → A  \n",
    "\n",
    "=== Instructions ===\n",
    "- Use only the identifier itself and its context.  \n",
    "- If the DOI prefix is in the list above, always choose A.  \n",
    "- If it belongs to a known publisher prefix (e.g. 10.1007, 10.1038, 10.1126, 10.1016…), choose B.  \n",
    "- Otherwise, rely on context words (“deposited”, “uploaded”, “archived”) to decide.  \n",
    "- Output exactly one letter: A or B, and nothing else.\n",
    "\"\"\".strip()\n",
    "\n",
    "def build_df():\n",
    "    df = pl.read_parquet('/tmp/extracted.parquet')\n",
    "    df.filter(~is_doi_link('dataset_id')).select('article_id', 'dataset_id').write_csv('/tmp/accid_sub.csv')\n",
    "    return df.filter(is_doi_link('dataset_id'))\n",
    "\n",
    "def build_prompt(tokenizer, df):\n",
    "    prompts = []\n",
    "    for doi, text in df.select('dataset_id', 'window').rows():\n",
    "        messages = [{'role':'system','content': SYS_PROMPT_CLASSIFY_DOI}, {'role':'user', 'content': text}]\n",
    "        prompts.append(tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False, enable_thinking=True))\n",
    "    return df.with_columns(pl.Series('prompt', prompts))\n",
    "\n",
    "if __name__=='__main__':\n",
    "    os.environ[\"VLLM_USE_V1\"] = \"0\"\n",
    "    import vllm\n",
    "    from logits_processor_zoo.vllm import MultipleChoiceLogitsProcessor\n",
    "    model_path = '/kaggle/input/qwen-3/transformers/8b-awq/1'#\"/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1\"\n",
    "    llm = vllm.LLM(model_path, quantization='awq', tensor_parallel_size=2, gpu_memory_utilization=0.9, trust_remote_code=True, dtype=\"half\", enforce_eager=True, max_model_len=2048, disable_log_stats=True, disable_custom_all_reduce=True, enable_prefix_caching=True, task='generate')\n",
    "    tokenizer = llm.get_tokenizer()\n",
    "    df = build_df()\n",
    "    df = build_prompt(tokenizer, df)\n",
    "    prompts = df['prompt'].to_list()\n",
    "    mclp = MultipleChoiceLogitsProcessor(tokenizer, choices=[\"A\", \"B\"])\n",
    "    outputs = llm.generate(prompts, vllm.SamplingParams(seed=777, temperature=0.1, skip_special_tokens=True, max_tokens=1, logits_processors=[mclp], logprobs=len(mclp.choices)), use_tqdm=True)\n",
    "    logprobs = [{lp.decoded_token: lp.logprob for lp in list(lps)} for lps in [output.outputs[0].logprobs[0].values() for output in outputs]]\n",
    "    choices = [max(d, key=d.get) for d in logprobs]\n",
    "    types = {'A': True, 'B': False}\n",
    "    choices = [types[c] for c in choices]\n",
    "    df = df.with_columns(pl.Series('type', choices))\n",
    "    df.filter(pl.col('type')).select('article_id', 'dataset_id').write_csv('/tmp/doi_sub.csv')\n",
    "    df = pl.concat([pl.read_csv('/tmp/doi_sub.csv'), pl.read_csv('/tmp/accid_sub.csv')])\n",
    "    df = assume_type(df)\n",
    "    df.select(['article_id', 'dataset_id', 'type']).with_row_index(name='row_id').write_csv('/kaggle/working/submission.csv')\n",
    "    if not IS_KAGGLE_SUBMISSION:\n",
    "        results = evaluate(df)\n",
    "        for r in results: l.info(r) \n",
    "        results = evaluate(df, on=['article_id', 'dataset_id', 'type'])\n",
    "        for r in results: l.info(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a6f0c22",
   "metadata": {
    "_cell_guid": "291dc09f-eda0-41f7-b598-6f3b6b8e28e6",
    "_kg_hide-output": true,
    "_uuid": "486d0d0b-1cb1-4046-8929-c7a4c6843d6c",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-07-25T11:51:22.549310Z",
     "iopub.status.busy": "2025-07-25T11:51:22.549127Z",
     "iopub.status.idle": "2025-07-25T11:51:31.036184Z",
     "shell.execute_reply": "2025-07-25T11:51:31.035350Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 8.500446,
     "end_time": "2025-07-25T11:51:31.037666",
     "exception": false,
     "start_time": "2025-07-25T11:51:22.537220",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp\n"
     ]
    }
   ],
   "source": [
    "%cd /tmp\n",
    "!LOG_LEVEL=INFO python src/parse.py /tmp/train_parse\n",
    "! python src/check_parse.py\n",
    "! python src/getid.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c1f53ac",
   "metadata": {
    "_cell_guid": "291dc09f-eda0-41f7-b598-6f3b6b8e28e6",
    "_kg_hide-output": true,
    "_uuid": "486d0d0b-1cb1-4046-8929-c7a4c6843d6c",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-07-25T11:51:31.059456Z",
     "iopub.status.busy": "2025-07-25T11:51:31.058908Z",
     "iopub.status.idle": "2025-07-25T11:54:06.062812Z",
     "shell.execute_reply": "2025-07-25T11:54:06.062026Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 155.016036,
     "end_time": "2025-07-25T11:54:06.064177",
     "exception": false,
     "start_time": "2025-07-25T11:51:31.048141",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-25 11:51:46.046480: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1753444306.234280     160 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1753444306.287569     160 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "INFO 07-25 11:51:59 [__init__.py:244] Automatically detected platform cuda.\r\n",
      "INFO 07-25 11:52:16 [config.py:1472] Using max model len 2048\r\n",
      "WARNING 07-25 11:52:17 [config.py:960] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\r\n",
      "WARNING 07-25 11:52:18 [cuda.py:102] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\r\n",
      "INFO 07-25 11:52:18 [llm_engine.py:230] Initializing a V0 LLM engine (v0.9.2) with config: model='/kaggle/input/qwen-3/transformers/8b-awq/1', speculative_config=None, tokenizer='/kaggle/input/qwen-3/transformers/8b-awq/1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=awq, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='xgrammar', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=None, served_model_name=/kaggle/input/qwen-3/transformers/8b-awq/1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=False, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":false,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":0,\"local_cache_dir\":null}, use_cached_outputs=False, \r\n",
      "WARNING 07-25 11:52:18 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 2 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=188)\u001b[0;0m INFO 07-25 11:52:18 [multiproc_worker_utils.py:226] Worker ready; awaiting tasks\r\n",
      "INFO 07-25 11:52:19 [cuda.py:311] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 07-25 11:52:19 [cuda.py:360] Using XFormers backend.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=188)\u001b[0;0m INFO 07-25 11:52:19 [cuda.py:311] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=188)\u001b[0;0m INFO 07-25 11:52:19 [cuda.py:360] Using XFormers backend.\r\n",
      "[W725 11:52:30.638562323 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W725 11:52:31.205860499 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W725 11:52:40.649225883 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W725 11:52:50.657953028 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "INFO 07-25 11:52:50 [__init__.py:1152] Found nccl from library libnccl.so.2\r\n",
      "INFO 07-25 11:52:50 [pynccl.py:70] vLLM is using nccl==2.26.2\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=188)\u001b[0;0m INFO 07-25 11:52:50 [__init__.py:1152] Found nccl from library libnccl.so.2\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=188)\u001b[0;0m INFO 07-25 11:52:50 [pynccl.py:70] vLLM is using nccl==2.26.2\r\n",
      "INFO 07-25 11:52:50 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_e83c2191'), local_subscribe_addr='ipc:///tmp/9538f912-1dad-436b-b507-62bcea04119f', remote_subscribe_addr=None, remote_addr_ipv6=False)\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=188)\u001b[0;0m INFO 07-25 11:52:50 [parallel_state.py:1076] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\r\n",
      "INFO 07-25 11:52:50 [parallel_state.py:1076] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\r\n",
      "INFO 07-25 11:52:50 [model_runner.py:1171] Starting to load model /kaggle/input/qwen-3/transformers/8b-awq/1...\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=188)\u001b[0;0m INFO 07-25 11:52:51 [model_runner.py:1171] Starting to load model /kaggle/input/qwen-3/transformers/8b-awq/1...\r\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\r\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:40<00:40, 40.81s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:46<00:00, 20.39s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:46<00:00, 23.45s/it]\r\n",
      "\r\n",
      "INFO 07-25 11:53:38 [default_loader.py:272] Loading weights took 46.94 seconds\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=188)\u001b[0;0m INFO 07-25 11:53:38 [default_loader.py:272] Loading weights took 46.59 seconds\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=188)\u001b[0;0m INFO 07-25 11:53:39 [model_runner.py:1203] Model loading took 2.8510 GiB and 46.829959 seconds\r\n",
      "INFO 07-25 11:53:39 [model_runner.py:1203] Model loading took 2.8510 GiB and 47.185770 seconds\r\n",
      "\r\n",
      "\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=188)\u001b[0;0m INFO 07-25 11:53:45 [worker.py:294] model weights take 2.85GiB; non_torch_memory takes 0.11GiB; PyTorch activation peak memory takes 0.20GiB; the rest of the memory reserved for KV Cache is 10.12GiB.\r\n",
      "\r\n",
      "\r\n",
      "INFO 07-25 11:53:45 [worker.py:294] model weights take 2.85GiB; non_torch_memory takes 0.11GiB; PyTorch activation peak memory takes 1.40GiB; the rest of the memory reserved for KV Cache is 8.91GiB.\r\n",
      "INFO 07-25 11:53:46 [executor_base.py:113] # cuda blocks: 8108, # CPU blocks: 3640\r\n",
      "INFO 07-25 11:53:46 [executor_base.py:118] Maximum concurrency for 2048 tokens per request: 63.34x\r\n",
      "INFO 07-25 11:53:50 [llm_engine.py:428] init engine (profile, create kv cache, warmup model) took 10.84 seconds\r\n",
      "Adding requests: 100%|█████████████████████████| 19/19 [00:00<00:00, 536.40it/s]\r\n",
      "Processed prompts: 100%|█| 19/19 [00:06<00:00,  2.72it/s, est. speed input: 2087\r\n",
      "INFO 07-25 11:53:59 [multiproc_worker_utils.py:125] Killing local vLLM worker processes\r\n",
      "[rank0]:[W725 11:54:01.305859681 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\r\n"
     ]
    }
   ],
   "source": [
    "! python src/llm_validate.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7bff9170",
   "metadata": {
    "_cell_guid": "bfcdb304-165a-40ea-a852-98750ed8d796",
    "_kg_hide-input": true,
    "_uuid": "4ae80ba9-edde-44eb-92be-f683b7cba0c0",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-07-25T11:54:06.088573Z",
     "iopub.status.busy": "2025-07-25T11:54:06.088320Z",
     "iopub.status.idle": "2025-07-25T11:54:06.205809Z",
     "shell.execute_reply": "2025-07-25T11:54:06.205018Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.131018,
     "end_time": "2025-07-25T11:54:06.207066",
     "exception": false,
     "start_time": "2025-07-25T11:54:06.076048",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "! cat /tmp/logs/project.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf86087",
   "metadata": {
    "papermill": {
     "duration": 0.011285,
     "end_time": "2025-07-25T11:54:06.230410",
     "exception": false,
     "start_time": "2025-07-25T11:54:06.219125",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 13015230,
     "sourceId": 82370,
     "sourceType": "competition"
    },
    {
     "datasetId": 7850099,
     "sourceId": 12444547,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 248118764,
     "sourceType": "kernelVersion"
    },
    {
     "modelId": 164048,
     "modelInstanceId": 141565,
     "sourceId": 166368,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 322000,
     "modelInstanceId": 322452,
     "sourceId": 391615,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 205.59771,
   "end_time": "2025-07-25T11:54:06.559149",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-07-25T11:50:40.961439",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
