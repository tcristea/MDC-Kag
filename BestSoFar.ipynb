{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c20880ac",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-06T08:08:47.771115Z",
     "iopub.status.busy": "2025-07-06T08:08:47.770801Z",
     "iopub.status.idle": "2025-07-06T08:08:49.189678Z",
     "shell.execute_reply": "2025-07-06T08:08:49.188408Z"
    },
    "papermill": {
     "duration": 1.424922,
     "end_time": "2025-07-06T08:08:49.191552",
     "exception": false,
     "start_time": "2025-07-06T08:08:47.766630",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "! uv pip install -q --system --no-index --find-links='/kaggle/input/latest-mdc-whls/whls' 'pymupdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9dd9903f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-06T08:08:49.200193Z",
     "iopub.status.busy": "2025-07-06T08:08:49.199791Z",
     "iopub.status.idle": "2025-07-06T08:08:49.320981Z",
     "shell.execute_reply": "2025-07-06T08:08:49.319672Z"
    },
    "papermill": {
     "duration": 0.1273,
     "end_time": "2025-07-06T08:08:49.323066",
     "exception": false,
     "start_time": "2025-07-06T08:08:49.195766",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "! mkdir src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e128aaa9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-06T08:08:49.330333Z",
     "iopub.status.busy": "2025-07-06T08:08:49.329985Z",
     "iopub.status.idle": "2025-07-06T08:08:49.338407Z",
     "shell.execute_reply": "2025-07-06T08:08:49.337069Z"
    },
    "papermill": {
     "duration": 0.014294,
     "end_time": "2025-07-06T08:08:49.340301",
     "exception": false,
     "start_time": "2025-07-06T08:08:49.326007",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/common.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/common.py\n",
    "import os\n",
    "import polars as pl\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "\n",
    "DOI_URL = 'https://doi.org/'\n",
    "\n",
    "def is_submission(): return bool(os.getenv('KAGGLE_IS_COMPETITION_RERUN'))\n",
    "def is_kaggle_env(): return (len([k for k in os.environ.keys() if 'KAGGLE' in k]) > 0) or is_submission()\n",
    "\n",
    "def get_prefix_path(prefix: str)->Path:\n",
    "    return Path(f'/kaggle/{prefix}' if is_kaggle_env() else f'.{prefix}').expanduser().resolve()\n",
    "\n",
    "def is_doi(name:str)->pl.Expr: return pl.col(name).str.starts_with(DOI_URL)\n",
    "\n",
    "def doi_link_to_id(name:str)->pl.Expr:\n",
    "    return pl.when(is_doi(name)).then(pl.col(name).str.split(DOI_URL).list.last()).otherwise(name).alias(name)\n",
    "\n",
    "def doi_id_to_link(name:str, substring:str, url:str=DOI_URL)->pl.Expr:\n",
    "    return pl.when(pl.col(name).str.starts_with(substring)).then(url+pl.col(name).str.to_lowercase()).otherwise(name).alias(name)\n",
    "\n",
    "def score(preds: pl.DataFrame, gt: pl.DataFrame, on: list = ['article_id', 'dataset_id'], verbose:bool=True) -> Tuple[float, float, float]:\n",
    "    if 'id' in preds.columns and 'dataset_id' not in preds.columns: preds = preds.rename({'id': 'dataset_id'})\n",
    "    hits = gt.join(preds, on=on)\n",
    "    tp = hits.height\n",
    "    fp = preds.height - tp\n",
    "    fn = gt.height - tp\n",
    "\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
    "        print(f\"True Positives: {tp}, False Positives: {fp}, False Negatives: {fn}\")\n",
    "\n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d198df5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-06T08:08:49.346936Z",
     "iopub.status.busy": "2025-07-06T08:08:49.346619Z",
     "iopub.status.idle": "2025-07-06T08:08:49.352765Z",
     "shell.execute_reply": "2025-07-06T08:08:49.351948Z"
    },
    "papermill": {
     "duration": 0.011128,
     "end_time": "2025-07-06T08:08:49.354096",
     "exception": false,
     "start_time": "2025-07-06T08:08:49.342968",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/parse.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/parse.py\n",
    "import argparse\n",
    "import pymupdf\n",
    "import pathlib\n",
    "import tqdm\n",
    "\n",
    "from common import get_prefix_path, is_submission\n",
    "\n",
    "def get_args():\n",
    "    p = argparse.ArgumentParser()\n",
    "    p.add_argument('-i', default=f'make-data-count-finding-data-references/{\"test\" if is_submission() else \"train\"}/PDF')\n",
    "    p.add_argument('-o', default='parsed')\n",
    "    return p.parse_args()\n",
    "\n",
    "def pdf2text(path: pathlib.Path, out_dir: pathlib.Path) -> None:\n",
    "    doc = pymupdf.open(str(path))\n",
    "    out = open(out_dir / f\"{path.stem}.txt\", \"wb\")\n",
    "    for page in doc:\n",
    "        text = page.get_text().encode(\"utf8\")\n",
    "        out.write(text)\n",
    "        out.write(b'\\n') # write page delimiter (form feed 0x0C)\n",
    "    out.close()\n",
    "\n",
    "def main():\n",
    "    args = get_args()\n",
    "    in_dir = get_prefix_path('input') / args.i\n",
    "    out_dir = get_prefix_path('working') / args.o\n",
    "\n",
    "    if out_dir.exists() and any(out_dir.iterdir()):\n",
    "        print(f'{out_dir} already populated, skipping...')\n",
    "        return\n",
    "\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    if not in_dir.is_dir(): raise ValueError(f'{in_dir} is not a directory...')\n",
    "    pdf_files = list(in_dir.glob('*.pdf'))\n",
    "    if not pdf_files: raise ValueError(f'No PDF files found in {in_dir}')\n",
    "\n",
    "    for pdf in tqdm.tqdm(pdf_files, desc=\"Processing PDFs\"): pdf2text(pdf, out_dir)\n",
    "    print('ending parsing...')\n",
    "\n",
    "if __name__ == '__main__': main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d51d947",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-06T08:08:49.361003Z",
     "iopub.status.busy": "2025-07-06T08:08:49.360696Z",
     "iopub.status.idle": "2025-07-06T08:08:49.369037Z",
     "shell.execute_reply": "2025-07-06T08:08:49.368132Z"
    },
    "papermill": {
     "duration": 0.013737,
     "end_time": "2025-07-06T08:08:49.370464",
     "exception": false,
     "start_time": "2025-07-06T08:08:49.356727",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/getacc.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/getacc.py\n",
    "import polars as pl\n",
    "import argparse\n",
    "import pathlib\n",
    "from common import score, get_prefix_path, is_submission, is_doi, doi_id_to_link\n",
    "\n",
    "def get_args():\n",
    "    p = argparse.ArgumentParser()\n",
    "    p.add_argument('-i', default='parsed')\n",
    "    p.add_argument('-o', default='extracted_ids.parquet')\n",
    "    p.add_argument('--gt', default='make-data-count-finding-data-references/train_labels.csv')\n",
    "    p.add_argument('--ws', default=100, type=int)\n",
    "    return p.parse_args()\n",
    "\n",
    "def get_text_df(parsed_dir: pathlib.Path):\n",
    "    paths = list(parsed_dir.rglob('*.txt'))\n",
    "    records = [{'article_id': p.stem, 'text': p.read_text()} for p in paths]\n",
    "    return (\n",
    "        pl.DataFrame(records)\n",
    "        .with_columns(pl.col(\"text\").str.normalize(\"NFKC\").str.replace_all(r\"[^\\p{Ascii}]\", \"\"))\n",
    "        .with_columns(pl.col('text').str.split(r'\\n{2,}').list.eval(pl.col(\"\").str.replace_all('\\n', ' ')).list.join('\\n').alias('text'))\n",
    "        .with_columns([\n",
    "            pl.col(\"text\").str.slice(pl.col(\"text\").str.len_chars()//4).str.reverse().alias('rtext'),\n",
    "            pl.col(\"text\").str.slice(0, pl.col(\"text\").str.len_chars()//4).alias('ltext'),\n",
    "        ])\n",
    "        .with_columns(pl.col('rtext').str.find(r'(?i)\\b(secnerefer|erutaretil detic|stnemegdelwonkca)\\b').alias('ref_idx'))\n",
    "        .with_columns(pl.when(pl.col('ref_idx').is_null()).then(0).otherwise('ref_idx').alias('ref_idx'))\n",
    "        .with_columns([\n",
    "            pl.col('rtext').str.slice(0, pl.col('ref_idx')).str.reverse().alias('refs'),\n",
    "            (pl.col('ltext') + pl.col('rtext').str.slice(pl.col('ref_idx')).str.reverse()).alias('body')\n",
    "        ])\n",
    "        .drop('rtext', 'ltext')\n",
    "    )\n",
    "\n",
    "\n",
    "def main():\n",
    "    print('starting extraction of accession ids')\n",
    "    args = get_args()\n",
    "    in_path, out_path = map(lambda x: get_prefix_path('working') / x, (args.i, args.o))\n",
    "    text_df = get_text_df(in_path)\n",
    "\n",
    "    df = (\n",
    "        text_df\n",
    "        .with_columns([\n",
    "            pl.col(\"text\").str.extract_all(r'(?i)\\b(?:CHEMBL\\d+|E-GEOD-\\d+|E-PROT-\\d+|EMPIAR-\\d+|ENSBTAG\\d+|ENSOARG\\d+|EPI_ISL_\\d{5,}|EPI\\d{6,7}|HPA\\d+|CP\\d{6}|IPR\\d{6}|PF\\d{5}|KX\\d{6}|K0\\d{4}|PRJNA\\d+|PXD\\d+|SAMN\\d+|dryad\\.[^\\s\"<>]+|pasta\\/[^\\s\"<>])').alias('id'),\n",
    "        ])\n",
    "        .explode('id')\n",
    "        .with_columns(pl.col('id').alias('match_id'))\n",
    "        .with_columns(pl.col('id').str.replace_all(r'\\s', ''))\n",
    "        .with_columns(pl.col('id').str.replace(r'[-.,;:!?\\/\\)\\]\\(\\[]+$', ''))\n",
    "        .with_columns(doi_id_to_link(name='id', substring='dryad.', url='https://doi.org/10.5061/'))\n",
    "        .with_columns(doi_id_to_link(name='id', substring='pasta/', url='https://doi.org/10.6073/'))\n",
    "        .filter(~pl.col('id').str.to_lowercase().str.contains(pl.col('article_id').str.to_lowercase().str.replace('_', '/')))\n",
    "        .filter(~pl.col('id').str.contains('figshare', literal=True))\n",
    "        .filter(pl.when(is_doi('id').and_(pl.col('id').str.split('/').list.last().str.len_chars()<4)).then(pl.lit(False)).otherwise(pl.lit(True)))\n",
    "        .filter(~pl.col('id').is_in(['https://doi.org/10.5061/dryad', 'https://doi.org/10.6073/pasta', 'https://doi.org/10.5281/zenodo']))\n",
    "        .filter(pl.col('id').str.count_matches(r'\\(') == pl.col('id').str.count_matches(r'\\)'))\n",
    "        .filter(pl.col('id').str.count_matches(r'\\[') == pl.col('id').str.count_matches(r'\\]'))\n",
    "        .with_columns(\n",
    "            pl.col('text').str.slice(pl.col('text').str.find(pl.col('match_id'), literal=True)-args.ws-pl.col('match_id').str.len_chars(), 2*(args.ws+pl.col('match_id').str.len_chars())).alias('window')\n",
    "        )\n",
    "        .unique(['article_id', 'id'])\n",
    "        .rename({'id': 'dataset_id'})\n",
    "    )\n",
    "    df.select('article_id', 'dataset_id', 'window').write_parquet(out_path)\n",
    "    print(f'id extraction written to {out_path}')\n",
    "\n",
    "    df = df.select('article_id', 'dataset_id').with_columns(pl.lit('Secondary').alias('type'))\n",
    "    df = df.with_columns(\n",
    "        pl.when(is_doi('dataset_id').or_(pl.col('dataset_id').str.starts_with('SAMN'))).then(pl.lit('Primary')).otherwise('type').alias('type')\n",
    "    )\n",
    "\n",
    "    df.with_row_index(name='row_id').write_csv(get_prefix_path('working')/'submission.csv')\n",
    "\n",
    "    if not is_submission():\n",
    "        gt_path = get_prefix_path('input') / args.gt\n",
    "        gt = pl.read_csv(gt_path).filter(pl.col('type')!='Missing').join(text_df, on='article_id')\n",
    "        print('### DOI ###')\n",
    "        score(df.filter(is_doi('dataset_id')), gt.filter(is_doi('dataset_id')))\n",
    "        print('### ACC ###')\n",
    "        score(df.filter(~is_doi('dataset_id')), gt.filter(~is_doi('dataset_id')))\n",
    "        print('### ALL ###')\n",
    "        score(df, gt)\n",
    "        print('### TYPE ###')\n",
    "        score(df, gt, on=['article_id', 'dataset_id', 'type'])\n",
    "\n",
    "if __name__=='__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f20e3ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-06T08:08:49.377074Z",
     "iopub.status.busy": "2025-07-06T08:08:49.376730Z",
     "iopub.status.idle": "2025-07-06T08:10:33.839568Z",
     "shell.execute_reply": "2025-07-06T08:10:33.838278Z"
    },
    "papermill": {
     "duration": 104.468312,
     "end_time": "2025-07-06T08:10:33.841448",
     "exception": false,
     "start_time": "2025-07-06T08:08:49.373136",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  13%|███▏                     | 67/524 [00:11<02:01,  3.75it/s]MuPDF error: unsupported error: cannot create appearance stream for  annotations\r\n",
      "\r\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\r\n",
      "\r\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\r\n",
      "\r\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\r\n",
      "\r\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\r\n",
      "\r\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\r\n",
      "\r\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\r\n",
      "\r\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\r\n",
      "\r\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\r\n",
      "\r\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\r\n",
      "\r\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\r\n",
      "\r\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\r\n",
      "\r\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\r\n",
      "\r\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\r\n",
      "\r\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\r\n",
      "\r\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\r\n",
      "\r\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\r\n",
      "\r\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\r\n",
      "\r\n",
      "Processing PDFs: 100%|████████████████████████| 524/524 [01:42<00:00,  5.12it/s]\r\n",
      "ending parsing...\r\n"
     ]
    }
   ],
   "source": [
    "! python src/parse.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19f32f99",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-06T08:10:33.888485Z",
     "iopub.status.busy": "2025-07-06T08:10:33.888134Z",
     "iopub.status.idle": "2025-07-06T08:10:37.416471Z",
     "shell.execute_reply": "2025-07-06T08:10:37.415481Z"
    },
    "papermill": {
     "duration": 3.553536,
     "end_time": "2025-07-06T08:10:37.418247",
     "exception": false,
     "start_time": "2025-07-06T08:10:33.864711",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting extraction of accession ids\r\n",
      "id extraction written to /kaggle/working/extracted_ids.parquet\r\n",
      "### DOI ###\r\n",
      "Precision: 0.9265, Recall: 0.1938, F1: 0.3206\r\n",
      "True Positives: 63, False Positives: 5, False Negatives: 262\r\n",
      "### ACC ###\r\n",
      "Precision: 0.7188, Recall: 0.8046, F1: 0.7593\r\n",
      "True Positives: 317, False Positives: 124, False Negatives: 77\r\n",
      "### ALL ###\r\n",
      "Precision: 0.7466, Recall: 0.5285, F1: 0.6189\r\n",
      "True Positives: 380, False Positives: 129, False Negatives: 339\r\n",
      "### TYPE ###\r\n",
      "Precision: 0.6621, Recall: 0.4687, F1: 0.5489\r\n",
      "True Positives: 337, False Positives: 172, False Negatives: 382\r\n"
     ]
    }
   ],
   "source": [
    "! python src/getacc.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce293927",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-06T08:10:37.465325Z",
     "iopub.status.busy": "2025-07-06T08:10:37.464336Z",
     "iopub.status.idle": "2025-07-06T08:10:37.843312Z",
     "shell.execute_reply": "2025-07-06T08:10:37.842025Z"
    },
    "papermill": {
     "duration": 0.404515,
     "end_time": "2025-07-06T08:10:37.845155",
     "exception": false,
     "start_time": "2025-07-06T08:10:37.440640",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "! rm -rf parsed\n",
    "! rm -rf src\n",
    "! rm -rf extracted_ids.parquet"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 12656064,
     "isSourceIdPinned": false,
     "sourceId": 82370,
     "sourceType": "competition"
    },
    {
     "sourceId": 248118764,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 115.784573,
   "end_time": "2025-07-06T08:10:38.388550",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-07-06T08:08:42.603977",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
