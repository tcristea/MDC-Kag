{
  "nbformat": 4,
  "nbformat_minor": 0,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["# âœ… Clean Inference Notebook Template\n\nThis notebook includes:\n- Robust Marker-pdf extraction with error handling\n- Text cleaning and DOI extraction\n- Accession ID fallback\n- Model loading and prediction\n- Submission file creation\n"]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": ["# ðŸ“¦ Imports\nimport os\nimport re\nimport pandas as pd\nfrom glob import glob\nfrom tqdm import tqdm\nimport torch\nfrom marker import Document\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification"]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": ["# âœ… Load model and tokenizer\nmodel_path = \"/kaggle/input/...\"\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_path).eval()\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)"]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": ["# âœ… Utility functions\ndef extract_article_id(filename):\n    return filename[:-4].replace('_','/')\n\ndef clean_doi(doi):\n    return doi.rstrip(')]>.,;')\n\ndef standardize_doi(doi):\n    doi = str(doi).strip()\n    if doi.startswith('http'): return doi.lower()\n    if doi.startswith('doi:'): return 'https://doi.org/' + doi[4:].lower()\n    if doi.startswith('10.'): return 'https://doi.org/' + doi.lower()\n    return doi.lower()\n\ndef extract_dataset_dois(text):\n    pattern = r'https?://doi\\.org/10\\.[^\\s\\)<>]+'\n    return sorted(set(standardize_doi(m) for m in re.findall(pattern, text)))\n\ndef find_accession_ids_in_text(text):\n    patterns = [r'\\b(GSE\\d+)', r'\\b(SRP\\d+)']\n    matches = []\n    for pat in patterns:\n        matches.extend(re.findall(pat, text, re.IGNORECASE))\n    return [m.lower() for m in matches]"]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": ["# âœ… Extract text with Marker-pdf\ndef extract_text(pdf_path):\n    try:\n        doc = Document(pdf_path)\n        return doc.text or ''\n    except Exception as e:\n        print(f'Error extracting {pdf_path}: {e}')\n        return ''"]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": ["# âœ… Process all PDFs\npdf_dir = '/kaggle/input/.../test/PDF'\npdf_files = sorted(glob(os.path.join(pdf_dir, '*.pdf')))\n\nrows = []\nfor pdf_path in tqdm(pdf_files):\n    aid = extract_article_id(os.path.basename(pdf_path))\n    text = extract_text(pdf_path)\n    rows.append({'article_id': aid, 'text': text})"]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": ["# âœ… Predict\nlabel_map = {0:'Primary',1:'Secondary',2:'Missing'}\npreds = []\nfor i in range(0, len(rows), 8):\n    batch = tokenizer([r['text'] for r in rows[i:i+8]], truncation=True, padding=True, max_length=512, return_tensors='pt').to(device)\n    with torch.no_grad():\n        logits = model(**batch).logits\n        preds.extend(torch.argmax(logits, axis=1).cpu().tolist())"]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": ["# âœ… Assemble submission\nresults = []\nfor i, r in enumerate(rows):\n    dois = extract_dataset_dois(r['text'])\n    accs = find_accession_ids_in_text(r['text'])\n    dataset_id = dois[0] if dois else (accs[0] if accs else '')\n    results.append({'article_id': r['article_id'].replace('/','_'), 'dataset_id': dataset_id, 'type': label_map[preds[i]]})\n\ndf = pd.DataFrame(results)\ndf = df[df.type != 'Missing'].drop_duplicates()\ndf.insert(0,'row_id',range(len(df)))\ndf.to_csv('submission.csv', index=False)"]
  }
 ]
}
