{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":82370,"databundleVersionId":13015230,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":12441074,"sourceType":"datasetVersion","datasetId":7847931},{"sourceId":12444547,"sourceType":"datasetVersion","datasetId":7850099},{"sourceId":248033444,"sourceType":"kernelVersion"},{"sourceId":166368,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":141565,"modelId":164048},{"sourceId":391615,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":322452,"modelId":322000}],"dockerImageVersionId":31090,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":2762.674057,"end_time":"2025-07-09T11:42:12.546859","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-07-09T10:56:09.872802","version":"2.6.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"0285b679c5d5424091163788c567102e":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"04152031458b42bbbe65821bc4c896af":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"05b2e0e258314f56929c90293c7dfbfe":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"05f988bec4ca42f39166d015b8ce8fbc":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"070d1eda42f14bd8af2b65cfccb9326c":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_cd851d37c64a4c22aa7ab643a82d59f7","placeholder":"​","style":"IPY_MODEL_a0d7b2eb7012498fbfb0640cdf3fcdba","tabbable":null,"tooltip":null,"value":"Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:45&lt;00:00, 19.71s/it]\n"}},"08edaba3f03f4bacaeaf0b24aa820fd1":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_0b101796756d4bae86ceadfd97fa33b1","max":1009,"min":0,"orientation":"horizontal","style":"IPY_MODEL_94c7c21bad8f4ec3809ee47be49ea8f1","tabbable":null,"tooltip":null,"value":1009}},"0b101796756d4bae86ceadfd97fa33b1":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0bd3c01d1cda44f289e7a40962a6b3c3":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"1856cad6218e44ee9a54d4dbf6aa806a":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_f667478076004405be8ea47ccf7b33dd","placeholder":"​","style":"IPY_MODEL_282f3d0360e8407d9b766391684ce6ed","tabbable":null,"tooltip":null,"value":" 524/524 [01:40&lt;00:00,  6.38it/s]"}},"1b4dec9d79f248669ae507d2f2524dc8":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1cbeacfcc18f47efa0d9a386ca0629c1":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1dc80828667f4df38aa1aab1fd9f2f44":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"226ee086d0dc4eb7875c9fdca6b5af46":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"23211fffc2ce44ae94a826e6ec42c114":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"27cfff0a9bf247ca96d42e5ee66e4a5c":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"282f3d0360e8407d9b766391684ce6ed":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"2a2d75ccbb4844368b7354f845258676":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"2efcdb28fc784b0689e5b4074bbed7d9":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2f4cabddcb7c4f11ae989079004da419":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"32c863afe4104f6dbec4eb0e90a6adc7":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3e3f6f18a05348a1a9f3f4f48fb0c980","IPY_MODEL_83f1c1607ea64afab5850aeb66287392","IPY_MODEL_bbe972fca4404c639fe842ee6c33cd7f"],"layout":"IPY_MODEL_39e546ce50d740b3ae89ae7a0c937d4c","tabbable":null,"tooltip":null}},"367d11eca8544e1db8d38e76c4af3977":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_381674bb74e34564be22274eed9db747","placeholder":"​","style":"IPY_MODEL_0bd3c01d1cda44f289e7a40962a6b3c3","tabbable":null,"tooltip":null,"value":"Processed prompts: 100%"}},"381674bb74e34564be22274eed9db747":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"39e546ce50d740b3ae89ae7a0c937d4c":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3e3f6f18a05348a1a9f3f4f48fb0c980":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_b68421a54f4a4c149aa5ecbe3cd55f02","placeholder":"​","style":"IPY_MODEL_27cfff0a9bf247ca96d42e5ee66e4a5c","tabbable":null,"tooltip":null,"value":"Adding requests: 100%"}},"410675309924442e85b2fcb36affee2d":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_5b5e09b8568b48a6a84730755ba5bca3","max":684,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d95cef64da1041a9952f71cf230701dc","tabbable":null,"tooltip":null,"value":684}},"43557c61adca4747bee979d264d94415":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"478669d8fe0d4c12b35791f5ff985253":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4978fbfcf9ed4feea579e1040af9b62a":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_6f4b4d2bf78c4faf83da02f9e63af0aa","max":1090,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f6c9297c49b34f3eb85eb5c2dc3aa3a6","tabbable":null,"tooltip":null,"value":1090}},"4c28649854b149f1990772971e5d8d16":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4c919e73f69d4eeeb500d5b5ce8beeeb":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_ad989c8e3705455284ae57a55ff5a691","max":524,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fb20c08457994f38a683608a6bbc6b9b","tabbable":null,"tooltip":null,"value":524}},"4e18685c620d45d98937b3961ab64792":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_0285b679c5d5424091163788c567102e","placeholder":"​","style":"IPY_MODEL_05b2e0e258314f56929c90293c7dfbfe","tabbable":null,"tooltip":null,"value":"Processed prompts: 100%"}},"57181779039d4251826180814edfdf1e":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"5b5e09b8568b48a6a84730755ba5bca3":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5b6c4645c40b496c8d44348011243acb":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e322fd37704b43299fd24f9bcd2cdf75","IPY_MODEL_4c919e73f69d4eeeb500d5b5ce8beeeb","IPY_MODEL_1856cad6218e44ee9a54d4dbf6aa806a"],"layout":"IPY_MODEL_8cddf0f55381424e8031bb5fb3d5254c","tabbable":null,"tooltip":null}},"5e0944e62904477db3820ebb7cf717ae":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_6a1ce50b35064644a5486b4e7ab8584d","placeholder":"​","style":"IPY_MODEL_b26d9bc24e2c4e2796edbeaffb47904a","tabbable":null,"tooltip":null,"value":" 684/684 [12:01&lt;00:00,  1.76s/it, est. speed input: 212.91 toks/s, output: 483.39 toks/s]"}},"6784c92c166049f2bb5916a237e530a3":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"68604161b8af47809a2a76b938b8508d":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6a1ce50b35064644a5486b4e7ab8584d":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6f4b4d2bf78c4faf83da02f9e63af0aa":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"724d12e4c4514630b8e09b0578a88c71":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_68604161b8af47809a2a76b938b8508d","placeholder":"​","style":"IPY_MODEL_226ee086d0dc4eb7875c9fdca6b5af46","tabbable":null,"tooltip":null,"value":""}},"77d02ffdac924d9d9d935e509f6147f7":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_2efcdb28fc784b0689e5b4074bbed7d9","placeholder":"​","style":"IPY_MODEL_90a02d5fb994403cb4c890e5a5100822","tabbable":null,"tooltip":null,"value":" 1009/1009 [00:01&lt;00:00, 827.31it/s]"}},"7920929ff87d4e26a08e844c469ac1be":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"813af15899e34344801a4b239b60e34e":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_05f988bec4ca42f39166d015b8ce8fbc","placeholder":"​","style":"IPY_MODEL_ab8175c93bc44c1ab4cccc61a7960fce","tabbable":null,"tooltip":null,"value":" 1090/1090 [28:05&lt;00:00,  6.14s/it, est. speed input: 369.96 toks/s, output: 327.36 toks/s]"}},"83f1c1607ea64afab5850aeb66287392":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_923302f4beed485c9e39bb6743aafbac","max":1090,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e8a0c7eb0d1a42fcb5af6f1bad452c68","tabbable":null,"tooltip":null,"value":1090}},"87be2f20dd894e88bc9ba2abdceea1f5":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8cddf0f55381424e8031bb5fb3d5254c":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8ec2438933df427f86aa576b64df33bb":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_7920929ff87d4e26a08e844c469ac1be","placeholder":"​","style":"IPY_MODEL_a10b5695e7974030b02a9d17ea434ecd","tabbable":null,"tooltip":null,"value":"Processed prompts: 100%"}},"90a02d5fb994403cb4c890e5a5100822":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"923302f4beed485c9e39bb6743aafbac":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"94c7c21bad8f4ec3809ee47be49ea8f1":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9c67c045b6c24c52ac962caa14e9680e":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_367d11eca8544e1db8d38e76c4af3977","IPY_MODEL_ab41d5a20ade44e79840fb93feb44d94","IPY_MODEL_5e0944e62904477db3820ebb7cf717ae"],"layout":"IPY_MODEL_43557c61adca4747bee979d264d94415","tabbable":null,"tooltip":null}},"a0d7b2eb7012498fbfb0640cdf3fcdba":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"a10b5695e7974030b02a9d17ea434ecd":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"a196ebc5630446e48913a37576b23f8c":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_eff51cad6de4430988e4e8ad9721fee7","max":1009,"min":0,"orientation":"horizontal","style":"IPY_MODEL_adffcf52924547b195b418a0b7ed0014","tabbable":null,"tooltip":null,"value":1009}},"a3ba6c41d55c4522a54799c86897ff40":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a55ac610ba944b708402fc471341ace5":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_4c28649854b149f1990772971e5d8d16","placeholder":"​","style":"IPY_MODEL_a7b0bdf3319b4ae491946a2167909810","tabbable":null,"tooltip":null,"value":"Adding requests: 100%"}},"a7b0bdf3319b4ae491946a2167909810":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"a94576335f2448db8764cebb46821876":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"a9b4f85f2ef04231ba277b4b9e47b8f1":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4e18685c620d45d98937b3961ab64792","IPY_MODEL_4978fbfcf9ed4feea579e1040af9b62a","IPY_MODEL_813af15899e34344801a4b239b60e34e"],"layout":"IPY_MODEL_df929a0862394f60ba84a990f32fdc01","tabbable":null,"tooltip":null}},"ab41d5a20ade44e79840fb93feb44d94":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_478669d8fe0d4c12b35791f5ff985253","max":684,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d7422cbc7b4a45bdae594f3f14b22839","tabbable":null,"tooltip":null,"value":684}},"ab8175c93bc44c1ab4cccc61a7960fce":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"ad989c8e3705455284ae57a55ff5a691":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"adffcf52924547b195b418a0b7ed0014":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b0a5f07c31664fabb9a87eb4e29883bc":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_1b4dec9d79f248669ae507d2f2524dc8","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_87be2f20dd894e88bc9ba2abdceea1f5","tabbable":null,"tooltip":null,"value":2}},"b26d9bc24e2c4e2796edbeaffb47904a":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"b68421a54f4a4c149aa5ecbe3cd55f02":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b7f0937e9b1b45d9ae5dfe7d70005042":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"baae441277524a709018519689933d67":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8ec2438933df427f86aa576b64df33bb","IPY_MODEL_a196ebc5630446e48913a37576b23f8c","IPY_MODEL_edf76ffe2f0b4858bc30bd1fb2b99644"],"layout":"IPY_MODEL_57181779039d4251826180814edfdf1e","tabbable":null,"tooltip":null}},"bbe972fca4404c639fe842ee6c33cd7f":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_2f4cabddcb7c4f11ae989079004da419","placeholder":"​","style":"IPY_MODEL_f7fd4defac8148be82242bac4b9ab1d9","tabbable":null,"tooltip":null,"value":" 1090/1090 [00:01&lt;00:00, 563.23it/s]"}},"c933155b19fa47098a5221e94bf73819":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cd851d37c64a4c22aa7ab643a82d59f7":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ceceb9e6366e4f08a1cdc06d227ffeb7":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_fcff0e37ee8e4a8cb9b5572187c230ff","placeholder":"​","style":"IPY_MODEL_6784c92c166049f2bb5916a237e530a3","tabbable":null,"tooltip":null,"value":"Adding requests: 100%"}},"d7422cbc7b4a45bdae594f3f14b22839":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d95cef64da1041a9952f71cf230701dc":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"dc285a233b08453492708d175e4d6093":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a55ac610ba944b708402fc471341ace5","IPY_MODEL_08edaba3f03f4bacaeaf0b24aa820fd1","IPY_MODEL_77d02ffdac924d9d9d935e509f6147f7"],"layout":"IPY_MODEL_04152031458b42bbbe65821bc4c896af","tabbable":null,"tooltip":null}},"df929a0862394f60ba84a990f32fdc01":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"e19150de7a5b4542b459bc3199c4aaa7":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_23211fffc2ce44ae94a826e6ec42c114","placeholder":"​","style":"IPY_MODEL_a94576335f2448db8764cebb46821876","tabbable":null,"tooltip":null,"value":" 684/684 [00:00&lt;00:00, 1209.08it/s]"}},"e322fd37704b43299fd24f9bcd2cdf75":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_1cbeacfcc18f47efa0d9a386ca0629c1","placeholder":"​","style":"IPY_MODEL_2a2d75ccbb4844368b7354f845258676","tabbable":null,"tooltip":null,"value":"Processing PDFs: 100%"}},"e3b3e7db09244a4da55dda6393766eca":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ceceb9e6366e4f08a1cdc06d227ffeb7","IPY_MODEL_410675309924442e85b2fcb36affee2d","IPY_MODEL_e19150de7a5b4542b459bc3199c4aaa7"],"layout":"IPY_MODEL_c933155b19fa47098a5221e94bf73819","tabbable":null,"tooltip":null}},"e8a0c7eb0d1a42fcb5af6f1bad452c68":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ebd43cfa579b45bfaf2ae9381cc9cfc6":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_724d12e4c4514630b8e09b0578a88c71","IPY_MODEL_b0a5f07c31664fabb9a87eb4e29883bc","IPY_MODEL_070d1eda42f14bd8af2b65cfccb9326c"],"layout":"IPY_MODEL_a3ba6c41d55c4522a54799c86897ff40","tabbable":null,"tooltip":null}},"edf76ffe2f0b4858bc30bd1fb2b99644":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_1dc80828667f4df38aa1aab1fd9f2f44","placeholder":"​","style":"IPY_MODEL_b7f0937e9b1b45d9ae5dfe7d70005042","tabbable":null,"tooltip":null,"value":" 1009/1009 [01:23&lt;00:00, 26.80it/s, est. speed input: 3586.49 toks/s, output: 214.76 toks/s]"}},"eff51cad6de4430988e4e8ad9721fee7":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f667478076004405be8ea47ccf7b33dd":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f6c9297c49b34f3eb85eb5c2dc3aa3a6":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f7fd4defac8148be82242bac4b9ab1d9":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"fb20c08457994f38a683608a6bbc6b9b":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fcff0e37ee8e4a8cb9b5572187c230ff":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}},"version_major":2,"version_minor":0}}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"31958172","cell_type":"markdown","source":"### **Enhancements based on the baseline: Dataset Mention Extraction 📄🔍**\n\n**Inspiration for using Regex & Context Chunking:**\n\nInspired by the need to extract dataset accessions and DOIs with high precision, I combined regex with smart context slicing and domain-specific heuristics.\n\n**What I changed:**\n\n1. **Regex-Based Identifier Extraction**\n\n   * Added robust patterns to detect **DOIs**, **GSE/SRA**, **CHEMBL**, **UniProt**, and other dataset-related IDs.\n\n2. **Heuristic Keyword Filtering**\n\n   * Matched surrounding text against known **dataset-related phrases** (e.g., “data available at”, “repository”) to filter meaningful mentions.\n\n3. **Smart Contextual Chunking**\n\n   * Implemented a `TextChunker` that aligns context by sentence boundaries, ensuring that extracted snippets are informative and self-contained.\n\n4. **Dataset DOI Classification**\n\n   * Checked matched DOIs against a curated list of known **dataset DOI prefixes** to validate dataset relevance.\n\n5. **Parallel PDF Processing**\n\n   * Boosted performance with **ThreadPoolExecutor**, allowing multiple PDFs to be parsed concurrently.\n\n6. **Model Testing: Non-Reasoning vs Reasoning**\n\n   * This notebook includes evaluation with **Qwen 2.5** for non-reasoning classification and **Qwen 3** for reasoning-intensive classification — allowing comparison and ablation between the two modes.\n\n7. **Detailed False Negative (FN) Analysis**\n\n   * Added in-depth analysis to categorize and quantify **False Negatives** (FN), separated into:\n\n     * **Wrongly classified**: Model predicted something, but not exactly correct.\n     * **Completely missed**: No prediction was made for a ground-truth item.\n   * Each group is further broken down into:\n\n     * **DOI**-based errors (e.g., wrong prefix or mismatched)\n     * **Accession ID** errors (e.g., GSE, PRJNA, etc.)\n   * This helps reveal weaknesses such as:\n\n     * Ambiguous contexts\n     * Incomplete extraction logic\n     * Confusions between similar dataset identifiers\n\n**Next Goal:**\n\n1. **Improve Chunk and Reduce Junk Chunk**\n2. **What to Improve Regex**\n3. **Added Prompt Caching**\n4. **Reduce Runtime for run**\n5. **More F1-Score Reduce FN**\n\n**I hope this notebook to goal gold medal notebook**\n","metadata":{"papermill":{"duration":0.004249,"end_time":"2025-07-09T10:56:14.322962","exception":false,"start_time":"2025-07-09T10:56:14.318713","status":"completed"},"tags":[]}},{"id":"dc266866-2d27-4fe0-b831-7c368d24b9ef","cell_type":"code","source":"!pip install /kaggle/input/mdcfitz/pymupdf-1.26.3-cp39-abi3-manylinux_2_28_x86_64.whl\n!pip install vllm --no-index --find-links file:///kaggle/input/mdcllm\n!pip install logits-processor-zoo==0.1.10 --no-index --find-links file:///kaggle/input/mdcllm\n!pip install triton==3.2.0 --no-index --find-links file:///kaggle/input/mdcllm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T05:09:38.378436Z","iopub.execute_input":"2025-07-17T05:09:38.378728Z","iopub.status.idle":"2025-07-17T05:12:41.604245Z","shell.execute_reply.started":"2025-07-17T05:09:38.378704Z","shell.execute_reply":"2025-07-17T05:12:41.603468Z"}},"outputs":[{"name":"stdout","text":"Processing /kaggle/input/mdcfitz/pymupdf-1.26.3-cp39-abi3-manylinux_2_28_x86_64.whl\nInstalling collected packages: pymupdf\nSuccessfully installed pymupdf-1.26.3\nLooking in links: file:///kaggle/input/mdcllm\nProcessing /kaggle/input/mdcllm/vllm-0.9.2-cp38-abi3-manylinux1_x86_64.whl\nRequirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from vllm) (2024.11.6)\nRequirement already satisfied: cachetools in /usr/local/lib/python3.11/dist-packages (from vllm) (5.5.2)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from vllm) (7.0.0)\nRequirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from vllm) (0.2.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from vllm) (1.26.4)\nRequirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (2.32.4)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from vllm) (4.67.1)\nProcessing /kaggle/input/mdcllm/blake3-1.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from vllm)\nRequirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from vllm) (9.0.0)\nRequirement already satisfied: transformers>=4.51.1 in /usr/local/lib/python3.11/dist-packages (from vllm) (4.52.4)\nRequirement already satisfied: huggingface-hub>=0.33.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub[hf_xet]>=0.33.0->vllm) (0.33.1)\nRequirement already satisfied: tokenizers>=0.21.1 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.21.2)\nRequirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from vllm) (3.20.3)\nRequirement already satisfied: fastapi>=0.115.0 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]>=0.115.0->vllm) (0.115.13)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from vllm) (3.12.13)\nProcessing /kaggle/input/mdcllm/openai-1.90.0-py3-none-any.whl (from vllm)\nRequirement already satisfied: pydantic>=2.10 in /usr/local/lib/python3.11/dist-packages (from vllm) (2.11.7)\nRequirement already satisfied: prometheus_client>=0.18.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.22.1)\nRequirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from vllm) (11.2.1)\nProcessing /kaggle/input/mdcllm/prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl (from vllm)\nRequirement already satisfied: tiktoken>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.9.0)\nProcessing /kaggle/input/mdcllm/lm_format_enforcer-0.10.11-py3-none-any.whl (from vllm)\nProcessing /kaggle/input/mdcllm/llguidance-0.7.30-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from vllm)\nProcessing /kaggle/input/mdcllm/outlines-0.1.11-py3-none-any.whl (from vllm)\nProcessing /kaggle/input/mdcllm/lark-1.2.2-py3-none-any.whl (from vllm)\nProcessing /kaggle/input/mdcllm/xgrammar-0.1.19-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from vllm)\nRequirement already satisfied: typing_extensions>=4.10 in /usr/local/lib/python3.11/dist-packages (from vllm) (4.14.0)\nRequirement already satisfied: filelock>=3.16.1 in /usr/local/lib/python3.11/dist-packages (from vllm) (3.18.0)\nProcessing /kaggle/input/mdcllm/partial_json_parser-0.2.1.1.post6-py3-none-any.whl (from vllm)\nProcessing /kaggle/input/mdcllm/pyzmq-27.0.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (from vllm)\nProcessing /kaggle/input/mdcllm/msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from vllm)\nProcessing /kaggle/input/mdcllm/gguf-0.17.1-py3-none-any.whl (from vllm)\nProcessing /kaggle/input/mdcllm/mistral_common-1.6.3-py3-none-any.whl (from mistral_common[opencv]>=1.6.2->vllm)\nRequirement already satisfied: opencv-python-headless>=4.11.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (4.11.0.86)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from vllm) (6.0.2)\nRequirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from vllm) (0.8.1)\nProcessing /kaggle/input/mdcllm/compressed_tensors-0.10.2-py3-none-any.whl (from vllm)\nProcessing /kaggle/input/mdcllm/depyf-0.18.0-py3-none-any.whl (from vllm)\nRequirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from vllm) (3.1.1)\nProcessing /kaggle/input/mdcllm/watchfiles-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from vllm)\nRequirement already satisfied: python-json-logger in /usr/local/lib/python3.11/dist-packages (from vllm) (3.3.0)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from vllm) (1.15.3)\nRequirement already satisfied: ninja in /usr/local/lib/python3.11/dist-packages (from vllm) (1.11.1.4)\nProcessing /kaggle/input/mdcllm/pybase64-1.4.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from vllm)\nProcessing /kaggle/input/mdcllm/numba-0.61.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (from vllm)\nRequirement already satisfied: ray!=2.44.*,>=2.43.0 in /usr/local/lib/python3.11/dist-packages (from ray[cgraph]!=2.44.*,>=2.43.0->vllm) (2.47.1)\nProcessing /kaggle/input/mdcllm/torch-2.7.0-cp311-cp311-manylinux_2_28_x86_64.whl (from vllm)\nProcessing /kaggle/input/mdcllm/torchaudio-2.7.0-cp311-cp311-manylinux_2_28_x86_64.whl (from vllm)\nProcessing /kaggle/input/mdcllm/torchvision-0.22.0-cp311-cp311-manylinux_2_28_x86_64.whl (from vllm)\nProcessing /kaggle/input/mdcllm/xformers-0.0.30-cp311-cp311-manylinux_2_28_x86_64.whl (from vllm)\nProcessing /kaggle/input/mdcllm/astor-0.8.1-py2.py3-none-any.whl (from depyf==0.18.0->vllm)\nRequirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from depyf==0.18.0->vllm) (0.3.8)\nProcessing /kaggle/input/mdcllm/llvmlite-0.44.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from numba==0.61.2->vllm)\nProcessing /kaggle/input/mdcllm/interegular-0.3.3-py37-none-any.whl (from outlines==0.1.11->vllm)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (3.1.6)\nRequirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (1.6.0)\nProcessing /kaggle/input/mdcllm/diskcache-5.6.3-py3-none-any.whl (from outlines==0.1.11->vllm)\nRequirement already satisfied: referencing in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (0.36.2)\nRequirement already satisfied: jsonschema in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (4.24.0)\nProcessing /kaggle/input/mdcllm/pycountry-24.6.1-py3-none-any.whl (from outlines==0.1.11->vllm)\nProcessing /kaggle/input/mdcllm/airportsdata-20250706-py3-none-any.whl (from outlines==0.1.11->vllm)\nProcessing /kaggle/input/mdcllm/outlines_core-0.1.26-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from outlines==0.1.11->vllm)\nProcessing /kaggle/input/mdcllm/sympy-1.14.0-py3-none-any.whl (from torch==2.7.0->vllm)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->vllm) (3.5)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->vllm) (2025.5.1)\nProcessing /kaggle/input/mdcllm/nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (from torch==2.7.0->vllm)\nProcessing /kaggle/input/mdcllm/nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (from torch==2.7.0->vllm)\nProcessing /kaggle/input/mdcllm/nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (from torch==2.7.0->vllm)\nProcessing /kaggle/input/mdcllm/nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (from torch==2.7.0->vllm)\nProcessing /kaggle/input/mdcllm/nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (from torch==2.7.0->vllm)\nProcessing /kaggle/input/mdcllm/nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (from torch==2.7.0->vllm)\nProcessing /kaggle/input/mdcllm/nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (from torch==2.7.0->vllm)\nProcessing /kaggle/input/mdcllm/nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (from torch==2.7.0->vllm)\nProcessing /kaggle/input/mdcllm/nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (from torch==2.7.0->vllm)\nProcessing /kaggle/input/mdcllm/nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (from torch==2.7.0->vllm)\nProcessing /kaggle/input/mdcllm/nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (from torch==2.7.0->vllm)\nProcessing /kaggle/input/mdcllm/nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (from torch==2.7.0->vllm)\nProcessing /kaggle/input/mdcllm/nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (from torch==2.7.0->vllm)\nProcessing /kaggle/input/mdcllm/nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (from torch==2.7.0->vllm)\nProcessing /kaggle/input/mdcllm/triton-3.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (from torch==2.7.0->vllm)\nRequirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.0->torch==2.7.0->vllm) (75.2.0)\nRequirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm) (0.46.2)\nProcessing /kaggle/input/mdcllm/fastapi_cli-0.0.8-py3-none-any.whl (from fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\nRequirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]>=0.115.0->vllm) (0.28.1)\nRequirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]>=0.115.0->vllm) (0.0.20)\nRequirement already satisfied: email-validator>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]>=0.115.0->vllm) (2.2.0)\nRequirement already satisfied: uvicorn>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.34.3)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.33.0->huggingface-hub[hf_xet]>=0.33.0->vllm) (25.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.33.0->huggingface-hub[hf_xet]>=0.33.0->vllm) (1.1.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->vllm) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->vllm) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->vllm) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->vllm) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->vllm) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->vllm) (2.4.1)\nRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<=1.90.0,>=1.52.0->vllm) (4.9.0)\nRequirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<=1.90.0,>=1.52.0->vllm) (1.9.0)\nRequirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<=1.90.0,>=1.52.0->vllm) (0.10.0)\nRequirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<=1.90.0,>=1.52.0->vllm) (1.3.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.10->vllm) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.10->vllm) (2.33.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.10->vllm) (0.4.1)\nRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from ray!=2.44.*,>=2.43.0->ray[cgraph]!=2.44.*,>=2.43.0->vllm) (8.2.1)\nRequirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ray!=2.44.*,>=2.43.0->ray[cgraph]!=2.44.*,>=2.43.0->vllm) (1.1.1)\nRequirement already satisfied: cupy-cuda12x in /usr/local/lib/python3.11/dist-packages (from ray[cgraph]!=2.44.*,>=2.43.0->vllm) (13.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->vllm) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->vllm) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->vllm) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->vllm) (2025.6.15)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.51.1->vllm) (0.5.3)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (6.6.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (1.20.1)\nRequirement already satisfied: dnspython>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from email-validator>=2.0.0->fastapi[standard]>=0.115.0->vllm) (2.7.0)\nRequirement already satisfied: typer>=0.15.1 in /usr/local/lib/python3.11/dist-packages (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.16.0)\nProcessing /kaggle/input/mdcllm/rich_toolkit-0.14.8-py3-none-any.whl (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\nProcessing /kaggle/input/mdcllm/fastapi_cloud_cli-0.1.1-py3-none-any.whl (from fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->fastapi[standard]>=0.115.0->vllm) (1.0.9)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.23.0->fastapi[standard]>=0.115.0->vllm) (0.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->outlines==0.1.11->vllm) (3.0.2)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema->outlines==0.1.11->vllm) (2025.4.1)\nRequirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema->outlines==0.1.11->vllm) (0.25.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch==2.7.0->vllm) (1.3.0)\nProcessing /kaggle/input/mdcllm/httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\nProcessing /kaggle/input/mdcllm/python_dotenv-1.1.1-py3-none-any.whl (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\nProcessing /kaggle/input/mdcllm/uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\nRequirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (15.0.1)\nRequirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.11/dist-packages (from cupy-cuda12x->ray[cgraph]!=2.44.*,>=2.43.0->vllm) (0.8.3)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->vllm) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->vllm) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->vllm) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->vllm) (2024.2.0)\nProcessing /kaggle/input/mdcllm/rignore-0.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\nProcessing /kaggle/input/mdcllm/httpx-0.27.2-py3-none-any.whl (from fastapi[standard]>=0.115.0->vllm)\nRequirement already satisfied: sentry-sdk>=2.20.0 in /usr/local/lib/python3.11/dist-packages (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (2.31.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->vllm) (2024.2.0)\nRequirement already satisfied: rich>=13.7.1 in /usr/local/lib/python3.11/dist-packages (from rich-toolkit>=0.14.8->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (14.0.0)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.15.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (1.5.4)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (2.19.2)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.1.2)\nInstalling collected packages: nvidia-cusparselt-cu12, blake3, uvloop, triton, sympy, rignore, pyzmq, python-dotenv, pycountry, pybase64, partial-json-parser, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, msgspec, llvmlite, llguidance, lark, interegular, httptools, diskcache, astor, airportsdata, watchfiles, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, httpx, depyf, rich-toolkit, prometheus-fastapi-instrumentator, openai, nvidia-cusolver-cu12, lm-format-enforcer, torch, outlines_core, fastapi-cloud-cli, fastapi-cli, torchaudio, mistral_common, xgrammar, xformers, torchvision, outlines, numba, gguf, compressed-tensors, vllm\n  Attempting uninstall: nvidia-cusparselt-cu12\n    Found existing installation: nvidia-cusparselt-cu12 0.6.2\n    Uninstalling nvidia-cusparselt-cu12-0.6.2:\n      Successfully uninstalled nvidia-cusparselt-cu12-0.6.2\n  Attempting uninstall: triton\n    Found existing installation: triton 3.2.0\n    Uninstalling triton-3.2.0:\n      Successfully uninstalled triton-3.2.0\n  Attempting uninstall: sympy\n    Found existing installation: sympy 1.13.1\n    Uninstalling sympy-1.13.1:\n      Successfully uninstalled sympy-1.13.1\n  Attempting uninstall: pyzmq\n    Found existing installation: pyzmq 24.0.1\n    Uninstalling pyzmq-24.0.1:\n      Successfully uninstalled pyzmq-24.0.1\n  Attempting uninstall: nvidia-nvtx-cu12\n    Found existing installation: nvidia-nvtx-cu12 12.4.127\n    Uninstalling nvidia-nvtx-cu12-12.4.127:\n      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-nccl-cu12\n    Found existing installation: nvidia-nccl-cu12 2.21.5\n    Uninstalling nvidia-nccl-cu12-2.21.5:\n      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: llvmlite\n    Found existing installation: llvmlite 0.43.0\n    Uninstalling llvmlite-0.43.0:\n      Successfully uninstalled llvmlite-0.43.0\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: httpx\n    Found existing installation: httpx 0.28.1\n    Uninstalling httpx-0.28.1:\n      Successfully uninstalled httpx-0.28.1\n  Attempting uninstall: openai\n    Found existing installation: openai 1.91.0\n    Uninstalling openai-1.91.0:\n      Successfully uninstalled openai-1.91.0\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n  Attempting uninstall: torch\n    Found existing installation: torch 2.6.0+cu124\n    Uninstalling torch-2.6.0+cu124:\n      Successfully uninstalled torch-2.6.0+cu124\n  Attempting uninstall: torchaudio\n    Found existing installation: torchaudio 2.6.0+cu124\n    Uninstalling torchaudio-2.6.0+cu124:\n      Successfully uninstalled torchaudio-2.6.0+cu124\n  Attempting uninstall: torchvision\n    Found existing installation: torchvision 0.21.0+cu124\n    Uninstalling torchvision-0.21.0+cu124:\n      Successfully uninstalled torchvision-0.21.0+cu124\n  Attempting uninstall: numba\n    Found existing installation: numba 0.60.0\n    Uninstalling numba-0.60.0:\n      Successfully uninstalled numba-0.60.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndask-cuda 25.2.0 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\ncuml-cu12 25.2.1 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\ncudf-cu12 25.2.2 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\ndistributed-ucxx-cu12 0.42.0 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\nydata-profiling 4.16.1 requires numba<=0.61,>=0.56.0, but you have numba 0.61.2 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.3 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.4 which is incompatible.\ngoogle-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.1 which is incompatible.\ngoogle-genai 1.21.1 requires httpx<1.0.0,>=0.28.1, but you have httpx 0.27.2 which is incompatible.\nfirebase-admin 6.9.0 requires httpx[http2]==0.28.1, but you have httpx 0.27.2 which is incompatible.\nlangchain-core 0.3.66 requires packaging<25,>=23.2, but you have packaging 25.0 which is incompatible.\nfastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.0 which is incompatible.\njupyter-kernel-gateway 2.5.2 requires jupyter-client<8.0,>=5.2.0, but you have jupyter-client 8.6.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed airportsdata-20250706 astor-0.8.1 blake3-1.0.5 compressed-tensors-0.10.2 depyf-0.18.0 diskcache-5.6.3 fastapi-cli-0.0.8 fastapi-cloud-cli-0.1.1 gguf-0.17.1 httptools-0.6.4 httpx-0.27.2 interegular-0.3.3 lark-1.2.2 llguidance-0.7.30 llvmlite-0.44.0 lm-format-enforcer-0.10.11 mistral_common-1.6.3 msgspec-0.19.0 numba-0.61.2 nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 openai-1.90.0 outlines-0.1.11 outlines_core-0.1.26 partial-json-parser-0.2.1.1.post6 prometheus-fastapi-instrumentator-7.1.0 pybase64-1.4.1 pycountry-24.6.1 python-dotenv-1.1.1 pyzmq-27.0.0 rich-toolkit-0.14.8 rignore-0.5.1 sympy-1.14.0 torch-2.7.0 torchaudio-2.7.0 torchvision-0.22.0 triton-3.3.0 uvloop-0.21.0 vllm-0.9.2 watchfiles-1.1.0 xformers-0.0.30 xgrammar-0.1.19\nLooking in links: file:///kaggle/input/mdcllm\nProcessing /kaggle/input/mdcllm/logits_processor_zoo-0.1.10-py3-none-any.whl\nRequirement already satisfied: accelerate>=0.26.1 in /usr/local/lib/python3.11/dist-packages (from logits-processor-zoo==0.1.10) (1.8.1)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from logits-processor-zoo==0.1.10) (2.7.0)\nRequirement already satisfied: transformers>=4.41.2 in /usr/local/lib/python3.11/dist-packages (from logits-processor-zoo==0.1.10) (4.52.4)\nRequirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.1->logits-processor-zoo==0.1.10) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.1->logits-processor-zoo==0.1.10) (25.0)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.1->logits-processor-zoo==0.1.10) (7.0.0)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.1->logits-processor-zoo==0.1.10) (6.0.2)\nRequirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.1->logits-processor-zoo==0.1.10) (0.33.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.1->logits-processor-zoo==0.1.10) (0.5.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->logits-processor-zoo==0.1.10) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->logits-processor-zoo==0.1.10) (4.14.0)\nRequirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch->logits-processor-zoo==0.1.10) (1.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->logits-processor-zoo==0.1.10) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->logits-processor-zoo==0.1.10) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->logits-processor-zoo==0.1.10) (2025.5.1)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch->logits-processor-zoo==0.1.10) (12.6.77)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch->logits-processor-zoo==0.1.10) (12.6.77)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.11/dist-packages (from torch->logits-processor-zoo==0.1.10) (12.6.80)\nRequirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /usr/local/lib/python3.11/dist-packages (from torch->logits-processor-zoo==0.1.10) (9.5.1.17)\nRequirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.11/dist-packages (from torch->logits-processor-zoo==0.1.10) (12.6.4.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.11/dist-packages (from torch->logits-processor-zoo==0.1.10) (11.3.0.4)\nRequirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.11/dist-packages (from torch->logits-processor-zoo==0.1.10) (10.3.7.77)\nRequirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.11/dist-packages (from torch->logits-processor-zoo==0.1.10) (11.7.1.2)\nRequirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.11/dist-packages (from torch->logits-processor-zoo==0.1.10) (12.5.4.2)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/dist-packages (from torch->logits-processor-zoo==0.1.10) (0.6.3)\nRequirement already satisfied: nvidia-nccl-cu12==2.26.2 in /usr/local/lib/python3.11/dist-packages (from torch->logits-processor-zoo==0.1.10) (2.26.2)\nRequirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch->logits-processor-zoo==0.1.10) (12.6.77)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.11/dist-packages (from torch->logits-processor-zoo==0.1.10) (12.6.85)\nRequirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.11/dist-packages (from torch->logits-processor-zoo==0.1.10) (1.11.1.6)\nRequirement already satisfied: triton==3.3.0 in /usr/local/lib/python3.11/dist-packages (from torch->logits-processor-zoo==0.1.10) (3.3.0)\nRequirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.0->torch->logits-processor-zoo==0.1.10) (75.2.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.41.2->logits-processor-zoo==0.1.10) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers>=4.41.2->logits-processor-zoo==0.1.10) (2.32.4)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.41.2->logits-processor-zoo==0.1.10) (0.21.2)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.41.2->logits-processor-zoo==0.1.10) (4.67.1)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate>=0.26.1->logits-processor-zoo==0.1.10) (1.1.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate>=0.26.1->logits-processor-zoo==0.1.10) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate>=0.26.1->logits-processor-zoo==0.1.10) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate>=0.26.1->logits-processor-zoo==0.1.10) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate>=0.26.1->logits-processor-zoo==0.1.10) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate>=0.26.1->logits-processor-zoo==0.1.10) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate>=0.26.1->logits-processor-zoo==0.1.10) (2.4.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch->logits-processor-zoo==0.1.10) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->logits-processor-zoo==0.1.10) (3.0.2)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.41.2->logits-processor-zoo==0.1.10) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.41.2->logits-processor-zoo==0.1.10) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.41.2->logits-processor-zoo==0.1.10) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.41.2->logits-processor-zoo==0.1.10) (2025.6.15)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0.0,>=1.17->accelerate>=0.26.1->logits-processor-zoo==0.1.10) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0.0,>=1.17->accelerate>=0.26.1->logits-processor-zoo==0.1.10) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3.0.0,>=1.17->accelerate>=0.26.1->logits-processor-zoo==0.1.10) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3.0.0,>=1.17->accelerate>=0.26.1->logits-processor-zoo==0.1.10) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3.0.0,>=1.17->accelerate>=0.26.1->logits-processor-zoo==0.1.10) (2024.2.0)\nInstalling collected packages: logits-processor-zoo\nSuccessfully installed logits-processor-zoo-0.1.10\nLooking in links: file:///kaggle/input/mdcllm\nProcessing /kaggle/input/mdcllm/triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nInstalling collected packages: triton\n  Attempting uninstall: triton\n    Found existing installation: triton 3.3.0\n    Uninstalling triton-3.3.0:\n      Successfully uninstalled triton-3.3.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntorch 2.7.0 requires triton==3.3.0; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have triton 3.2.0 which is incompatible.\nfastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed triton-3.2.0\n","output_type":"stream"}],"execution_count":1},{"id":"b61c2953","cell_type":"code","source":"import os\n\nos.environ[\"VLLM_USE_V1\"] = \"0\"\n\nimport re\nimport pymupdf\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\nfrom logits_processor_zoo.vllm import MultipleChoiceLogitsProcessor\nimport pickle\nimport vllm\nimport torch","metadata":{"papermill":{"duration":111.938725,"end_time":"2025-07-09T10:58:06.265227","exception":false,"start_time":"2025-07-09T10:56:14.326502","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T05:12:41.606019Z","iopub.execute_input":"2025-07-17T05:12:41.606964Z","iopub.status.idle":"2025-07-17T05:12:47.957306Z","shell.execute_reply.started":"2025-07-17T05:12:41.606937Z","shell.execute_reply":"2025-07-17T05:12:47.956727Z"}},"outputs":[],"execution_count":2},{"id":"fd46aaa5-9957-4907-9b52-a35bd9da06de","cell_type":"code","source":"os.environ[\"KAGGLE_IS_COMPETITION_RERUN\"] = \"1\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T05:12:47.957976Z","iopub.execute_input":"2025-07-17T05:12:47.958383Z","iopub.status.idle":"2025-07-17T05:12:47.961893Z","shell.execute_reply.started":"2025-07-17T05:12:47.958361Z","shell.execute_reply":"2025-07-17T05:12:47.961270Z"}},"outputs":[],"execution_count":3},{"id":"6373b966-36e3-4e12-bad7-ef2a2aa3652c","cell_type":"code","source":"# vLLM V1 does not accept logits processor, so disable it\n# https://docs.vllm.ai/en/latest/getting_started/v1_user_guide.html#deprecated-features\npdf_directory = \"/kaggle/input/make-data-count-finding-data-references/test/PDF\" \\\n                if os.getenv('KAGGLE_IS_COMPETITION_RERUN') \\\n                else \"/kaggle/input/make-data-count-finding-data-references/train/PDF\"\nchunks = []\nchunks2 = []\ntext_span_len = 300\n\nre_doi = re.compile(r\"10\\.\\d{4,9}/[-._;()/:A-Z0-9]+\", re.IGNORECASE)\nre_gsr = re.compile(r\"GSE\\d+|SR[APRX]\\d+|PRJ[NAED][A-Z]?\\d+|E-[A-Z]+-\\d+\", re.IGNORECASE)\nre_ipe = re.compile(r\"IPR\\d{6}|PF\\d{5}|EMPIAR-\\d{5}|EMD-\\d{4,5}\", re.IGNORECASE)\nre_c = re.compile(r\"CHEMBL\\d+|CVCL_[A-Z0-9]{4}|CID:\\d+\", re.IGNORECASE)\nre_e = re.compile(r\"ENS[A-Z]{0,6}[GT]\\d{11}|ENSG\\d{11}\", re.IGNORECASE)\nre_r = re.compile(r\"N[MC]_\\d+(?:\\.\\d+)?|rs\\d+|XM_\\d+|XP_\\d+\", re.IGNORECASE)\nre_u = re.compile(r\"(?:uniprot:)?(?:[OPQ][0-9][A-Z0-9]{3}[0-9]|[A-NR-Z][0-9][A-Z][A-Z0-9]{2}[0-9])\", re.IGNORECASE)\nre_g = re.compile(r\"EPI(?:_ISL_)?\\d+|GISAID\", re.IGNORECASE)\nre_p = re.compile(r\"PXD\\d{6}|SAM[ND]\\d+|ERR\\d+|DRR\\d+|MSV\\d+\", re.IGNORECASE)\nre_pdb = re.compile(r\"\\b[0-9][A-Z0-9]{3}\\b\", re.IGNORECASE)\nre_geo = re.compile(r\"GDS\\d+|GPL\\d+|GSM\\d+\", re.IGNORECASE)\nre_arrayexpress = re.compile(r\"E-[A-Z]+-\\d+\", re.IGNORECASE)\n\nrelist = [re_gsr, re_ipe, re_c, re_e, re_r, re_g, re_p, re_geo, re_arrayexpress]\nids = []\n\ndef remove_references_section(text):\n    lines = text.split('\\n')\n    cut_index = -1\n    \n    # Look backwards from end of document\n    for i in range(len(lines) - 1, max(0, int(len(lines) * 0.2)), -1):\n        line = lines[i].strip()\n        obvious_patterns = [\n            r'^REFERENCES?$',\n            r'^\\d+\\.?\\s+REFERENCES?$',\n            r'^\\d+\\.?\\s+References?$',\n            r'^References?:?$',\n            r'^BIBLIOGRAPHY$',\n            r'^\\d+\\.?\\s+BIBLIOGRAPHY$',\n            r'^\\d+\\.?\\s+Bibliography$',\n            r'^Bibliography:?$',\n            r'^Literature\\s+Cited$',\n            r'^Works\\s+Cited$',\n            r'^ACKNOWLEDGMENTS?$',\n            r'^Acknowledgments?$',\n            r'^FUNDING$',\n            r'^CONFLICTS?\\s+OF\\s+INTEREST$'\n        ]\n        if any(re.match(pattern, line, re.IGNORECASE) for pattern in obvious_patterns):\n            # Double-check: look at following lines for citation patterns\n            following_lines = lines[i+1:i+5]\n            has_citations = False\n            for follow_line in following_lines:\n                if follow_line.strip():\n                    # Check for obvious citation patterns\n                    if (re.search(r'\\(\\d{4}\\)', follow_line) or\n                        re.search(r'\\d{4}\\.', follow_line) or\n                        'doi:' in follow_line.lower() or\n                        ' et al' in follow_line.lower() or\n                        re.search(r'^\\[\\d+\\]', follow_line.strip()) or\n                        re.search(r'^\\d+\\.', follow_line.strip())):\n                        has_citations = True\n                        break\n            # Only cut if we found citation-like content\n            if has_citations or i >= len(lines) - 5:  # Or very near end\n                cut_index = i\n                break\n    if cut_index != -1:\n        return '\\n'.join(lines[:cut_index]).strip()\n    return text.strip()\n\ndef extract_context_with_keywords(text, match_start, match_end, span_len=300):\n    keyword_scores = {\n        \"data are available\": 5, \"datasets are available\": 5, \"deposited in\": 5, \n        \"submitted to\": 5, \"accession number\": 5, \"accession code\": 5, \n        \"accession id\": 5, \"archived in\": 4, \"uploaded to\": 4, \"source code\": 4, \n        \"raw data\": 4, \"sequencing data\": 4, \"retrieved from\": 3, \"downloaded from\": 3, \n        \"obtained from\": 3, \"supplementary data\": 3, \"supporting information\": 3, \n        'deposited': 3, 'submitted': 3, 'accession': 3, \"available in the\": 2, \n        \"publicly available\": 2, \"freely available\": 2, \"supplementary material\": 2, \n        'dataset': 2, 'datasets': 2, 'database': 2, 'repository': 2, 'code': 2, \n        'scripts': 2, 'available': 1, 'download': 1, 'supplementary': 1, \n        'supporting': 1, 'software': 1, 'protocol': 1, 'data': 0.5\n    }\n    \n    contexts = {\n        'standard': text[max(0, match_start - span_len):min(len(text), match_end + span_len)],\n        'extended': text[max(0, match_start - span_len * 2):min(len(text), match_end + span_len * 2)]\n    }\n    \n    def score_context(context):\n        return sum(\n            context.lower().count(k) * v if ' ' in k \n            else len(re.findall(r'\\b' + re.escape(k) + r'\\b', context.lower())) * v\n            for k, v in keyword_scores.items()\n        )\n    \n    scores = {k: score_context(v) for k, v in contexts.items()}\n    return contexts['extended'] if scores['extended'] > scores['standard'] and scores['extended'] > 4 else contexts['standard']\n\nrows = []\nfor filename in tqdm(os.listdir(pdf_directory), total=len(os.listdir(pdf_directory))):\n    if filename.endswith(\".pdf\"):\n        pdf_path = os.path.join(pdf_directory, filename)\n        article_id = filename.split(\".pdf\")[0]\n        try:\n            with pymupdf.open(pdf_path) as doc:\n                text = \"\\n\".join(page.get_text() for page in doc)\n        except Exception as e:\n            print(f\"Could not process {filename}: {e}\")\n            continue\n\n        text = remove_references_section(text)\n        rows.append({\"article_id\": article_id, \"text\": text})\n        doi_matches = list(re_doi.finditer(text))\n        for match in doi_matches:\n            # Exclude the article's own DOI if it's mentioned\n            if match.group() in article_id:\n                continue\n            chunk = extract_context_with_keywords(text, match.start(), match.end(), text_span_len)\n            chunks.append((article_id, chunk))\n            \n        for rr in relist:\n            matches = list(rr.finditer(text))\n            for match in matches:\n                ids.append(match.group())\n                chunk = extract_context_with_keywords(text, match.start(), match.end(), text_span_len)\n                chunks2.append((article_id, chunk))\n\nprint(f\"DOI chunks: {len(chunks)}\")\nprint(f\"Other ID chunks: {len(chunks2)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T05:12:47.963499Z","iopub.execute_input":"2025-07-17T05:12:47.963701Z","iopub.status.idle":"2025-07-17T05:12:53.063886Z","shell.execute_reply.started":"2025-07-17T05:12:47.963685Z","shell.execute_reply":"2025-07-17T05:12:53.062951Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/30 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b79277d6af9f4009acb8e9dbeb4164d6"}},"metadata":{}},{"name":"stdout","text":"DOI chunks: 293\nOther ID chunks: 1\n","output_type":"stream"}],"execution_count":4},{"id":"f51793da-ec37-411a-862f-2a5c94516713","cell_type":"markdown","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification\n\n# 🧠 Load model and tokenizer\nmodel_path = \"/kaggle/input/makedatacount-mixed-train/saved_model_dual_text\"\ntoken_path = model_path\n\ntokenizer = AutoTokenizer.from_pretrained(token_path)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_path)\nmodel.eval()\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)","metadata":{}},{"id":"445bcf41-2692-488c-9d2f-76d7f171040b","cell_type":"markdown","source":"# 🔮 Predict type\nlabel_map = {0: \"Primary\", 1: \"Secondary\", 2: \"Missing\"}\nbatch_size = 8\npreds = []\n\nfor i in tqdm(range(0, len(rows), batch_size)):\n    batch_texts = [r[\"text\"] for r in rows[i:i+batch_size]]\n    enc = tokenizer(batch_texts, truncation=True, padding=True, max_length=512, return_tensors=\"pt\").to(device)\n    with torch.no_grad():\n        logits = model(**enc).logits\n        p = torch.argmax(logits, dim=1).cpu().tolist()\n        preds.extend(p)","metadata":{}},{"id":"035fe562-ea88-4f8a-8b5c-dd365cecf191","cell_type":"code","source":"def extract_dataset_ids(text):\n    \"\"\"\n    Extract DOIs and known accession IDs robustly.\n    Returns a sorted list of unique dataset IDs.\n    \"\"\"\n    #repos = ['dryad','zenodo','figshare','pangaea','tcia','p9','d9','pasta','cranfield','dtu','usn','f7','jb','xyb','dl']\n    repos = ['dryad', 'zenodo', 'figshare', 'pangaea', 'tcia']\n    candidates = set()\n\n    # DOI pattern (strict)\n    doi_pattern = r'10\\.\\d{4,9}/[^\\s\\)\\]<]+'\n    for match in re.findall(doi_pattern, text):\n        clean = match.rstrip('.,;)]>').strip()\n        # Keep only known repositories\n        if any(repo in clean.lower() for repo in repos):\n            candidates.add(f\"https://doi.org/{clean}\")\n\n    for pat in relist:\n        candidates.update(re.findall(pat, text))\n\n    # Filter: drop very short garbage\n    candidates = [c for c in candidates if len(c) >= 5]\n\n    return None # sorted(candidates)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T05:12:53.064554Z","iopub.execute_input":"2025-07-17T05:12:53.064767Z","iopub.status.idle":"2025-07-17T05:12:53.070338Z","shell.execute_reply.started":"2025-07-17T05:12:53.064741Z","shell.execute_reply":"2025-07-17T05:12:53.069595Z"}},"outputs":[],"execution_count":5},{"id":"fe1a9435-8015-4ee3-997a-118a98dfe92b","cell_type":"markdown","source":"# 🏷️ Extract dataset IDs and build results\nresults = []\nfor i, r in enumerate(rows):\n    t = r[\"text\"]\n    dataset_ids = extract_dataset_ids(t)\n    if not dataset_ids:\n        dataset_ids = [\"Missing\"]  # or optionally: [\"Missing\"]\n    \n    for did in dataset_ids:\n        results.append({\n            \"article_id\": r[\"article_id\"],\n            \"dataset_id\": did,\n            \"type\": label_map[preds[i]]\n        })","metadata":{}},{"id":"f9162ce7-326b-41c4-b89b-41a87431e60f","cell_type":"code","source":"# df_preds = pd.DataFrame(results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T05:12:53.071061Z","iopub.execute_input":"2025-07-17T05:12:53.071297Z","iopub.status.idle":"2025-07-17T05:12:53.085626Z","shell.execute_reply.started":"2025-07-17T05:12:53.071280Z","shell.execute_reply":"2025-07-17T05:12:53.084950Z"}},"outputs":[],"execution_count":6},{"id":"9c777bb7","cell_type":"markdown","source":"## Load LLM","metadata":{"papermill":{"duration":0.003217,"end_time":"2025-07-09T10:58:06.272101","exception":false,"start_time":"2025-07-09T10:58:06.268884","status":"completed"},"tags":[]}},{"id":"c776e9ad","cell_type":"code","source":"think_mode = True","metadata":{"papermill":{"duration":0.008624,"end_time":"2025-07-09T10:58:06.284002","exception":false,"start_time":"2025-07-09T10:58:06.275378","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T05:12:53.086307Z","iopub.execute_input":"2025-07-17T05:12:53.086529Z","iopub.status.idle":"2025-07-17T05:12:53.100500Z","shell.execute_reply.started":"2025-07-17T05:12:53.086512Z","shell.execute_reply":"2025-07-17T05:12:53.099986Z"}},"outputs":[],"execution_count":7},{"id":"cf732f96","cell_type":"code","source":"if think_mode:\n    model_path = \"/kaggle/input/qwen-3/transformers/8b-awq/1\"\n    llm = vllm.LLM(\n        model_path,\n        quantization='awq',\n        tensor_parallel_size=torch.cuda.device_count(),\n        gpu_memory_utilization=0.92,\n        trust_remote_code=True,\n        dtype=\"half\",\n        enforce_eager=True,\n        max_model_len=4096,\n        disable_log_stats=True,\n        enable_prefix_caching=True\n    )\nelse:\n    model_path = \"/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1\"\n    llm = vllm.LLM(\n        model_path,\n        quantization='awq',\n        tensor_parallel_size=torch.cuda.device_count(),\n        gpu_memory_utilization=0.92,\n        trust_remote_code=True,\n        dtype=\"half\",\n        enforce_eager=True,\n        max_model_len=1024+512,\n        disable_log_stats=True,\n        enable_prefix_caching=True\n    )\ntokenizer = llm.get_tokenizer()","metadata":{"papermill":{"duration":146.31726,"end_time":"2025-07-09T11:00:32.604353","exception":false,"start_time":"2025-07-09T10:58:06.287093","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T05:12:53.101235Z","iopub.execute_input":"2025-07-17T05:12:53.101492Z","iopub.status.idle":"2025-07-17T05:15:16.999631Z","shell.execute_reply.started":"2025-07-17T05:12:53.101468Z","shell.execute_reply":"2025-07-17T05:15:16.998665Z"}},"outputs":[{"name":"stderr","text":"2025-07-17 05:12:56.054660: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1752729176.248174      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1752729176.305336      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"INFO 07-17 05:13:06 [__init__.py:244] Automatically detected platform cuda.\nINFO 07-17 05:13:23 [config.py:841] This model supports multiple tasks: {'reward', 'embed', 'classify', 'generate'}. Defaulting to 'generate'.\nINFO 07-17 05:13:23 [config.py:1472] Using max model len 4096\nWARNING 07-17 05:13:23 [config.py:960] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\nWARNING 07-17 05:13:24 [cuda.py:102] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\nINFO 07-17 05:13:24 [llm_engine.py:230] Initializing a V0 LLM engine (v0.9.2) with config: model='/kaggle/input/qwen-3/transformers/8b-awq/1', speculative_config=None, tokenizer='/kaggle/input/qwen-3/transformers/8b-awq/1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='xgrammar', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=None, served_model_name=/kaggle/input/qwen-3/transformers/8b-awq/1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=False, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":false,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":0,\"local_cache_dir\":null}, use_cached_outputs=False, \nWARNING 07-17 05:13:24 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 2 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n\u001b[1;36m(VllmWorkerProcess pid=121)\u001b[0;0m INFO 07-17 05:13:24 [multiproc_worker_utils.py:226] Worker ready; awaiting tasks\nINFO 07-17 05:13:25 [cuda.py:311] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\nINFO 07-17 05:13:25 [cuda.py:360] Using XFormers backend.\n\u001b[1;36m(VllmWorkerProcess pid=121)\u001b[0;0m INFO 07-17 05:13:25 [cuda.py:311] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n\u001b[1;36m(VllmWorkerProcess pid=121)\u001b[0;0m INFO 07-17 05:13:25 [cuda.py:360] Using XFormers backend.\n","output_type":"stream"},{"name":"stderr","text":"[W717 05:13:36.091873774 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[W717 05:13:36.644822137 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[W717 05:13:46.102506508 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1;36m(VllmWorkerProcess pid=121)\u001b[0;0m INFO 07-17 05:13:56 [__init__.py:1152] Found nccl from library libnccl.so.2\nINFO 07-17 05:13:56 [__init__.py:1152] Found nccl from library libnccl.so.2\n\u001b[1;36m(VllmWorkerProcess pid=121)\u001b[0;0m INFO 07-17 05:13:56 [pynccl.py:70] vLLM is using nccl==2.26.2\nINFO 07-17 05:13:56 [pynccl.py:70] vLLM is using nccl==2.26.2\n","output_type":"stream"},{"name":"stderr","text":"[W717 05:13:56.110591574 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n","output_type":"stream"},{"name":"stdout","text":"INFO 07-17 05:13:56 [custom_all_reduce_utils.py:208] generating GPU P2P access cache in /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\nINFO 07-17 05:14:19 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n\u001b[1;36m(VllmWorkerProcess pid=121)\u001b[0;0m INFO 07-17 05:14:19 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\nINFO 07-17 05:14:19 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_30d80bdc'), local_subscribe_addr='ipc:///tmp/4fdb97c1-4287-4e85-b026-45e1ab30e3e8', remote_subscribe_addr=None, remote_addr_ipv6=False)\nINFO 07-17 05:14:19 [parallel_state.py:1076] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n\u001b[1;36m(VllmWorkerProcess pid=121)\u001b[0;0m INFO 07-17 05:14:19 [parallel_state.py:1076] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\nINFO 07-17 05:14:19 [model_runner.py:1171] Starting to load model /kaggle/input/qwen-3/transformers/8b-awq/1...\n\u001b[1;36m(VllmWorkerProcess pid=121)\u001b[0;0m INFO 07-17 05:14:19 [model_runner.py:1171] Starting to load model /kaggle/input/qwen-3/transformers/8b-awq/1...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0496cb677f7d487b880d5ef5364df5ea"}},"metadata":{}},{"name":"stdout","text":"INFO 07-17 05:15:03 [default_loader.py:272] Loading weights took 43.91 seconds\n\u001b[1;36m(VllmWorkerProcess pid=121)\u001b[0;0m INFO 07-17 05:15:04 [default_loader.py:272] Loading weights took 44.12 seconds\nINFO 07-17 05:15:04 [model_runner.py:1203] Model loading took 2.8510 GiB and 44.161543 seconds\n\u001b[1;36m(VllmWorkerProcess pid=121)\u001b[0;0m INFO 07-17 05:15:04 [model_runner.py:1203] Model loading took 2.8510 GiB and 44.368781 seconds\n\u001b[1;36m(VllmWorkerProcess pid=121)\u001b[0;0m INFO 07-17 05:15:11 [worker.py:294] Memory profiling takes 6.17 seconds\n\u001b[1;36m(VllmWorkerProcess pid=121)\u001b[0;0m INFO 07-17 05:15:11 [worker.py:294] the current vLLM instance can use total_gpu_memory (14.74GiB) x gpu_memory_utilization (0.92) = 13.56GiB\n\u001b[1;36m(VllmWorkerProcess pid=121)\u001b[0;0m INFO 07-17 05:15:11 [worker.py:294] model weights take 2.85GiB; non_torch_memory takes 0.12GiB; PyTorch activation peak memory takes 0.32GiB; the rest of the memory reserved for KV Cache is 10.27GiB.\nINFO 07-17 05:15:11 [worker.py:294] Memory profiling takes 6.29 seconds\nINFO 07-17 05:15:11 [worker.py:294] the current vLLM instance can use total_gpu_memory (14.74GiB) x gpu_memory_utilization (0.92) = 13.56GiB\nINFO 07-17 05:15:11 [worker.py:294] model weights take 2.85GiB; non_torch_memory takes 0.12GiB; PyTorch activation peak memory takes 1.42GiB; the rest of the memory reserved for KV Cache is 9.17GiB.\nINFO 07-17 05:15:12 [executor_base.py:113] # cuda blocks: 8346, # CPU blocks: 3640\nINFO 07-17 05:15:12 [executor_base.py:118] Maximum concurrency for 4096 tokens per request: 32.60x\nINFO 07-17 05:15:16 [llm_engine.py:428] init engine (profile, create kv cache, warmup model) took 11.90 seconds\n","output_type":"stream"}],"execution_count":8},{"id":"d8256573","cell_type":"markdown","source":"# System prompts","metadata":{"papermill":{"duration":0.004554,"end_time":"2025-07-09T11:00:32.614342","exception":false,"start_time":"2025-07-09T11:00:32.609788","status":"completed"},"tags":[]}},{"id":"28ba16ac","cell_type":"code","source":"SYS_PROMPT_DOI = \"\"\"\nYou are an expert at identifying RESEARCH DATA citations in academic papers.\nYour task is to determine if a DOI in the provided text specifically refers to a dataset, software, or data repository, NOT another academic paper.\n\n**Crucial Rules:**\n1.  **LOOK FOR DATA CONTEXT:** The DOI must be near keywords like \"data available\", \"deposited in\", \"repository\", \"accession number\", \"software\", \"code\".\n2.  **IGNORE BIBLIOGRAPHY:** If the DOI is clearly part of a numbered or author-year list in a \"References\" or \"Bibliography\" section, you MUST respond with \"Irrelevant\".\n3.  **PRIORITIZE DATA DOIs:** If there are multiple DOIs, return the one most likely to be a dataset.\n\nOnly respond with either a full normalized DOI URL starting with \"https://doi.org/\" or the single word \"Irrelevant\".\nDo NOT include any other text or explanation.\n\"\"\"\n\nif think_mode:\n    \n    SYS_PROMPT_ACCESSION = \"\"\"\n    You are an expert at analyzing research data usage in academic papers.\n    \n    Think step-by-step about the surrounding text, identifying clues such as:\n    - PRIMARY data: “we deposited”, “data generated in this study”, “our data”, “submitted to”, “newly generated”\n    - SECONDARY data: “downloaded from”, “obtained from”, “previously published”, “publicly available”, “existing dataset”\n    - MISSING: mentioned only in references, general methodology descriptions without actual usage, or contexts unrelated to research data\n\n    If any dataset is mentioned, extract its ID and classify its type as Primary, Secondary, or Missing. Do not leave type as Missing if the \n    dataset is clearly referenced as reused or newly generated\n    \n    Silently reason through the classification.\n    \n    Please show your choice in the answer field with only the choice letter, e.g.,  \n    \"answer\": \"C\"\n    \"\"\"\n    \n    SYS_PROMPT_CLASSIFY_DOI = \"\"\"\n    You are an expert at analyzing research data citations in academic papers.\n    \n    First, reason step-by-step about whether the DOI refers to data that is:\n    A) Primary – generated specifically for this study  \n    B) Secondary – reused or derived from prior work  \n    C) Missing – merely cited in references, not research data, or otherwise unrelated\n    \n    If any dataset is mentioned, extract its ID and classify its type as Primary, \n    Secondary, or Missing. Do not leave type as Missing if the dataset is clearly referenced as reused or newly generated\n    \n    Perform this reasoning silently.\n    \n    Please show your choice in the answer field with only the choice letter, e.g.,  \n    \"answer\": \"B\"\n    \"\"\"\n\nelse:    \n    SYS_PROMPT_ACCESSION = \"\"\"\n    You are an expert at analyzing research data usage in academic papers.\n    \n    Look for contextual clues:\n    - For PRIMARY data: \"we deposited\", \"data generated in this study\", \"our data\", \"submitted to\", \"newly generated\"\n    - For SECONDARY data: \"downloaded from\", \"obtained from\", \"previously published\", \"publicly available\", \"existing dataset\"\n    - For MISSING: mentioned in references, methodology descriptions without actual usage, or unrelated contexts\n    \n    Respond with only one letter: A, B, or C.\n    \"\"\"\n    \n    SYS_PROMPT_CLASSIFY_DOI = \"\"\"\n    You are an expert at analyzing research data citations in academic papers.\n    \n    Classify the data as:\n    A) Primary: if the data was generated specifically for this study\n    B) Secondary: if the data was reused or derived from prior work  \n    C) Missing: if the DOI is in references, doesn't refer to research data, or is unrelated\n    \n    Respond with only one letter: A, B, or C.\n    \"\"\"","metadata":{"papermill":{"duration":0.014504,"end_time":"2025-07-09T11:00:32.633787","exception":false,"start_time":"2025-07-09T11:00:32.619283","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T05:15:17.000971Z","iopub.execute_input":"2025-07-17T05:15:17.001313Z","iopub.status.idle":"2025-07-17T05:15:17.007124Z","shell.execute_reply.started":"2025-07-17T05:15:17.001270Z","shell.execute_reply":"2025-07-17T05:15:17.006504Z"}},"outputs":[],"execution_count":9},{"id":"a1eea855","cell_type":"markdown","source":"## Ask LLM to extract DOI links","metadata":{"papermill":{"duration":0.008878,"end_time":"2025-07-09T11:00:32.648589","exception":false,"start_time":"2025-07-09T11:00:32.639711","status":"completed"},"tags":[]}},{"id":"baa2b996","cell_type":"code","source":"prompts = []\nfor article_id, academic_text in chunks:\n    messages = [\n        {\"role\": \"system\", \"content\": SYS_PROMPT_DOI},\n        {\"role\": \"user\", \"content\": academic_text}\n    ]\n\n    if think_mode:\n\n        prompt = tokenizer.apply_chat_template(\n            messages,\n            add_generation_prompt=True,\n            tokenize=False,enable_thinking=False\n        )\n    else:\n         prompt = tokenizer.apply_chat_template(\n            messages,\n            add_generation_prompt=True,\n            tokenize=False\n        )\n    \n    prompts.append(prompt)\n\noutputs = llm.generate(\n    prompts,\n    vllm.SamplingParams(\n        seed=0,\n        skip_special_tokens=True,\n        max_tokens=64,\n        temperature=0\n    ),\n    use_tqdm=True\n)\n\nresponses = [output.outputs[0].text.strip() for output in outputs]\n\ndoi_pattern = re.compile(r'(10\\.\\d{4,9}/[-._;()/:A-Z0-9]+)', re.I)\n\ndoi_urls = []\nfor response in responses:\n    if response.lower() == \"irrelevant\":\n        doi_urls.append(\"Irrelevant\")\n    else:\n        match = doi_pattern.search(response)\n        if match:\n            doi_urls.append(\"https://doi.org/\" + match.group(1))\n        else:\n            doi_urls.append(\"Irrelevant\")\n","metadata":{"papermill":{"duration":84.595614,"end_time":"2025-07-09T11:01:57.254151","exception":false,"start_time":"2025-07-09T11:00:32.658537","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T05:15:17.010236Z","iopub.execute_input":"2025-07-17T05:15:17.010433Z","iopub.status.idle":"2025-07-17T05:16:07.519325Z","shell.execute_reply.started":"2025-07-17T05:15:17.010417Z","shell.execute_reply":"2025-07-17T05:16:07.518516Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Adding requests:   0%|          | 0/293 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02e7e636eeab4822b2e5ca6e79f8cf6d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processed prompts:   0%|          | 0/293 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3b6f21f573e46c48e18cf1a97934160"}},"metadata":{}}],"execution_count":10},{"id":"003f3225","cell_type":"code","source":"import re\n\ndef parse_answer_with_regex(response_text: str):\n\n    if not isinstance(response_text, str):\n        return 'Missing'\n\n    match = re.search(r'answer\\b.*?([ABC])\\b', response_text, re.IGNORECASE | re.DOTALL)\n    if match:\n        return match.group(1)\n\n    all_choices = re.findall(r'[ABC]', response_text)\n    if all_choices:\n        return all_choices[-1]\n        \n    return 'Missing'","metadata":{"papermill":{"duration":0.010799,"end_time":"2025-07-09T11:01:57.270445","exception":false,"start_time":"2025-07-09T11:01:57.259646","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T05:16:07.520034Z","iopub.execute_input":"2025-07-17T05:16:07.520310Z","iopub.status.idle":"2025-07-17T05:16:07.525134Z","shell.execute_reply.started":"2025-07-17T05:16:07.520283Z","shell.execute_reply":"2025-07-17T05:16:07.524411Z"}},"outputs":[],"execution_count":11},{"id":"8b0f1650","cell_type":"code","source":"prompts = []\nvalid_indices = []\n\nif think_mode:\n    for i, (chunk, url) in enumerate(zip(chunks, doi_urls)):\n        if url == \"Irrelevant\":\n            continue\n    \n        article_id, academic_text = chunk\n        messages = [\n            {\"role\": \"system\", \"content\": SYS_PROMPT_CLASSIFY_DOI},\n            {\"role\": \"user\", \"content\": f\"DOI: {url}\\n\\nAcademic text:\\n{academic_text}\"}\n        ]\n    \n        prompt = tokenizer.apply_chat_template(\n            messages,\n            add_generation_prompt=True,\n            tokenize=False,\n            enable_thinking=True\n        )\n        prompts.append(prompt)\n        valid_indices.append(i)\n    \n    outputs = llm.generate(\n        prompts,\n        vllm.SamplingParams(\n            seed=777,\n            temperature=0.65,\n            top_p=0.95,\n            top_k=20,\n            skip_special_tokens=True,\n            max_tokens=2048+1024,\n            presence_penalty=1.5\n        ),\n        use_tqdm=True\n    )\n\n    choice_to_type_map = {'A': 'Primary', 'B': 'Secondary', 'C': 'Missing'}\n\n    responses = [output.outputs[0].text.strip() for output in outputs]\n    \n    parsed_doi_choices = [parse_answer_with_regex(resp) for resp in responses]\n    final_doi_answers = [choice_to_type_map.get(choice) for choice in parsed_doi_choices]\n    \n    answers = ['Missing'] * len(chunks)\n    for i, answer in zip(valid_indices, final_doi_answers):\n        answers[i] = answer\n    \n    \nelse:\n    for i, (chunk, url) in enumerate(zip(chunks, doi_urls)):\n        if url == \"Irrelevant\":\n            continue\n    \n        article_id, academic_text = chunk\n        messages = [\n            {\"role\": \"system\", \"content\": SYS_PROMPT_CLASSIFY_DOI},\n            {\"role\": \"user\", \"content\": f\"DOI: {url}\\n\\nAcademic text:\\n{academic_text}\"}\n        ]\n    \n        prompt = tokenizer.apply_chat_template(\n            messages,\n            add_generation_prompt=True,\n            tokenize=False,enable_thinking=False\n        )\n        prompts.append(prompt)\n        valid_indices.append(i)\n    \n    mclp = MultipleChoiceLogitsProcessor(tokenizer, choices=[\"A\", \"B\", \"C\"])\n    \n    outputs = llm.generate(\n        prompts,\n        vllm.SamplingParams(\n            seed=777,\n            temperature=0.05, \n            skip_special_tokens=True,\n            max_tokens=1,\n            logits_processors=[mclp],\n            logprobs=len(mclp.choices)\n        ),\n        use_tqdm=True\n    )\n    \n    logprobs = []\n    for lps in [output.outputs[0].logprobs[0].values() for output in outputs]:\n        logprobs.append({lp.decoded_token: lp.logprob for lp in list(lps)})\n    \n    logit_matrix = pd.DataFrame(logprobs)[[\"A\", \"B\", \"C\"]].values\n    choices = [\"Primary\", \"Secondary\", 'Missing']\n    answers = ['Missing'] * len(chunks)\n    \n    for i, (idx, logit_row) in enumerate(zip(valid_indices, logit_matrix)):\n        max_logit = np.max(logit_row)\n        max_idx = np.argmax(logit_row)\n        \n        if max_logit > -2.0:\n            answers[idx] = choices[max_idx]","metadata":{"papermill":{"duration":722.628841,"end_time":"2025-07-09T11:13:59.904380","exception":false,"start_time":"2025-07-09T11:01:57.275539","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T05:16:07.526033Z","iopub.execute_input":"2025-07-17T05:16:07.526298Z","iopub.status.idle":"2025-07-17T05:18:11.770085Z","shell.execute_reply.started":"2025-07-17T05:16:07.526265Z","shell.execute_reply":"2025-07-17T05:18:11.769359Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Adding requests:   0%|          | 0/69 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"adac16b239f04793bf07f41fa373fd84"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processed prompts:   0%|          | 0/69 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"252b18e28e7a4156a3951d6bfc3acbc9"}},"metadata":{}}],"execution_count":12},{"id":"b0a6cf5f","cell_type":"code","source":"prompts = []\n\nif think_mode:\n    for chunk, acc_id in zip(chunks2, ids):\n        article_id, academic_text = chunk\n        messages = [\n            {\"role\": \"system\", \"content\": SYS_PROMPT_ACCESSION},\n            {\"role\": \"user\", \"content\": f\"Accession ID: {acc_id}\\n\\nAcademic text:\\n{academic_text}\"}\n        ]\n    \n        prompt = tokenizer.apply_chat_template(\n            messages,\n            add_generation_prompt=True,\n            tokenize=False,\n            enable_thinking=True\n        )\n        prompts.append(prompt)\n    \n    outputs = llm.generate(\n        prompts,\n        vllm.SamplingParams(\n            seed=777,\n            temperature=0.65,\n            top_p=0.95,\n            top_k=20,\n            skip_special_tokens=True,\n            max_tokens=2048+1024,\n            presence_penalty=1.5\n        ),\n        use_tqdm=True\n    )\n    choice_to_type_map = {'A': 'Primary', 'B': 'Secondary', 'C': 'Missing'}\n\n    responses = [output.outputs[0].text.strip() for output in outputs]\n    \n    parsed_doi_choices = [parse_answer_with_regex(resp) for resp in responses]\n    answers2 = [choice_to_type_map.get(choice) for choice in parsed_doi_choices]\n\nelse:\n    for chunk, acc_id in zip(chunks2, ids):\n        article_id, academic_text = chunk\n        messages = [\n            {\"role\": \"system\", \"content\": SYS_PROMPT_ACCESSION},\n            {\"role\": \"user\", \"content\": f\"Accession ID: {acc_id}\\n\\nAcademic text:\\n{academic_text}\"}\n        ]\n    \n        prompt = tokenizer.apply_chat_template(\n            messages,\n            add_generation_prompt=True,\n            tokenize=False,enable_thinking=False\n        )\n        prompts.append(prompt)\n    \n    outputs = llm.generate(\n        prompts,\n        vllm.SamplingParams(\n            seed=777,\n            temperature=0.05,\n            skip_special_tokens=True,\n            max_tokens=1,\n            logits_processors=[mclp],\n            logprobs=len(mclp.choices)\n        ),\n        use_tqdm=True\n    )\n    \n    logprobs2 = []\n    for lps in [output.outputs[0].logprobs[0].values() for output in outputs]:\n        logprobs2.append({lp.decoded_token: lp.logprob for lp in list(lps)})\n    \n    logit_matrix2 = pd.DataFrame(logprobs2)[[\"A\", \"B\", \"C\"]].values\n    choices2 = [\"Primary\", \"Secondary\", 'Missing']\n    \n    answers2 = []\n    for logit_row in logit_matrix2:\n        max_logit = np.max(logit_row)\n        max_idx = np.argmax(logit_row)\n        \n        if max_logit > -2.0:\n            answers2.append(choices2[max_idx])\n        else:\n            answers2.append('')\n    ","metadata":{"papermill":{"duration":1687.987088,"end_time":"2025-07-09T11:42:07.897069","exception":false,"start_time":"2025-07-09T11:13:59.909981","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T05:18:11.771046Z","iopub.execute_input":"2025-07-17T05:18:11.771625Z","iopub.status.idle":"2025-07-17T05:18:19.128781Z","shell.execute_reply.started":"2025-07-17T05:18:11.771603Z","shell.execute_reply":"2025-07-17T05:18:19.127819Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bac7e8d14fcd4728a5a3527e9c12d6e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da2afe7fdc054d508e17b28e3671ee0b"}},"metadata":{}}],"execution_count":13},{"id":"1a6d9d0a","cell_type":"markdown","source":"## Prepare Submission","metadata":{"papermill":{"duration":0.006004,"end_time":"2025-07-09T11:42:07.909622","exception":false,"start_time":"2025-07-09T11:42:07.903618","status":"completed"},"tags":[]}},{"id":"501a1a0b","cell_type":"code","source":"# change \"llm_type\" to \"type\" to use LLM predictions\n# as is, will use /kaggle/input/makedatacount-mixed-train model\n# for predicting \"type\"\n\nsub_df = pd.DataFrame()\nsub_df[\"article_id\"] = [c[0] for c in chunks]\nsub_df[\"dataset_id\"] = doi_urls\nsub_df[\"dataset_id\"] = sub_df[\"dataset_id\"].str.lower()\nsub_df[\"type\"] = answers\nsub_df = sub_df[sub_df[\"type\"].notnull()].reset_index(drop=True)\n\nsub_df2 = pd.DataFrame()\nsub_df2[\"article_id\"] = [c[0] for c in chunks2]\nsub_df2[\"dataset_id\"] = ids\n\nsub_df2[\"type\"] = answers2\nsub_df2 = sub_df2[sub_df2[\"type\"].notnull()].reset_index(drop=True)\n\n# Combine and clean\nsub_df = pd.concat([sub_df, sub_df2], ignore_index=True)\n\nprint(\"Final submission stats:\")\nprint(sub_df[\"type\"].value_counts())\nprint(f\"Total entries: {len(sub_df)}\")","metadata":{"papermill":{"duration":0.109375,"end_time":"2025-07-09T11:42:08.024830","exception":false,"start_time":"2025-07-09T11:42:07.915455","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T05:18:19.129668Z","iopub.execute_input":"2025-07-17T05:18:19.130498Z","iopub.status.idle":"2025-07-17T05:18:19.175906Z","shell.execute_reply.started":"2025-07-17T05:18:19.130476Z","shell.execute_reply":"2025-07-17T05:18:19.175186Z"}},"outputs":[{"name":"stdout","text":"Final submission stats:\ntype\nMissing      243\nPrimary       25\nSecondary     10\nName: count, dtype: int64\nTotal entries: 278\n","output_type":"stream"}],"execution_count":14},{"id":"11424364-63f5-4744-9560-ad2dbb25b068","cell_type":"code","source":"mask = sub_df.applymap(lambda x: x is None)\ncols = sub_df.columns[(mask).any()]\nfor col in sub_df[cols]:\n    sub_df.loc[mask[col], col] = 'Missing'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T05:18:19.176693Z","iopub.execute_input":"2025-07-17T05:18:19.176927Z","iopub.status.idle":"2025-07-17T05:18:19.186993Z","shell.execute_reply.started":"2025-07-17T05:18:19.176901Z","shell.execute_reply":"2025-07-17T05:18:19.186180Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_36/300274337.py:1: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n  mask = sub_df.applymap(lambda x: x is None)\n","output_type":"stream"}],"execution_count":15},{"id":"ff95f658-e587-4465-bd2a-a8dfd4ac91eb","cell_type":"code","source":"# Uses use /kaggle/input/makedatacount-mixed-train model\n# predictions on \"type\"\n# sub_df = pd.merge(sub_df, df_preds[['article_id','type']], on=\"article_id\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T05:18:19.187767Z","iopub.execute_input":"2025-07-17T05:18:19.188269Z","iopub.status.idle":"2025-07-17T05:18:19.201096Z","shell.execute_reply.started":"2025-07-17T05:18:19.188244Z","shell.execute_reply":"2025-07-17T05:18:19.200533Z"}},"outputs":[],"execution_count":16},{"id":"c957eb69-504e-45b7-9e08-b8b91cf6d836","cell_type":"code","source":"sub_df=sub_df[~sub_df['dataset_id'].isin([\"irrelevant\"])]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T05:18:19.201837Z","iopub.execute_input":"2025-07-17T05:18:19.202004Z","iopub.status.idle":"2025-07-17T05:18:19.218385Z","shell.execute_reply.started":"2025-07-17T05:18:19.201991Z","shell.execute_reply":"2025-07-17T05:18:19.217710Z"}},"outputs":[],"execution_count":17},{"id":"bff0f98d-489f-47fa-9723-1db2cacce0aa","cell_type":"code","source":"# sub_df.type_x = sub_df.type_y.where(sub_df.type_x == 'Missing', sub_df.type_x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T05:18:19.219242Z","iopub.execute_input":"2025-07-17T05:18:19.219494Z","iopub.status.idle":"2025-07-17T05:18:19.234230Z","shell.execute_reply.started":"2025-07-17T05:18:19.219472Z","shell.execute_reply":"2025-07-17T05:18:19.233627Z"}},"outputs":[],"execution_count":18},{"id":"b5d5c129-ed21-4314-ad52-a4286be3fa56","cell_type":"code","source":"sub_df = sub_df[sub_df[\"type\"].isin([\"Primary\", \"Secondary\"])].reset_index(drop=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T05:18:19.235035Z","iopub.execute_input":"2025-07-17T05:18:19.235744Z","iopub.status.idle":"2025-07-17T05:18:19.249301Z","shell.execute_reply.started":"2025-07-17T05:18:19.235720Z","shell.execute_reply":"2025-07-17T05:18:19.248653Z"}},"outputs":[],"execution_count":19},{"id":"d9130727-721f-4138-827e-abc296e7b0e2","cell_type":"code","source":"# del sub_df['type_y']\n# sub_df = sub_df.rename(columns={'type_x': 'type'})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T05:18:19.249895Z","iopub.execute_input":"2025-07-17T05:18:19.250080Z","iopub.status.idle":"2025-07-17T05:18:19.262765Z","shell.execute_reply.started":"2025-07-17T05:18:19.250066Z","shell.execute_reply":"2025-07-17T05:18:19.262193Z"}},"outputs":[],"execution_count":20},{"id":"b800cca6-4c16-4a0c-bde2-e9c56b1e1bfc","cell_type":"code","source":"# Enhanced deduplication with priority to Primary data\nsub_df = sub_df.sort_values(by=[\"article_id\", \"dataset_id\", \"type\"], \n                           key=lambda x: x.map({\"Primary\": 0, \"Secondary\": 1}) if x.name == \"type\" else x)\\\n               .drop_duplicates(subset=['article_id', 'dataset_id'], keep=\"first\")\\\n               .reset_index(drop=True)\n\nsub_df['row_id'] = range(len(sub_df))\nsub_df.to_csv(\"submission.csv\", index=False, columns=[\"row_id\", \"article_id\", \"dataset_id\", \"type\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T05:18:19.263516Z","iopub.execute_input":"2025-07-17T05:18:19.263715Z","iopub.status.idle":"2025-07-17T05:18:19.292587Z","shell.execute_reply.started":"2025-07-17T05:18:19.263693Z","shell.execute_reply":"2025-07-17T05:18:19.291981Z"}},"outputs":[],"execution_count":21},{"id":"c5432ede","cell_type":"markdown","source":"## Evaluate validation score","metadata":{"papermill":{"duration":0.006,"end_time":"2025-07-09T11:42:08.037449","exception":false,"start_time":"2025-07-09T11:42:08.031449","status":"completed"},"tags":[]}},{"id":"e5098fe5","cell_type":"code","source":"def f1_score(tp, fp, fn):\n    return 2 * tp / (2 * tp + fp + fn) if (2 * tp + fp + fn) != 0 else 0.0\n    \nif not os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    pred_df = pd.read_csv(\"submission.csv\")\n    label_df = pd.read_csv(\"/kaggle/input/make-data-count-finding-data-references/train_labels.csv\")\n    label_df = label_df[label_df['type'] != 'Missing'].reset_index(drop=True)\n\n    hits_df = label_df.merge(pred_df, on=[\"article_id\", \"dataset_id\", \"type\"])\n    \n    tp = hits_df.shape[0]\n    fp = pred_df.shape[0] - tp\n    fn = label_df.shape[0] - tp\n    \n    print(\"\\nValidation Results:\")\n    print(\"TP:\", tp)\n    print(\"FP:\", fp)\n    print(\"FN:\", fn)\n    print(\"F1 Score:\", round(f1_score(tp, fp, fn), 3))","metadata":{"papermill":{"duration":0.060687,"end_time":"2025-07-09T11:42:08.104202","exception":false,"start_time":"2025-07-09T11:42:08.043515","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T05:18:19.293345Z","iopub.execute_input":"2025-07-17T05:18:19.293681Z","iopub.status.idle":"2025-07-17T05:18:19.299099Z","shell.execute_reply.started":"2025-07-17T05:18:19.293662Z","shell.execute_reply":"2025-07-17T05:18:19.298524Z"}},"outputs":[],"execution_count":22},{"id":"fce48dc1","cell_type":"code","source":"import os\nimport pandas as pd\n\ndef calculate_f1_score(y_true, y_pred):\n    if y_true.empty or y_pred.empty:\n        tp = 0\n        fp = len(y_pred)\n        fn = len(y_true)\n    else:\n        hits = y_true.merge(y_pred, on=[\"article_id\", \"dataset_id\", \"type\"])\n        tp = len(hits)\n        fp = len(y_pred) - tp\n        fn = len(y_true) - tp\n    \n    f1 = 2 * tp / (2 * tp + fp + fn) if (2 * tp + fp + fn) > 0 else 0.0\n    return tp, fp, fn, f1\n\ndef analyze_error_sources(pred_df, label_df):\n    label_df_filtered = label_df[label_df['type'] != 'Missing'].copy()\n\n    is_doi_pred = pred_df['dataset_id'].str.startswith('https://doi.org/')\n    is_doi_label = label_df_filtered['dataset_id'].str.startswith('10.')\n\n    pred_doi = pred_df[is_doi_pred]\n    pred_accession = pred_df[~is_doi_pred]\n    label_df_filtered['dataset_id_normalized'] = label_df_filtered['dataset_id'].apply(\n        lambda x: f\"https://doi.org/{x}\" if x.startswith('10.') else x\n    )\n    label_df_filtered = label_df_filtered.rename(columns={'dataset_id': 'original_dataset_id', 'dataset_id_normalized': 'dataset_id'})\n    \n    is_doi_label_norm = label_df_filtered['dataset_id'].str.startswith('https://doi.org/')\n\n    label_doi = label_df_filtered[is_doi_label_norm]\n    label_accession = label_df_filtered[~is_doi_label_norm]\n\n    tp_doi, fp_doi, fn_doi, f1_doi = calculate_f1_score(label_doi, pred_doi)\n    tp_acc, fp_acc, fn_acc, f1_acc = calculate_f1_score(label_accession, pred_accession)\n    \n    print(\"=\"*40)\n    print(\"🔬 Error Analysis by ID Type\")\n    print(\"=\"*40)\n\n    print(\"\\n--- DOI ---\")\n    print(f\"Total Predictions: {len(pred_doi)}\")\n    print(f\"True Positives (TP): {tp_doi}\")\n    print(f\"False Positives (FP): {fp_doi}\")\n    print(f\"False Negatives (FN): {fn_doi}\")\n    print(f\"F1 Score: {f1_doi:.4f}\")\n\n    print(\"\\n--- Accession ID ---\")\n    print(f\"Total Predictions: {len(pred_accession)}\")\n    print(f\"True Positives (TP): {tp_acc}\")\n    print(f\"False Positives (FP): {fp_acc}\")\n    print(f\"False Negatives (FN): {fn_acc}\")\n    print(f\"F1 Score: {f1_acc:.4f}\")\n    \n    print(\"\\n\" + \"=\"*40)\n    print(\"Total FP:\", fp_doi + fp_acc)\n    print(\"Total FN:\", fn_doi + fn_acc)\n    print(\"=\"*40)\n\nif not os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    try:\n        pred_df = pd.read_csv(\"submission.csv\")\n        pred_df['dataset_id'] = pred_df['dataset_id'].astype(str)\n        \n        label_df = pd.read_csv(\"/kaggle/input/make-data-count-finding-data-references/train_labels.csv\")\n        label_df['dataset_id'] = label_df['dataset_id'].astype(str)\n\n        analyze_error_sources(pred_df, label_df)\n\n    except FileNotFoundError as e:\n        print(f\"Error: Could not find a required file. {e}\")\n    except Exception as e:\n        print(f\"An unexpected error occurred: {e}\")","metadata":{"papermill":{"duration":0.038385,"end_time":"2025-07-09T11:42:08.148888","exception":false,"start_time":"2025-07-09T11:42:08.110503","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T05:18:19.300229Z","iopub.execute_input":"2025-07-17T05:18:19.300482Z","iopub.status.idle":"2025-07-17T05:18:19.321412Z","shell.execute_reply.started":"2025-07-17T05:18:19.300467Z","shell.execute_reply":"2025-07-17T05:18:19.320835Z"}},"outputs":[],"execution_count":23},{"id":"5fbd5d7a","cell_type":"code","source":"import pandas as pd\n\ntry:\n    pred_df = pd.read_csv(\"submission.csv\")\n    label_df = pd.read_csv(\"/kaggle/input/make-data-count-finding-data-references/train_labels.csv\")\n    label_df_filtered = label_df[label_df['type'] != 'Missing'].copy()\nexcept FileNotFoundError as e:\n    print(f\"An error occurred: File not found. - {e}\")\n    exit()\n\nfn_df = pd.merge(\n    label_df_filtered,\n    pred_df,\n    on=['article_id', 'dataset_id', 'type'],\n    how='left',\n    indicator=True\n).query('_merge == \"left_only\"').drop(columns=['_merge'])\n\nmerged_df = pd.merge(\n    fn_df,\n    pred_df,\n    on=['article_id', 'dataset_id'],\n    how='left',\n    indicator='source'\n)\n\nclassified_incorrectly_df = merged_df[merged_df['source'] == 'both']\nclassified_incorrectly_count = len(classified_incorrectly_df)\n\ncompletely_missed_df = merged_df[merged_df['source'] == 'left_only']\ncompletely_missed_count = len(completely_missed_df)\n\nincorrect_doi_count = classified_incorrectly_df[classified_incorrectly_df['dataset_id'].str.startswith('https://', na=False)].shape[0]\nincorrect_accession_count = classified_incorrectly_df[~classified_incorrectly_df['dataset_id'].str.startswith('https://', na=False)].shape[0]\n\n\nmissed_doi_count = completely_missed_df[completely_missed_df['dataset_id'].str.startswith('https://', na=False)].shape[0]\nmissed_accession_count = completely_missed_df[~completely_missed_df['dataset_id'].str.startswith('https://', na=False)].shape[0]\n\n\nprint(\"=\"*55)\nprint(\"Analyst False Negatives (FN)\")\nprint(\"=\"*55)\nprint(f\"All FN: {fn_df.shape[0]} record\")\nprint(\"-\" * 55)\nprint(f\"↳ It have but wrong answer: {classified_incorrectly_count} record\")\nprint(f\"    Wrong DOI: {incorrect_doi_count} record\")\nprint(f\"    Wrong Accession ID: {incorrect_accession_count} record\")\nprint(\"-\" * 55)\nprint(f\"↳ Can't find this: {completely_missed_count} record\")\nprint(f\"    Can't find DOI: {missed_doi_count} record\")\nprint(f\"    Can't find Accession ID: {missed_accession_count} record\")\nprint(\"=\"*55)","metadata":{"papermill":{"duration":0.059407,"end_time":"2025-07-09T11:42:08.215090","exception":false,"start_time":"2025-07-09T11:42:08.155683","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T05:18:19.322144Z","iopub.execute_input":"2025-07-17T05:18:19.322417Z","iopub.status.idle":"2025-07-17T05:18:19.389461Z","shell.execute_reply.started":"2025-07-17T05:18:19.322400Z","shell.execute_reply":"2025-07-17T05:18:19.388910Z"}},"outputs":[{"name":"stdout","text":"=======================================================\nAnalyst False Negatives (FN)\n=======================================================\nAll FN: 714 record\n-------------------------------------------------------\n↳ It have but wrong answer: 1 record\n    Wrong DOI: 1 record\n    Wrong Accession ID: 0 record\n-------------------------------------------------------\n↳ Can't find this: 713 record\n    Can't find DOI: 319 record\n    Can't find Accession ID: 394 record\n=======================================================\n","output_type":"stream"}],"execution_count":24},{"id":"63b87e2f-2f30-48bb-85d1-08a7ed9e454a","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}